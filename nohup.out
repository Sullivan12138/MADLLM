Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs()>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7084
val 1416
test 7084
Start train feature encoder...
Epoch: 0
Epoch: 1
Epoch: 2
Epoch: 3
Epoch: 4
Epoch: 5
Epoch: 6
Epoch: 7
Epoch: 8
Epoch: 9
Epoch: 10
Epoch: 11
Epoch: 12
Epoch: 13
Epoch: 14
Epoch: 15
Epoch: 16
Epoch: 17
Epoch: 18
Epoch: 19
Epoch: 20
Epoch: 21
Epoch: 22
Epoch: 23
Epoch: 24
Epoch: 25
Epoch: 26
Save Encoder Model...
Traceback (most recent call last):
  File "run.py", line 146, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 100, in train
    feature_embedding = self.feature_encoder(batch_x, self.args.patch_size).detach()
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/FeatureEncoder.py", line 41, in forward
    x = x * \
RuntimeError: The size of tensor a (384) must match the size of tensor b (38) at non-singleton dimension 3
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs()>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7084
val 1416
test 7084
Start train feature encoder...
Epoch: 0
Epoch: 1
Epoch: 2
Epoch: 3
Epoch: 4
Epoch: 5
Epoch: 6
Epoch: 7
Epoch: 8
Epoch: 9
Epoch: 10
Epoch: 11
Epoch: 12
Epoch: 13
Epoch: 14
Epoch: 15
Epoch: 16
Epoch: 17
Epoch: 18
Epoch: 19
Epoch: 20
Epoch: 21
Epoch: 22
Epoch: 23
Epoch: 24
Epoch: 25
Epoch: 26
Save Encoder Model...
Traceback (most recent call last):
  File "run.py", line 146, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 102, in train
    outputs = self.model(batch_x, feature_embedding)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
TypeError: forward() takes 2 positional arguments but 3 were given
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs()>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7084
val 1416
test 7084
Start train feature encoder...
Epoch: 0
Epoch: 1
Epoch: 2
Epoch: 3
Epoch: 4
Epoch: 5
Epoch: 6
Epoch: 7
Epoch: 8
Epoch: 9
Epoch: 10
Epoch: 11
Epoch: 12
Epoch: 13
Epoch: 14
Epoch: 15
Epoch: 16
Epoch: 17
Epoch: 18
Epoch: 19
Epoch: 20
Epoch: 21
Epoch: 22
Epoch: 23
Epoch: 24
Epoch: 25
Epoch: 26
Save Encoder Model...
Epoch: 1 cost time: 9.76094651222229
Epoch: 1, Steps: 56 | Train Loss: 0.2200140 Vali Loss: 0.1757216 Test Loss: 0.1797603
Validation loss decreased (inf --> 0.175722).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 9.780278205871582
Epoch: 2, Steps: 56 | Train Loss: 0.2547669 Vali Loss: 0.1656074 Test Loss: 0.1797268
Validation loss decreased (0.175722 --> 0.165607).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 9.759962797164917
Epoch: 3, Steps: 56 | Train Loss: 0.2185555 Vali Loss: 0.1686882 Test Loss: 0.1797036
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 9.781188488006592
Epoch: 4, Steps: 56 | Train Loss: 0.2186387 Vali Loss: 0.1623748 Test Loss: 0.1796867
Validation loss decreased (0.165607 --> 0.162375).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 9.760308504104614
Epoch: 5, Steps: 56 | Train Loss: 0.2168833 Vali Loss: 0.1634652 Test Loss: 0.1796826
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs()<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 7084
train 7084
Threshold : 2.9186248183249908
pred:    (708400,)
gt:      (708400,)
pred:  (708400,)
gt:    (708400,)
Accuracy : 0.9876, Precision : 0.9009, Recall : 0.7874, F-score : 0.8404, AUC : 0.8918
usage: run.py [-h] --is_training IS_TRAINING --model_id MODEL_ID --model MODEL
              --data DATA [--root_path ROOT_PATH] [--data_path DATA_PATH]
              [--features FEATURES] [--target TARGET] [--freq FREQ]
              [--checkpoints CHECKPOINTS] [--seq_len SEQ_LEN]
              [--label_len LABEL_LEN] [--pred_len PRED_LEN]
              [--seasonal_patterns SEASONAL_PATTERNS] [--mask_rate MASK_RATE]
              [--anomaly_ratio ANOMALY_RATIO] [--enc_in ENC_IN]
              [--dec_in DEC_IN] [--c_out C_OUT] [--d_model D_MODEL]
              [--d_ff D_FF] [--moving_avg MOVING_AVG] [--factor FACTOR]
              [--distil] [--dropout DROPOUT] [--embed EMBED]
              [--activation ACTIVATION] [--output_attention]
              [--num_workers NUM_WORKERS] [--itr ITR]
              [--train_epochs TRAIN_EPOCHS] [--batch_size BATCH_SIZE]
              [--patience PATIENCE] [--learning_rate LEARNING_RATE]
              [--des DES] [--loss LOSS] [--lradj LRADJ] [--use_amp]
              [--use_gpu USE_GPU] [--gpu GPU] [--use_multi_gpu]
              [--devices DEVICES]
              [--p_hidden_dims P_HIDDEN_DIMS [P_HIDDEN_DIMS ...]]
              [--p_hidden_layers P_HIDDEN_LAYERS] [--patch_size PATCH_SIZE]
              [--stride STRIDE] [--gpt_layers GPT_LAYERS] [--ln LN]
              [--mlp MLP] [--weight WEIGHT] [--percent PERCENT]
              [--use_skip_embedding USE_SKIP_EMBEDDING]
              [--use_feature_embedding USE_FEATURE_EMBEDDING]
              [--nb_random_samples NB_RANDOM_SAMPLES] [--top_k TOP_K]
              [--prompt_len PROMPT_LEN] [--pool_size POOL_SIZE]
              [--use_prompt_pool USE_PROMPT_POOL] [--visualize VISUALIZE]
run.py: error: unrecognized arguments: --use_prompt_embedding True
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Epoch: 1
Epoch: 2
Epoch: 3
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000004
	speed: 0.1735s/iter; left time: 772.1596s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1708s/iter; left time: 743.3318s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1711s/iter; left time: 727.2606s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1709s/iter; left time: 709.3523s
Epoch: 1 cost time: 78.13925790786743
Epoch: 1, Steps: 455 | Train Loss: 0.0000009 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.9601s/iter; left time: 3836.5065s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1708s/iter; left time: 665.5527s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1708s/iter; left time: 648.2330s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1708s/iter; left time: 631.3973s
Epoch: 2 cost time: 78.08729076385498
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.9596s/iter; left time: 3397.9189s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1708s/iter; left time: 587.8333s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1708s/iter; left time: 570.7364s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1708s/iter; left time: 553.6497s
Epoch: 3 cost time: 78.08261728286743
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.9604s/iter; left time: 2963.8534s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 510.2069s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 493.1189s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1708s/iter; left time: 475.9602s
Epoch: 4 cost time: 78.12664723396301
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.9598s/iter; left time: 2525.2375s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 432.4394s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 415.3638s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1708s/iter; left time: 398.1971s
Epoch: 5 cost time: 78.11489939689636
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.9608s/iter; left time: 2090.6969s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 354.7177s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1708s/iter; left time: 337.4756s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1710s/iter; left time: 320.8656s
Epoch: 6 cost time: 78.12138438224792
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.9601s/iter; left time: 1652.2554s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 277.0929s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 259.9448s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1708s/iter; left time: 242.7459s
Epoch: 7 cost time: 78.12512063980103
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.9604s/iter; left time: 1215.8497s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1710s/iter; left time: 199.3296s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1708s/iter; left time: 182.0846s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 165.0508s
Epoch: 8 cost time: 78.1223292350769
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.9608s/iter; left time: 779.1804s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1711s/iter; left time: 121.6663s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 104.4294s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 87.3238s
Epoch: 9 cost time: 78.16624307632446
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.9605s/iter; left time: 341.9396s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 43.7484s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1709s/iter; left time: 26.6673s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1710s/iter; left time: 9.5781s
Epoch: 10 cost time: 78.16028165817261
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.0028804538819703e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9724, Precision : 0.8670, Recall : 0.8724, F-score : 0.8697, AUC : 0.9283
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=25, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='SMAP', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=25, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0005, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMAP', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMAP', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMAP_GPT4TS_SMAP_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (427617, 25)
train: (135183, 25)
train 135084
test: (427617, 25)
train: (135183, 25)
val 427518
test: (427617, 25)
train: (135183, 25)
test 427518
Start train feature encoder...
Epoch: 0
Epoch: 1
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000003
	speed: 0.1727s/iter; left time: 894.8319s
	iters: 200, epoch: 1 | loss: 0.0000002
	speed: 0.1702s/iter; left time: 864.9575s
	iters: 300, epoch: 1 | loss: 0.0000001
	speed: 0.1705s/iter; left time: 849.4190s
	iters: 400, epoch: 1 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 830.7537s
	iters: 500, epoch: 1 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 813.8797s
	iters: 600, epoch: 1 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 797.2460s
	iters: 700, epoch: 1 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 780.2733s
	iters: 800, epoch: 1 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 763.0263s
	iters: 900, epoch: 1 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 745.8684s
	iters: 1000, epoch: 1 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 729.1458s
Epoch: 1 cost time: 180.1170494556427
Epoch: 1, Steps: 1056 | Train Loss: 0.0000001 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.0000000
	speed: 4.1937s/iter; left time: 17299.0022s
	iters: 200, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 685.0761s
	iters: 300, epoch: 2 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 668.8241s
	iters: 400, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 650.8861s
	iters: 500, epoch: 2 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 634.2252s
	iters: 600, epoch: 2 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 617.2847s
	iters: 700, epoch: 2 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 600.3193s
	iters: 800, epoch: 2 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 583.3749s
	iters: 900, epoch: 2 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 566.2613s
	iters: 1000, epoch: 2 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 549.3557s
Epoch: 2 cost time: 180.11024856567383
Epoch: 2, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 3 | loss: 0.0000000
	speed: 4.1939s/iter; left time: 12871.0307s
	iters: 200, epoch: 3 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 505.8201s
	iters: 300, epoch: 3 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 488.6600s
	iters: 400, epoch: 3 | loss: 0.0000000
	speed: 0.1706s/iter; left time: 472.4697s
	iters: 500, epoch: 3 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 454.8064s
	iters: 600, epoch: 3 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 437.6241s
	iters: 700, epoch: 3 | loss: 0.0000000
	speed: 0.1705s/iter; left time: 420.9149s
	iters: 800, epoch: 3 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 403.4537s
	iters: 900, epoch: 3 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 386.3289s
	iters: 1000, epoch: 3 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 369.4462s
Epoch: 3 cost time: 180.20866751670837
Epoch: 3, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 0.000125
	iters: 100, epoch: 4 | loss: 0.0000000
	speed: 4.1963s/iter; left time: 8447.1782s
	iters: 200, epoch: 4 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 325.9076s
	iters: 300, epoch: 4 | loss: 0.0000000
	speed: 0.1705s/iter; left time: 309.0809s
	iters: 400, epoch: 4 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 291.8364s
	iters: 500, epoch: 4 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 274.8038s
	iters: 600, epoch: 4 | loss: 0.0000000
	speed: 0.1705s/iter; left time: 258.0409s
	iters: 700, epoch: 4 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 240.5933s
	iters: 800, epoch: 4 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 223.5930s
	iters: 900, epoch: 4 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 206.4942s
	iters: 1000, epoch: 4 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 189.5071s
Epoch: 4 cost time: 180.18085551261902
Epoch: 4, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-05
	iters: 100, epoch: 5 | loss: 0.0000000
	speed: 4.1893s/iter; left time: 4009.1501s
	iters: 200, epoch: 5 | loss: 0.0000000
	speed: 0.1695s/iter; left time: 145.2289s
	iters: 300, epoch: 5 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 128.2669s
	iters: 400, epoch: 5 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 111.2516s
	iters: 500, epoch: 5 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 94.3229s
	iters: 600, epoch: 5 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 77.3943s
	iters: 700, epoch: 5 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 60.5503s
	iters: 800, epoch: 5 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 43.5139s
	iters: 900, epoch: 5 | loss: 0.0000000
	speed: 0.1695s/iter; left time: 26.6043s
	iters: 1000, epoch: 5 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 9.6561s
Epoch: 5 cost time: 179.19764828681946
Epoch: 5, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-05
>>>>>>>testing : SMAP_GPT4TS_SMAP_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (427617, 25)
train: (135183, 25)
test 427518
test: (427617, 25)
train: (135183, 25)
train 135084
Threshold : 6.889349566385754e-09
pred:    (42751800,)
gt:      (42751800,)
pred:  (42751800,)
gt:    (42751800,)
Accuracy : 0.9657, Precision : 0.9088, Recall : 0.8135, F-score : 0.8585, AUC : 0.9008
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 494901
test: (449919, 51)
train: (495000, 51)
val 449820
test: (449919, 51)
train: (495000, 51)
test 449820
Start train feature encoder...
Epoch: 0
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000002
	speed: 0.1726s/iter; left time: 3320.1286s
	iters: 200, epoch: 1 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 3256.2143s
	iters: 300, epoch: 1 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 3243.8166s
	iters: 400, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3221.4317s
	iters: 500, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3203.6396s
	iters: 600, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3186.5100s
	iters: 700, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3169.2423s
	iters: 800, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3151.8445s
	iters: 900, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3135.5068s
	iters: 1000, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3116.0686s
	iters: 1100, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3098.8585s
	iters: 1200, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3082.4061s
	iters: 1300, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3065.3404s
	iters: 1400, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3048.4099s
	iters: 1500, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3034.7936s
	iters: 1600, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3015.2313s
	iters: 1700, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2999.0834s
	iters: 1800, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2982.4419s
	iters: 1900, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2964.8119s
	iters: 2000, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2948.0296s
	iters: 2100, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2930.5606s
	iters: 2200, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2914.6252s
	iters: 2300, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2896.8739s
	iters: 2400, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2879.0357s
	iters: 2500, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2862.7219s
	iters: 2600, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2845.9506s
	iters: 2700, epoch: 1 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 2832.3769s
	iters: 2800, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2811.6142s
	iters: 2900, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2795.1496s
	iters: 3000, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2779.1408s
	iters: 3100, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2762.2925s
	iters: 3200, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2743.8096s
	iters: 3300, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2726.8897s
	iters: 3400, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2709.6306s
	iters: 3500, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2692.7320s
	iters: 3600, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2675.2991s
	iters: 3700, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2658.1071s
	iters: 3800, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2641.5492s
Epoch: 1 cost time: 657.9434237480164
Epoch: 1, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000000
	speed: 4.4438s/iter; left time: 68296.7765s
	iters: 200, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2597.6675s
	iters: 300, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2578.4205s
	iters: 400, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2562.1104s
	iters: 500, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2545.4392s
	iters: 600, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2528.1976s
	iters: 700, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2511.3980s
	iters: 800, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2495.5652s
	iters: 900, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2477.7020s
	iters: 1000, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2460.6956s
	iters: 1100, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2443.1329s
	iters: 1200, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2426.6872s
	iters: 1300, epoch: 2 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 2413.9500s
	iters: 1400, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2393.7671s
	iters: 1500, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2376.6629s
	iters: 1600, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2359.0511s
	iters: 1700, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2342.3470s
	iters: 1800, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2325.2168s
	iters: 1900, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2309.3169s
	iters: 2000, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2292.6413s
	iters: 2100, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2275.1313s
	iters: 2200, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2258.0843s
	iters: 2300, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2239.4491s
	iters: 2400, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2222.4676s
	iters: 2500, epoch: 2 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 2208.2101s
	iters: 2600, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2188.4281s
	iters: 2700, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2172.5289s
	iters: 2800, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2154.6764s
	iters: 2900, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2137.7817s
	iters: 3000, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2121.8543s
	iters: 3100, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2104.4234s
	iters: 3200, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2087.3963s
	iters: 3300, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2070.6898s
	iters: 3400, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2052.6982s
	iters: 3500, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2035.9147s
	iters: 3600, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2019.2263s
	iters: 3700, epoch: 2 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 2005.0223s
	iters: 3800, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1985.0336s
Epoch: 2 cost time: 658.1532011032104
Epoch: 2, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000000
	speed: 4.4369s/iter; left time: 51033.6031s
	iters: 200, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1937.2586s
	iters: 300, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1921.0908s
	iters: 400, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1903.0202s
	iters: 500, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1885.3337s
	iters: 600, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1871.6004s
	iters: 700, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1854.8450s
	iters: 800, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1837.4209s
	iters: 900, epoch: 3 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 1821.1769s
	iters: 1000, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1803.6728s
	iters: 1100, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1786.7032s
	iters: 1200, epoch: 3 | loss: 0.0000000
	speed: 0.1705s/iter; left time: 1773.3168s
	iters: 1300, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1752.4491s
	iters: 1400, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1734.3818s
	iters: 1500, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1717.8158s
	iters: 1600, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1701.1820s
	iters: 1700, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1683.4965s
	iters: 1800, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1666.5340s
	iters: 1900, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1649.3966s
	iters: 2000, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1632.5019s
	iters: 2100, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1615.8164s
	iters: 2200, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1598.7122s
	iters: 2300, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1582.3259s
	iters: 2400, epoch: 3 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 1566.6024s
	iters: 2500, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1548.3985s
	iters: 2600, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1530.8616s
	iters: 2700, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1513.8958s
	iters: 2800, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1497.4442s
	iters: 2900, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1480.0852s
	iters: 3000, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1462.8244s
	iters: 3100, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1445.7980s
	iters: 3200, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1428.9943s
	iters: 3300, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1411.8085s
	iters: 3400, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1394.7317s
	iters: 3500, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1377.6584s
	iters: 3600, epoch: 3 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 1362.3427s
	iters: 3700, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1344.0719s
	iters: 3800, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1326.3831s
Epoch: 3 cost time: 657.9745321273804
Epoch: 3, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000000
	speed: 4.4360s/iter; left time: 33868.8473s
	iters: 200, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1281.2523s
	iters: 300, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1264.1400s
	iters: 400, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1246.7686s
	iters: 500, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1229.8467s
	iters: 600, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1213.1490s
	iters: 700, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1195.2780s
	iters: 800, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1178.3128s
	iters: 900, epoch: 4 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 1164.0663s
	iters: 1000, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1145.2579s
	iters: 1100, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1127.9978s
	iters: 1200, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1110.6219s
	iters: 1300, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1094.2294s
	iters: 1400, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1077.4118s
	iters: 1500, epoch: 4 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 1061.0577s
	iters: 1600, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1043.5347s
	iters: 1700, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1026.1795s
	iters: 1800, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1009.2155s
	iters: 1900, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 992.1167s
	iters: 2000, epoch: 4 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 976.3332s
	iters: 2100, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 958.3750s
	iters: 2200, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 941.2940s
	iters: 2300, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 924.2044s
	iters: 2400, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 906.9155s
	iters: 2500, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 889.7898s
	iters: 2600, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 872.9883s
	iters: 2700, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 855.9583s
	iters: 2800, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 839.0570s
	iters: 2900, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 821.8535s
	iters: 3000, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 804.9672s
	iters: 3100, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 788.0229s
	iters: 3200, epoch: 4 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 772.0814s
	iters: 3300, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 754.1976s
	iters: 3400, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 737.2020s
	iters: 3500, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 720.1629s
	iters: 3600, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 703.0589s
	iters: 3700, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 686.0135s
	iters: 3800, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 668.8741s
Epoch: 4 cost time: 657.8453848361969
Epoch: 4, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000000
	speed: 4.4385s/iter; left time: 16724.2636s
	iters: 200, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 623.3069s
	iters: 300, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 606.5677s
	iters: 400, epoch: 5 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 590.3122s
	iters: 500, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 572.4022s
	iters: 600, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 555.4640s
	iters: 700, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 538.3968s
	iters: 800, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 521.5248s
	iters: 900, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 504.3643s
	iters: 1000, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 487.5918s
	iters: 1100, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 470.0909s
	iters: 1200, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 453.3928s
	iters: 1300, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 436.4585s
	iters: 1400, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 419.4039s
	iters: 1500, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 402.4518s
	iters: 1600, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 385.8707s
	iters: 1700, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 368.4714s
	iters: 1800, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 351.5181s
	iters: 1900, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 334.4960s
	iters: 2000, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 317.6062s
	iters: 2100, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 300.6443s
	iters: 2200, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 283.5050s
	iters: 2300, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 266.5547s
	iters: 2400, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 249.7025s
	iters: 2500, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 232.5923s
	iters: 2600, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 215.5653s
	iters: 2700, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 198.5807s
	iters: 2800, epoch: 5 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 181.7641s
	iters: 2900, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 164.6313s
	iters: 3000, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 147.6097s
	iters: 3100, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 130.6244s
	iters: 3200, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 113.6018s
	iters: 3300, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 96.6082s
	iters: 3400, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 79.6085s
	iters: 3500, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 62.5810s
	iters: 3600, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 45.5610s
	iters: 3700, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 28.5584s
	iters: 3800, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 11.5637s
Epoch: 5 cost time: 657.7484979629517
Epoch: 5, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 449820
test: (449919, 51)
train: (495000, 51)
train 494901
Threshold : 2.0767055648462938e-08
pred:    (44982000,)
gt:      (44982000,)
pred:  (44982000,)
gt:    (44982000,)
Accuracy : 0.9700, Precision : 0.9061, Recall : 0.8400, F-score : 0.8718, AUC : 0.9140
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
Traceback (most recent call last):
  File "run.py", line 151, in <module>
    exp = Exp(args)  # set experiments
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 25, in __init__
    super(Exp_Anomaly_Detection, self).__init__(args)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_basic.py", line 28, in __init__
    self.model, self.feature_encoder = self._build_model()
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 28, in _build_model
    model = self.model_dict[self.args.model].Model(self.args).float()
  File "/root/taowei/Project/LLM/MADLLM/models/GPT4TS.py", line 69, in __init__
    self.gpt2.to(device=device)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/transformers/modeling_utils.py", line 1902, in to
    return super().to(*args, **kwargs)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 673, in to
    return self._apply(convert)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 409, in _apply
    param_applied = fn(param)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 671, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
KeyboardInterrupt
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 494901
test: (449919, 51)
train: (495000, 51)
val 449820
test: (449919, 51)
train: (495000, 51)
test 449820
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Batch: 455
Batch: 456
Batch: 457
Batch: 458
Batch: 459
Batch: 460
Batch: 461
Batch: 462
Batch: 463
Batch: 464
Batch: 465
Batch: 466
Batch: 467
Batch: 468
Batch: 469
Batch: 470
Batch: 471
Batch: 472
Batch: 473
Batch: 474
Batch: 475
Batch: 476
Batch: 477
Batch: 478
Batch: 479
Batch: 480
Batch: 481
Batch: 482
Batch: 483
Batch: 484
Batch: 485
Batch: 486
Batch: 487
Batch: 488
Batch: 489
Batch: 490
Batch: 491
Batch: 492
Batch: 493
Batch: 494
Batch: 495
Batch: 496
Batch: 497
Batch: 498
Batch: 499
Batch: 500
Batch: 501
Batch: 502
Batch: 503
Batch: 504
Batch: 505
Batch: 506
Batch: 507
Batch: 508
Batch: 509
Batch: 510
Batch: 511
Batch: 512
Batch: 513
Batch: 514
Batch: 515
Batch: 516
Batch: 517
Batch: 518
Batch: 519
Batch: 520
Batch: 521
Batch: 522
Batch: 523
Batch: 524
Batch: 525
Batch: 526
Batch: 527
Batch: 528
Batch: 529
Batch: 530
Batch: 531
Batch: 532
Batch: 533
Batch: 534
Batch: 535
Batch: 536
Batch: 537
Batch: 538
Batch: 539
Batch: 540
Batch: 541
Batch: 542
Batch: 543
Batch: 544
Batch: 545
Batch: 546
Batch: 547
Batch: 548
Batch: 549
Batch: 550
Batch: 551
Batch: 552
Batch: 553
Batch: 554
Batch: 555
Batch: 556
Batch: 557
Batch: 558
Batch: 559
Batch: 560
Batch: 561
Batch: 562
Batch: 563
Batch: 564
Batch: 565
Batch: 566
Batch: 567
Batch: 568
Batch: 569
Batch: 570
Batch: 571
Batch: 572
Batch: 573
Batch: 574
Batch: 575
Batch: 576
Batch: 577
Batch: 578
Batch: 579
Batch: 580
Batch: 581
Batch: 582
Batch: 583
Batch: 584
Batch: 585
Batch: 586
Batch: 587
Batch: 588
Batch: 589
Batch: 590
Batch: 591
Batch: 592
Batch: 593
Batch: 594
Batch: 595
Batch: 596
Batch: 597
Batch: 598
Batch: 599
Batch: 600
Batch: 601
Batch: 602
Batch: 603
Batch: 604
Batch: 605
Batch: 606
Batch: 607
Batch: 608
Batch: 609
Batch: 610
Batch: 611
Batch: 612
Batch: 613
Batch: 614
Batch: 615
Batch: 616
Batch: 617
Batch: 618
Batch: 619
Batch: 620
Batch: 621
Batch: 622
Batch: 623
Batch: 624
Batch: 625
Batch: 626
Batch: 627
Batch: 628
Batch: 629
Batch: 630
Batch: 631
Batch: 632
Batch: 633
Batch: 634
Batch: 635
Batch: 636
Batch: 637
Batch: 638
Batch: 639
Batch: 640
Batch: 641
Batch: 642
Batch: 643
Batch: 644
Batch: 645
Batch: 646
Batch: 647
Batch: 648
Batch: 649
Batch: 650
Batch: 651
Batch: 652
Batch: 653
Batch: 654
Batch: 655
Batch: 656
Batch: 657
Batch: 658
Batch: 659
Batch: 660
Batch: 661
Batch: 662
Batch: 663
Batch: 664
Batch: 665
Batch: 666
Batch: 667
Batch: 668
Batch: 669
Batch: 670
Batch: 671
Batch: 672
Batch: 673
Batch: 674
Batch: 675
Batch: 676
Batch: 677
Batch: 678
Batch: 679
Batch: 680
Batch: 681
Batch: 682
Batch: 683
Batch: 684
Batch: 685
Batch: 686
Batch: 687
Batch: 688
Batch: 689
Batch: 690
Batch: 691
Batch: 692
Batch: 693
Batch: 694
Batch: 695
Batch: 696
Batch: 697
Batch: 698
Batch: 699
Batch: 700
Batch: 701
Batch: 702
Batch: 703
Batch: 704
Batch: 705
Batch: 706
Batch: 707
Batch: 708
Batch: 709
Batch: 710
Batch: 711
Batch: 712
Batch: 713
Batch: 714
Batch: 715
Batch: 716
Batch: 717
Batch: 718
Batch: 719
Batch: 720
Batch: 721
Batch: 722
Batch: 723
Batch: 724
Batch: 725
Batch: 726
Batch: 727
Batch: 728
Batch: 729
Batch: 730
Batch: 731
Batch: 732
Batch: 733
Batch: 734
Batch: 735
Batch: 736
Batch: 737
Batch: 738
Batch: 739
Batch: 740
Batch: 741
Batch: 742
Batch: 743
Batch: 744
Batch: 745
Batch: 746
Batch: 747
Batch: 748
Batch: 749
Batch: 750
Batch: 751
Batch: 752
Batch: 753
Batch: 754
Batch: 755
Batch: 756
Batch: 757
Batch: 758
Batch: 759
Batch: 760
Batch: 761
Batch: 762
Batch: 763
Batch: 764
Batch: 765
Batch: 766
Batch: 767
Batch: 768
Batch: 769
Batch: 770
Batch: 771
Batch: 772
Batch: 773
Batch: 774
Batch: 775
Batch: 776
Batch: 777
Batch: 778
Batch: 779
Batch: 780
Batch: 781
Batch: 782
Batch: 783
Batch: 784
Batch: 785
Batch: 786
Batch: 787
Batch: 788
Batch: 789
Batch: 790
Batch: 791
Batch: 792
Batch: 793
Batch: 794
Batch: 795
Batch: 796
Batch: 797
Batch: 798
Batch: 799
Batch: 800
Batch: 801
Batch: 802
Batch: 803
Batch: 804
Batch: 805
Batch: 806
Batch: 807
Batch: 808
Batch: 809
Batch: 810
Batch: 811
Batch: 812
Batch: 813
Batch: 814
Batch: 815
Batch: 816
Batch: 817
Batch: 818
Batch: 819
Batch: 820
Batch: 821
Batch: 822
Batch: 823
Batch: 824
Batch: 825
Batch: 826
Batch: 827
Batch: 828
Batch: 829
Batch: 830
Batch: 831
Batch: 832
Batch: 833
Batch: 834
Batch: 835
Batch: 836
Batch: 837
Batch: 838
Batch: 839
Batch: 840
Batch: 841
Batch: 842
Batch: 843
Batch: 844
Batch: 845
Batch: 846
Batch: 847
Batch: 848
Batch: 849
Batch: 850
Batch: 851
Batch: 852
Batch: 853
Batch: 854
Batch: 855
Batch: 856
Batch: 857
Batch: 858
Batch: 859
Batch: 860
Batch: 861
Batch: 862
Batch: 863
Batch: 864
Batch: 865
Batch: 866
Batch: 867
Batch: 868
Batch: 869
Batch: 870
Batch: 871
Batch: 872
Batch: 873
Batch: 874
Batch: 875
Batch: 876
Batch: 877
Batch: 878
Batch: 879
Batch: 880
Batch: 881
Batch: 882
Batch: 883
Batch: 884
Batch: 885
Batch: 886
Batch: 887
Batch: 888
Batch: 889
Batch: 890
Batch: 891
Batch: 892
Batch: 893
Batch: 894
Batch: 895
Batch: 896
Batch: 897
Batch: 898
Batch: 899
Batch: 900
Batch: 901
Batch: 902
Batch: 903
Batch: 904
Batch: 905
Batch: 906
Batch: 907
Batch: 908
Batch: 909
Batch: 910
Batch: 911
Batch: 912
Batch: 913
Batch: 914
Batch: 915
Batch: 916
Batch: 917
Batch: 918
Batch: 919
Batch: 920
Batch: 921
Batch: 922
Batch: 923
Batch: 924
Batch: 925
Batch: 926
Batch: 927
Batch: 928
Batch: 929
Batch: 930
Batch: 931
Batch: 932
Batch: 933
Batch: 934
Batch: 935
Batch: 936
Batch: 937
Batch: 938
Batch: 939
Batch: 940
Batch: 941
Batch: 942
Batch: 943
Batch: 944
Batch: 945
Batch: 946
Batch: 947
Batch: 948
Batch: 949
Batch: 950
Batch: 951
Batch: 952
Batch: 953
Batch: 954
Batch: 955
Batch: 956
Batch: 957
Batch: 958
Batch: 959
Batch: 960
Batch: 961
Batch: 962
Batch: 963
Batch: 964
Batch: 965
Batch: 966
Batch: 967
Batch: 968
Batch: 969
Batch: 970
Batch: 971
Batch: 972
Batch: 973
Batch: 974
Batch: 975
Batch: 976
Batch: 977
Batch: 978
Batch: 979
Batch: 980
Batch: 981
Batch: 982
Batch: 983
Batch: 984
Batch: 985
Batch: 986
Batch: 987
Batch: 988
Batch: 989
Batch: 990
Batch: 991
Batch: 992
Batch: 993
Batch: 994
Batch: 995
Batch: 996
Batch: 997
Batch: 998
Batch: 999
Batch: 1000
Batch: 1001
Batch: 1002
Batch: 1003
Batch: 1004
Batch: 1005
Batch: 1006
Batch: 1007
Batch: 1008
Batch: 1009
Batch: 1010
Batch: 1011
Batch: 1012
Batch: 1013
Batch: 1014
Batch: 1015
Batch: 1016
Batch: 1017
Batch: 1018
Batch: 1019
Batch: 1020
Batch: 1021
Batch: 1022
Batch: 1023
Batch: 1024
Batch: 1025
Batch: 1026
Batch: 1027
Batch: 1028
Batch: 1029
Batch: 1030
Batch: 1031
Batch: 1032
Batch: 1033
Batch: 1034
Batch: 1035
Batch: 1036
Batch: 1037
Batch: 1038
Batch: 1039
Batch: 1040
Batch: 1041
Batch: 1042
Batch: 1043
Batch: 1044
Batch: 1045
Batch: 1046
Batch: 1047
Batch: 1048
Batch: 1049
Batch: 1050
Batch: 1051
Batch: 1052
Batch: 1053
Batch: 1054
Batch: 1055
Batch: 1056
Batch: 1057
Batch: 1058
Batch: 1059
Batch: 1060
Batch: 1061
Batch: 1062
Batch: 1063
Batch: 1064
Batch: 1065
Batch: 1066
Batch: 1067
Batch: 1068
Batch: 1069
Batch: 1070
Batch: 1071
Batch: 1072
Batch: 1073
Batch: 1074
Batch: 1075
Batch: 1076
Batch: 1077
Batch: 1078
Batch: 1079
Batch: 1080
Batch: 1081
Batch: 1082
Batch: 1083
Batch: 1084
Batch: 1085
Batch: 1086
Batch: 1087
Batch: 1088
Batch: 1089
Batch: 1090
Batch: 1091
Batch: 1092
Batch: 1093
Batch: 1094
Batch: 1095
Batch: 1096
Batch: 1097
Batch: 1098
Batch: 1099
Batch: 1100
Batch: 1101
Batch: 1102
Batch: 1103
Batch: 1104
Batch: 1105
Batch: 1106
Batch: 1107
Batch: 1108
Batch: 1109
Batch: 1110
Batch: 1111
Batch: 1112
Batch: 1113
Batch: 1114
Batch: 1115
Batch: 1116
Batch: 1117
Batch: 1118
Batch: 1119
Batch: 1120
Batch: 1121
Batch: 1122
Batch: 1123
Batch: 1124
Batch: 1125
Batch: 1126
Batch: 1127
Batch: 1128
Batch: 1129
Batch: 1130
Batch: 1131
Batch: 1132
Batch: 1133
Batch: 1134
Batch: 1135
Batch: 1136
Batch: 1137
Batch: 1138
Batch: 1139
Batch: 1140
Batch: 1141
Batch: 1142
Batch: 1143
Batch: 1144
Batch: 1145
Batch: 1146
Batch: 1147
Batch: 1148
Batch: 1149
Batch: 1150
Batch: 1151
Batch: 1152
Batch: 1153
Batch: 1154
Batch: 1155
Batch: 1156
Batch: 1157
Batch: 1158
Batch: 1159
Batch: 1160
Batch: 1161
Batch: 1162
Batch: 1163
Batch: 1164
Batch: 1165
Batch: 1166
Batch: 1167
Batch: 1168
Batch: 1169
Batch: 1170
Batch: 1171
Batch: 1172
Batch: 1173
Batch: 1174
Batch: 1175
Batch: 1176
Batch: 1177
Batch: 1178
Batch: 1179
Batch: 1180
Batch: 1181
Batch: 1182
Batch: 1183
Batch: 1184
Batch: 1185
Batch: 1186
Batch: 1187
Batch: 1188
Batch: 1189
Batch: 1190
Batch: 1191
Batch: 1192
Batch: 1193
Batch: 1194
Batch: 1195
Batch: 1196
Batch: 1197
Batch: 1198
Batch: 1199
Batch: 1200
Batch: 1201
Batch: 1202
Batch: 1203
Batch: 1204
Batch: 1205
Batch: 1206
Batch: 1207
Batch: 1208
Batch: 1209
Batch: 1210
Batch: 1211
Batch: 1212
Batch: 1213
Batch: 1214
Batch: 1215
Batch: 1216
Batch: 1217
Batch: 1218
Batch: 1219
Batch: 1220
Batch: 1221
Batch: 1222
Batch: 1223
Batch: 1224
Batch: 1225
Batch: 1226
Batch: 1227
Batch: 1228
Batch: 1229
Batch: 1230
Batch: 1231
Batch: 1232
Batch: 1233
Batch: 1234
Batch: 1235
Batch: 1236
Batch: 1237
Batch: 1238
Batch: 1239
Batch: 1240
Batch: 1241
Batch: 1242
Batch: 1243
Batch: 1244
Batch: 1245
Batch: 1246
Batch: 1247
Batch: 1248
Batch: 1249
Batch: 1250
Batch: 1251
Batch: 1252
Batch: 1253
Batch: 1254
Batch: 1255
Batch: 1256
Batch: 1257
Batch: 1258
Batch: 1259
Batch: 1260
Batch: 1261
Batch: 1262
Batch: 1263
Batch: 1264
Batch: 1265
Batch: 1266
Batch: 1267
Batch: 1268
Batch: 1269
Batch: 1270
Batch: 1271
Batch: 1272
Batch: 1273
Batch: 1274
Batch: 1275
Batch: 1276
Batch: 1277
Batch: 1278
Batch: 1279
Batch: 1280
Batch: 1281
Batch: 1282
Batch: 1283
Batch: 1284
Batch: 1285
Batch: 1286
Batch: 1287
Batch: 1288
Batch: 1289
Batch: 1290
Batch: 1291
Batch: 1292
Batch: 1293
Batch: 1294
Batch: 1295
Batch: 1296
Batch: 1297
Batch: 1298
Batch: 1299
Batch: 1300
Batch: 1301
Batch: 1302
Batch: 1303
Batch: 1304
Batch: 1305
Batch: 1306
Batch: 1307
Batch: 1308
Batch: 1309
Batch: 1310
Batch: 1311
Batch: 1312
Batch: 1313
Batch: 1314
Batch: 1315
Batch: 1316
Batch: 1317
Batch: 1318
Batch: 1319
Batch: 1320
Batch: 1321
Batch: 1322
Batch: 1323
Batch: 1324
Batch: 1325
Batch: 1326
Batch: 1327
Batch: 1328
Batch: 1329
Batch: 1330
Batch: 1331
Batch: 1332
Batch: 1333
Batch: 1334
Batch: 1335
Batch: 1336
Batch: 1337
Batch: 1338
Batch: 1339
Batch: 1340
Batch: 1341
Batch: 1342
Batch: 1343
Batch: 1344
Batch: 1345
Batch: 1346
Batch: 1347
Batch: 1348
Batch: 1349
Batch: 1350
Batch: 1351
Batch: 1352
Batch: 1353
Batch: 1354
Batch: 1355
Batch: 1356
Batch: 1357
Batch: 1358
Batch: 1359
Batch: 1360
Batch: 1361
Batch: 1362
Batch: 1363
Batch: 1364
Batch: 1365
Batch: 1366
Batch: 1367
Batch: 1368
Batch: 1369
Batch: 1370
Batch: 1371
Batch: 1372
Batch: 1373
Batch: 1374
Batch: 1375
Batch: 1376
Batch: 1377
Batch: 1378
Batch: 1379
Batch: 1380
Batch: 1381
Batch: 1382
Batch: 1383
Batch: 1384
Batch: 1385
Batch: 1386
Batch: 1387
Batch: 1388
Batch: 1389
Batch: 1390
Batch: 1391
Batch: 1392
Batch: 1393
Batch: 1394
Batch: 1395
Batch: 1396
Batch: 1397
Batch: 1398
Batch: 1399
Batch: 1400
Batch: 1401
Batch: 1402
Batch: 1403
Batch: 1404
Batch: 1405
Batch: 1406
Batch: 1407
Batch: 1408
Batch: 1409
Batch: 1410
Batch: 1411
Batch: 1412
Batch: 1413
Batch: 1414
Batch: 1415
Batch: 1416
Batch: 1417
Batch: 1418
Batch: 1419
Batch: 1420
Batch: 1421
Batch: 1422
Batch: 1423
Batch: 1424
Batch: 1425
Batch: 1426
Batch: 1427
Batch: 1428
Batch: 1429
Batch: 1430
Batch: 1431
Batch: 1432
Batch: 1433
Batch: 1434
Batch: 1435
Batch: 1436
Batch: 1437
Batch: 1438
Batch: 1439
Batch: 1440
Batch: 1441
Batch: 1442
Batch: 1443
Batch: 1444
Batch: 1445
Batch: 1446
Batch: 1447
Batch: 1448
Batch: 1449
Batch: 1450
Batch: 1451
Batch: 1452
Batch: 1453
Batch: 1454
Batch: 1455
Batch: 1456
Batch: 1457
Batch: 1458
Batch: 1459
Batch: 1460
Batch: 1461
Batch: 1462
Batch: 1463
Batch: 1464
Batch: 1465
Batch: 1466
Batch: 1467
Batch: 1468
Batch: 1469
Batch: 1470
Batch: 1471
Batch: 1472
Batch: 1473
Batch: 1474
Batch: 1475
Batch: 1476
Batch: 1477
Batch: 1478
Batch: 1479
Batch: 1480
Batch: 1481
Batch: 1482
Batch: 1483
Batch: 1484
Batch: 1485
Batch: 1486
Batch: 1487
Batch: 1488
Batch: 1489
Batch: 1490
Batch: 1491
Batch: 1492
Batch: 1493
Batch: 1494
Batch: 1495
Batch: 1496
Batch: 1497
Batch: 1498
Batch: 1499
Batch: 1500
Batch: 1501
Batch: 1502
Batch: 1503
Batch: 1504
Batch: 1505
Batch: 1506
Batch: 1507
Batch: 1508
Batch: 1509
Batch: 1510
Batch: 1511
Batch: 1512
Batch: 1513
Batch: 1514
Batch: 1515
Batch: 1516
Batch: 1517
Batch: 1518
Batch: 1519
Batch: 1520
Batch: 1521
Batch: 1522
Batch: 1523
Batch: 1524
Batch: 1525
Batch: 1526
Batch: 1527
Batch: 1528
Batch: 1529
Batch: 1530
Batch: 1531
Batch: 1532
Batch: 1533
Batch: 1534
Batch: 1535
Batch: 1536
Batch: 1537
Batch: 1538
Batch: 1539
Batch: 1540
Batch: 1541
Batch: 1542
Batch: 1543
Batch: 1544
Batch: 1545
Batch: 1546
Batch: 1547
Batch: 1548
Batch: 1549
Batch: 1550
Batch: 1551
Batch: 1552
Batch: 1553
Batch: 1554
Batch: 1555
Batch: 1556
Batch: 1557
Batch: 1558
Batch: 1559
Batch: 1560
Batch: 1561
Batch: 1562
Batch: 1563
Batch: 1564
Batch: 1565
Batch: 1566
Batch: 1567
Batch: 1568
Batch: 1569
Batch: 1570
Batch: 1571
Batch: 1572
Batch: 1573
Batch: 1574
Batch: 1575
Batch: 1576
Batch: 1577
Batch: 1578
Batch: 1579
Batch: 1580
Batch: 1581
Batch: 1582
Batch: 1583
Batch: 1584
Batch: 1585
Batch: 1586
Batch: 1587
Batch: 1588
Batch: 1589
Batch: 1590
Batch: 1591
Batch: 1592
Batch: 1593
Batch: 1594
Batch: 1595
Batch: 1596
Batch: 1597
Batch: 1598
Batch: 1599
Batch: 1600
Batch: 1601
Batch: 1602
Batch: 1603
Batch: 1604
Batch: 1605
Batch: 1606
Batch: 1607
Batch: 1608
Batch: 1609
Batch: 1610
Batch: 1611
Batch: 1612
Batch: 1613
Batch: 1614
Batch: 1615
Batch: 1616
Batch: 1617
Batch: 1618
Batch: 1619
Batch: 1620
Batch: 1621
Batch: 1622
Batch: 1623
Batch: 1624
Batch: 1625
Batch: 1626
Batch: 1627
Batch: 1628
Batch: 1629
Batch: 1630
Batch: 1631
Batch: 1632
Batch: 1633
Batch: 1634
Batch: 1635
Batch: 1636
Batch: 1637
Batch: 1638
Batch: 1639
Batch: 1640
Batch: 1641
Batch: 1642
Batch: 1643
Batch: 1644
Batch: 1645
Batch: 1646
Batch: 1647
Batch: 1648
Batch: 1649
Batch: 1650
Batch: 1651
Batch: 1652
Batch: 1653
Batch: 1654
Batch: 1655
Batch: 1656
Batch: 1657
Batch: 1658
Batch: 1659
Batch: 1660
Batch: 1661
Batch: 1662
Batch: 1663
Batch: 1664
Batch: 1665
Batch: 1666
Batch: 1667
Batch: 1668
Batch: 1669
Batch: 1670
Batch: 1671
Batch: 1672
Batch: 1673
Batch: 1674
Batch: 1675
Batch: 1676
Batch: 1677
Batch: 1678
Batch: 1679
Batch: 1680
Batch: 1681
Batch: 1682
Batch: 1683
Batch: 1684
Batch: 1685
Batch: 1686
Batch: 1687
Batch: 1688
Batch: 1689
Batch: 1690
Batch: 1691
Batch: 1692
Batch: 1693
Batch: 1694
Batch: 1695
Batch: 1696
Batch: 1697
Batch: 1698
Batch: 1699
Batch: 1700
Batch: 1701
Batch: 1702
Batch: 1703
Batch: 1704
Batch: 1705
Batch: 1706
Batch: 1707
Batch: 1708
Batch: 1709
Batch: 1710
Batch: 1711
Batch: 1712
Batch: 1713
Batch: 1714
Batch: 1715
Batch: 1716
Batch: 1717
Batch: 1718
Batch: 1719
Batch: 1720
Batch: 1721
Batch: 1722
Batch: 1723
Batch: 1724
Batch: 1725
Batch: 1726
Batch: 1727
Batch: 1728
Batch: 1729
Batch: 1730
Batch: 1731
Batch: 1732
Batch: 1733
Batch: 1734
Batch: 1735
Batch: 1736
Batch: 1737
Batch: 1738
Batch: 1739
Batch: 1740
Batch: 1741
Batch: 1742
Batch: 1743
Batch: 1744
Batch: 1745
Batch: 1746
Batch: 1747
Batch: 1748
Batch: 1749
Batch: 1750
Batch: 1751
Batch: 1752
Batch: 1753
Batch: 1754
Batch: 1755
Batch: 1756
Batch: 1757
Batch: 1758
Batch: 1759
Batch: 1760
Batch: 1761
Batch: 1762
Batch: 1763
Batch: 1764
Batch: 1765
Batch: 1766
Batch: 1767
Batch: 1768
Batch: 1769
Batch: 1770
Batch: 1771
Batch: 1772
Batch: 1773
Batch: 1774
Batch: 1775
Batch: 1776
Batch: 1777
Batch: 1778
Batch: 1779
Batch: 1780
Batch: 1781
Batch: 1782
Batch: 1783
Batch: 1784
Batch: 1785
Batch: 1786
Batch: 1787
Batch: 1788
Batch: 1789
Batch: 1790
Batch: 1791
Batch: 1792
Batch: 1793
Batch: 1794
Batch: 1795
Batch: 1796
Batch: 1797
Batch: 1798
Batch: 1799
Batch: 1800
Batch: 1801
Batch: 1802
Batch: 1803
Batch: 1804
Batch: 1805
Batch: 1806
Batch: 1807
Batch: 1808
Batch: 1809
Batch: 1810
Batch: 1811
Batch: 1812
Batch: 1813
Batch: 1814
Batch: 1815
Batch: 1816
Batch: 1817
Batch: 1818
Batch: 1819
Batch: 1820
Batch: 1821
Batch: 1822
Batch: 1823
Batch: 1824
Batch: 1825
Batch: 1826
Batch: 1827
Batch: 1828
Batch: 1829
Batch: 1830
Batch: 1831
Batch: 1832
Batch: 1833
Batch: 1834
Batch: 1835
Batch: 1836
Batch: 1837
Batch: 1838
Batch: 1839
Batch: 1840
Batch: 1841
Batch: 1842
Batch: 1843
Batch: 1844
Batch: 1845
Batch: 1846
Batch: 1847
Batch: 1848
Batch: 1849
Batch: 1850
Batch: 1851
Batch: 1852
Batch: 1853
Batch: 1854
Batch: 1855
Batch: 1856
Batch: 1857
Batch: 1858
Batch: 1859
Batch: 1860
Batch: 1861
Batch: 1862
Batch: 1863
Batch: 1864
Batch: 1865
Batch: 1866
Batch: 1867
Batch: 1868
Batch: 1869
Batch: 1870
Batch: 1871
Batch: 1872
Batch: 1873
Batch: 1874
Batch: 1875
Batch: 1876
Batch: 1877
Batch: 1878
Batch: 1879
Batch: 1880
Batch: 1881
Batch: 1882
Batch: 1883
Batch: 1884
Batch: 1885
Batch: 1886
Batch: 1887
Batch: 1888
Batch: 1889
Batch: 1890
Batch: 1891
Batch: 1892
Batch: 1893
Batch: 1894
Batch: 1895
Batch: 1896
Batch: 1897
Batch: 1898
Batch: 1899
Batch: 1900
Batch: 1901
Batch: 1902
Batch: 1903
Batch: 1904
Batch: 1905
Batch: 1906
Batch: 1907
Batch: 1908
Batch: 1909
Batch: 1910
Batch: 1911
Batch: 1912
Batch: 1913
Batch: 1914
Batch: 1915
Batch: 1916
Batch: 1917
Batch: 1918
Batch: 1919
Batch: 1920
Batch: 1921
Batch: 1922
Batch: 1923
Batch: 1924
Batch: 1925
Batch: 1926
Batch: 1927
Batch: 1928
Batch: 1929
Batch: 1930
Batch: 1931
Batch: 1932
Batch: 1933
Batch: 1934
Batch: 1935
Batch: 1936
Batch: 1937
Batch: 1938
Batch: 1939
Batch: 1940
Batch: 1941
Batch: 1942
Batch: 1943
Batch: 1944
Batch: 1945
Batch: 1946
Batch: 1947
Batch: 1948
Batch: 1949
Batch: 1950
Batch: 1951
Batch: 1952
Batch: 1953
Batch: 1954
Batch: 1955
Batch: 1956
Batch: 1957
Batch: 1958
Batch: 1959
Batch: 1960
Batch: 1961
Batch: 1962
Batch: 1963
Batch: 1964
Batch: 1965
Batch: 1966
Batch: 1967
Batch: 1968
Batch: 1969
Batch: 1970
Batch: 1971
Batch: 1972
Batch: 1973
Batch: 1974
Batch: 1975
Batch: 1976
Batch: 1977
Batch: 1978
Batch: 1979
Batch: 1980
Batch: 1981
Batch: 1982
Batch: 1983
Batch: 1984
Batch: 1985
Batch: 1986
Batch: 1987
Batch: 1988
Batch: 1989
Batch: 1990
Batch: 1991
Batch: 1992
Batch: 1993
Batch: 1994
Batch: 1995
Batch: 1996
Batch: 1997
Batch: 1998
Batch: 1999
Batch: 2000
Batch: 2001
Batch: 2002
Batch: 2003
Batch: 2004
Batch: 2005
Batch: 2006
Batch: 2007
Batch: 2008
Batch: 2009
Batch: 2010
Batch: 2011
Batch: 2012
Batch: 2013
Batch: 2014
Batch: 2015
Batch: 2016
Batch: 2017
Batch: 2018
Batch: 2019
Batch: 2020
Batch: 2021
Batch: 2022
Batch: 2023
Batch: 2024
Batch: 2025
Batch: 2026
Batch: 2027
Batch: 2028
Batch: 2029
Batch: 2030
Batch: 2031
Batch: 2032
Batch: 2033
Batch: 2034
Batch: 2035
Batch: 2036
Batch: 2037
Batch: 2038
Batch: 2039
Batch: 2040
Batch: 2041
Batch: 2042
Batch: 2043
Batch: 2044
Batch: 2045
Batch: 2046
Batch: 2047
Batch: 2048
Batch: 2049
Batch: 2050
Batch: 2051
Batch: 2052
Batch: 2053
Batch: 2054
Batch: 2055
Batch: 2056
Batch: 2057
Batch: 2058
Batch: 2059
Batch: 2060
Batch: 2061
Batch: 2062
Batch: 2063
Batch: 2064
Batch: 2065
Batch: 2066
Batch: 2067
Batch: 2068
Batch: 2069
Batch: 2070
Batch: 2071
Batch: 2072
Batch: 2073
Batch: 2074
Batch: 2075
Batch: 2076
Batch: 2077
Batch: 2078
Batch: 2079
Batch: 2080
Batch: 2081
Batch: 2082
Batch: 2083
Batch: 2084
Batch: 2085
Batch: 2086
Batch: 2087
Batch: 2088
Batch: 2089
Batch: 2090
Batch: 2091
Batch: 2092
Batch: 2093
Batch: 2094
Batch: 2095
Batch: 2096
Batch: 2097
Batch: 2098
Batch: 2099
Batch: 2100
Batch: 2101
Batch: 2102
Batch: 2103
Batch: 2104
Batch: 2105
Batch: 2106
Batch: 2107
Batch: 2108
Batch: 2109
Batch: 2110
Batch: 2111
Batch: 2112
Batch: 2113
Batch: 2114
Batch: 2115
Batch: 2116
Batch: 2117
Batch: 2118
Batch: 2119
Batch: 2120
Batch: 2121
Batch: 2122
Batch: 2123
Batch: 2124
Batch: 2125
Batch: 2126
Batch: 2127
Batch: 2128
Batch: 2129
Batch: 2130
Batch: 2131
Batch: 2132
Batch: 2133
Batch: 2134
Batch: 2135
Batch: 2136
Batch: 2137
Batch: 2138
Batch: 2139
Batch: 2140
Batch: 2141
Batch: 2142
Batch: 2143
Batch: 2144
Batch: 2145
Batch: 2146
Batch: 2147
Batch: 2148
Batch: 2149
Batch: 2150
Batch: 2151
Batch: 2152
Batch: 2153
Batch: 2154
Batch: 2155
Batch: 2156
Batch: 2157
Batch: 2158
Batch: 2159
Batch: 2160
Batch: 2161
Batch: 2162
Batch: 2163
Batch: 2164
Batch: 2165
Batch: 2166
Batch: 2167
Batch: 2168
Batch: 2169
Batch: 2170
Batch: 2171
Batch: 2172
Batch: 2173
Batch: 2174
Batch: 2175
Batch: 2176
Batch: 2177
Batch: 2178
Batch: 2179
Batch: 2180
Batch: 2181
Batch: 2182
Batch: 2183
Batch: 2184
Batch: 2185
Batch: 2186
Batch: 2187
Batch: 2188
Batch: 2189
Batch: 2190
Batch: 2191
Batch: 2192
Batch: 2193
Batch: 2194
Batch: 2195
Batch: 2196
Batch: 2197
Batch: 2198
Batch: 2199
Batch: 2200
Batch: 2201
Batch: 2202
Batch: 2203
Batch: 2204
Batch: 2205
Batch: 2206
Batch: 2207
Batch: 2208
Batch: 2209
Batch: 2210
Batch: 2211
Batch: 2212
Batch: 2213
Batch: 2214
Batch: 2215
Batch: 2216
Batch: 2217
Batch: 2218
Batch: 2219
Batch: 2220
Batch: 2221
Batch: 2222
Batch: 2223
Batch: 2224
Batch: 2225
Batch: 2226
Batch: 2227
Batch: 2228
Batch: 2229
Batch: 2230
Batch: 2231
Batch: 2232
Batch: 2233
Batch: 2234
Batch: 2235
Batch: 2236
Batch: 2237
Batch: 2238
Batch: 2239
Batch: 2240
Batch: 2241
Batch: 2242
Batch: 2243
Batch: 2244
Batch: 2245
Batch: 2246
Batch: 2247
Batch: 2248
Batch: 2249
Batch: 2250
Batch: 2251
Batch: 2252
Batch: 2253
Batch: 2254
Batch: 2255
Batch: 2256
Batch: 2257
Batch: 2258
Batch: 2259
Batch: 2260
Batch: 2261
Batch: 2262
Batch: 2263
Batch: 2264
Batch: 2265
Batch: 2266
Batch: 2267
Batch: 2268
Batch: 2269
Batch: 2270
Batch: 2271
Batch: 2272
Batch: 2273
Batch: 2274
Batch: 2275
Batch: 2276
Batch: 2277
Batch: 2278
Batch: 2279
Batch: 2280
Batch: 2281
Batch: 2282
Batch: 2283
Batch: 2284
Batch: 2285
Batch: 2286
Batch: 2287
Batch: 2288
Batch: 2289
Batch: 2290
Batch: 2291
Batch: 2292
Batch: 2293
Batch: 2294
Batch: 2295
Batch: 2296
Batch: 2297
Batch: 2298
Batch: 2299
Batch: 2300
Batch: 2301
Batch: 2302
Batch: 2303
Batch: 2304
Batch: 2305
Batch: 2306
Batch: 2307
Batch: 2308
Batch: 2309
Batch: 2310
Batch: 2311
Batch: 2312
Batch: 2313
Batch: 2314
Batch: 2315
Batch: 2316
Batch: 2317
Batch: 2318
Batch: 2319
Batch: 2320
Batch: 2321
Batch: 2322
Batch: 2323
Batch: 2324
Batch: 2325
Batch: 2326
Batch: 2327
Batch: 2328
Batch: 2329
Batch: 2330
Batch: 2331
Batch: 2332
Batch: 2333
Batch: 2334
Batch: 2335
Batch: 2336
Batch: 2337
Batch: 2338
Batch: 2339
Batch: 2340
Batch: 2341
Batch: 2342
Batch: 2343
Batch: 2344
Batch: 2345
Batch: 2346
Batch: 2347
Batch: 2348
Batch: 2349
Batch: 2350
Batch: 2351
Batch: 2352
Batch: 2353
Batch: 2354
Batch: 2355
Batch: 2356
Batch: 2357
Batch: 2358
Batch: 2359
Batch: 2360
Batch: 2361
Batch: 2362
Batch: 2363
Batch: 2364
Batch: 2365
Batch: 2366
Batch: 2367
Batch: 2368
Batch: 2369
Batch: 2370
Batch: 2371
Batch: 2372
Batch: 2373
Batch: 2374
Batch: 2375
Batch: 2376
Batch: 2377
Batch: 2378
Batch: 2379
Batch: 2380
Batch: 2381
Batch: 2382
Batch: 2383
Batch: 2384
Batch: 2385
Batch: 2386
Batch: 2387
Batch: 2388
Batch: 2389
Batch: 2390
Batch: 2391
Batch: 2392
Batch: 2393
Batch: 2394
Batch: 2395
Batch: 2396
Batch: 2397
Batch: 2398
Batch: 2399
Batch: 2400
Batch: 2401
Batch: 2402
Batch: 2403
Batch: 2404
Batch: 2405
Batch: 2406
Batch: 2407
Batch: 2408
Batch: 2409
Batch: 2410
Batch: 2411
Batch: 2412
Batch: 2413
Batch: 2414
Batch: 2415
Batch: 2416
Batch: 2417
Batch: 2418
Batch: 2419
Batch: 2420
Batch: 2421
Batch: 2422
Batch: 2423
Batch: 2424
Batch: 2425
Batch: 2426
Batch: 2427
Batch: 2428
Batch: 2429
Batch: 2430
Batch: 2431
Batch: 2432
Batch: 2433
Batch: 2434
Batch: 2435
Batch: 2436
Batch: 2437
Batch: 2438
Batch: 2439
Batch: 2440
Batch: 2441
Batch: 2442
Batch: 2443
Batch: 2444
Batch: 2445
Batch: 2446
Batch: 2447
Batch: 2448
Batch: 2449
Batch: 2450
Batch: 2451
Batch: 2452
Batch: 2453
Batch: 2454
Batch: 2455
Batch: 2456
Batch: 2457
Batch: 2458
Batch: 2459
Batch: 2460
Batch: 2461
Batch: 2462
Batch: 2463
Batch: 2464
Batch: 2465
Batch: 2466
Batch: 2467
Batch: 2468
Batch: 2469
Batch: 2470
Batch: 2471
Batch: 2472
Batch: 2473
Batch: 2474
Batch: 2475
Batch: 2476
Batch: 2477
Batch: 2478
Batch: 2479
Batch: 2480
Batch: 2481
Batch: 2482
Batch: 2483
Batch: 2484
Batch: 2485
Batch: 2486
Batch: 2487
Batch: 2488
Batch: 2489
Batch: 2490
Batch: 2491
Batch: 2492
Batch: 2493
Batch: 2494
Batch: 2495
Batch: 2496
Batch: 2497
Batch: 2498
Batch: 2499
Batch: 2500
Batch: 2501
Batch: 2502
Batch: 2503
Batch: 2504
Batch: 2505
Batch: 2506
Batch: 2507
Batch: 2508
Batch: 2509
Batch: 2510
Batch: 2511
Batch: 2512
Batch: 2513
Batch: 2514
Batch: 2515
Batch: 2516
Batch: 2517
Batch: 2518
Batch: 2519
Batch: 2520
Batch: 2521
Batch: 2522
Batch: 2523
Batch: 2524
Batch: 2525
Batch: 2526
Batch: 2527
Batch: 2528
Batch: 2529
Batch: 2530
Batch: 2531
Batch: 2532
Batch: 2533
Batch: 2534
Batch: 2535
Batch: 2536
Batch: 2537
Batch: 2538
Batch: 2539
Batch: 2540
Batch: 2541
Batch: 2542
Batch: 2543
Batch: 2544
Batch: 2545
Batch: 2546
Batch: 2547
Batch: 2548
Batch: 2549
Batch: 2550
Batch: 2551
Batch: 2552
Batch: 2553
Batch: 2554
Batch: 2555
Batch: 2556
Batch: 2557
Batch: 2558
Batch: 2559
Batch: 2560
Batch: 2561
Batch: 2562
Batch: 2563
Batch: 2564
Batch: 2565
Batch: 2566
Batch: 2567
Batch: 2568
Batch: 2569
Batch: 2570
Batch: 2571
Batch: 2572
Batch: 2573
Batch: 2574
Batch: 2575
Batch: 2576
Batch: 2577
Batch: 2578
Batch: 2579
Batch: 2580
Batch: 2581
Batch: 2582
Batch: 2583
Batch: 2584
Batch: 2585
Batch: 2586
Batch: 2587
Batch: 2588
Batch: 2589
Batch: 2590
Batch: 2591
Batch: 2592
Batch: 2593
Batch: 2594
Batch: 2595
Batch: 2596
Batch: 2597
Batch: 2598
Batch: 2599
Batch: 2600
Batch: 2601
Batch: 2602
Batch: 2603
Batch: 2604
Batch: 2605
Batch: 2606
Batch: 2607
Batch: 2608
Batch: 2609
Batch: 2610
Batch: 2611
Batch: 2612
Batch: 2613
Batch: 2614
Batch: 2615
Batch: 2616
Batch: 2617
Batch: 2618
Batch: 2619
Batch: 2620
Batch: 2621
Batch: 2622
Batch: 2623
Batch: 2624
Batch: 2625
Batch: 2626
Batch: 2627
Batch: 2628
Batch: 2629
Batch: 2630
Batch: 2631
Batch: 2632
Batch: 2633
Batch: 2634
Batch: 2635
Batch: 2636
Batch: 2637
Batch: 2638
Batch: 2639
Batch: 2640
Batch: 2641
Batch: 2642
Batch: 2643
Batch: 2644
Batch: 2645
Batch: 2646
Batch: 2647
Batch: 2648
Batch: 2649
Batch: 2650
Batch: 2651
Batch: 2652
Batch: 2653
Batch: 2654
Batch: 2655
Batch: 2656
Batch: 2657
Batch: 2658
Batch: 2659
Batch: 2660
Batch: 2661
Batch: 2662
Batch: 2663
Batch: 2664
Batch: 2665
Batch: 2666
Batch: 2667
Batch: 2668
Batch: 2669
Batch: 2670
Batch: 2671
Batch: 2672
Batch: 2673
Batch: 2674
Batch: 2675
Batch: 2676
Batch: 2677
Batch: 2678
Batch: 2679
Batch: 2680
Batch: 2681
Batch: 2682
Batch: 2683
Batch: 2684
Batch: 2685
Batch: 2686
Batch: 2687
Batch: 2688
Batch: 2689
Batch: 2690
Batch: 2691
Batch: 2692
Batch: 2693
Batch: 2694
Batch: 2695
Batch: 2696
Batch: 2697
Batch: 2698
Batch: 2699
Batch: 2700
Batch: 2701
Batch: 2702
Batch: 2703
Batch: 2704
Batch: 2705
Batch: 2706
Batch: 2707
Batch: 2708
Batch: 2709
Batch: 2710
Batch: 2711
Batch: 2712
Batch: 2713
Batch: 2714
Batch: 2715
Batch: 2716
Batch: 2717
Batch: 2718
Batch: 2719
Batch: 2720
Batch: 2721
Batch: 2722
Batch: 2723
Batch: 2724
Batch: 2725
Batch: 2726
Batch: 2727
Batch: 2728
Batch: 2729
Batch: 2730
Batch: 2731
Batch: 2732
Batch: 2733
Batch: 2734
Batch: 2735
Batch: 2736
Batch: 2737
Batch: 2738
Batch: 2739
Batch: 2740
Batch: 2741
Batch: 2742
Batch: 2743
Batch: 2744
Batch: 2745
Batch: 2746
Batch: 2747
Batch: 2748
Batch: 2749
Batch: 2750
Batch: 2751
Batch: 2752
Batch: 2753
Batch: 2754
Batch: 2755
Batch: 2756
Batch: 2757
Batch: 2758
Batch: 2759
Batch: 2760
Batch: 2761
Batch: 2762
Batch: 2763
Batch: 2764
Batch: 2765
Batch: 2766
Batch: 2767
Batch: 2768
Batch: 2769
Batch: 2770
Batch: 2771
Batch: 2772
Batch: 2773
Batch: 2774
Batch: 2775
Batch: 2776
Batch: 2777
Batch: 2778
Batch: 2779
Batch: 2780
Batch: 2781
Batch: 2782
Batch: 2783
Batch: 2784
Batch: 2785
Batch: 2786
Batch: 2787
Batch: 2788
Batch: 2789
Batch: 2790
Batch: 2791
Batch: 2792
Batch: 2793
Batch: 2794
Batch: 2795
Batch: 2796
Batch: 2797
Batch: 2798
Batch: 2799
Batch: 2800
Batch: 2801
Batch: 2802
Batch: 2803
Batch: 2804
Batch: 2805
Batch: 2806
Batch: 2807
Batch: 2808
Batch: 2809
Batch: 2810
Batch: 2811
Batch: 2812
Batch: 2813
Batch: 2814
Batch: 2815
Batch: 2816
Batch: 2817
Batch: 2818
Batch: 2819
Batch: 2820
Batch: 2821
Batch: 2822
Batch: 2823
Batch: 2824
Batch: 2825
Batch: 2826
Batch: 2827
Batch: 2828
Batch: 2829
Batch: 2830
Batch: 2831
Batch: 2832
Batch: 2833
Batch: 2834
Batch: 2835
Batch: 2836
Batch: 2837
Batch: 2838
Batch: 2839
Batch: 2840
Batch: 2841
Batch: 2842
Batch: 2843
Batch: 2844
Batch: 2845
Batch: 2846
Batch: 2847
Batch: 2848
Batch: 2849
Batch: 2850
Batch: 2851
Batch: 2852
Batch: 2853
Batch: 2854
Batch: 2855
Batch: 2856
Batch: 2857
Batch: 2858
Batch: 2859
Batch: 2860
Batch: 2861
Batch: 2862
Batch: 2863
Batch: 2864
Batch: 2865
Batch: 2866
Batch: 2867
Batch: 2868
Batch: 2869
Batch: 2870
Batch: 2871
Batch: 2872
Batch: 2873
Batch: 2874
Batch: 2875
Batch: 2876
Batch: 2877
Batch: 2878
Batch: 2879
Batch: 2880
Batch: 2881
Batch: 2882
Batch: 2883
Batch: 2884
Batch: 2885
Batch: 2886
Batch: 2887
Batch: 2888
Batch: 2889
Batch: 2890
Batch: 2891
Batch: 2892
Batch: 2893
Batch: 2894
Batch: 2895
Batch: 2896
Batch: 2897
Batch: 2898
Batch: 2899
Batch: 2900
Batch: 2901
Batch: 2902
Batch: 2903
Batch: 2904
Batch: 2905
Batch: 2906
Batch: 2907
Batch: 2908
Batch: 2909
Batch: 2910
Batch: 2911
Batch: 2912
Batch: 2913
Batch: 2914
Batch: 2915
Batch: 2916
Batch: 2917
Batch: 2918
Batch: 2919
Batch: 2920
Batch: 2921
Batch: 2922
Batch: 2923
Batch: 2924
Batch: 2925
Batch: 2926
Batch: 2927
Batch: 2928
Batch: 2929
Batch: 2930
Batch: 2931
Batch: 2932
Batch: 2933
Batch: 2934
Batch: 2935
Batch: 2936
Batch: 2937
Batch: 2938
Batch: 2939
Batch: 2940
Batch: 2941
Batch: 2942
Batch: 2943
Batch: 2944
Batch: 2945
Batch: 2946
Batch: 2947
Batch: 2948
Batch: 2949
Batch: 2950
Batch: 2951
Batch: 2952
Batch: 2953
Batch: 2954
Batch: 2955
Batch: 2956
Batch: 2957
Batch: 2958
Batch: 2959
Batch: 2960
Batch: 2961
Batch: 2962
Batch: 2963
Batch: 2964
Batch: 2965
Batch: 2966
Batch: 2967
Batch: 2968
Batch: 2969
Batch: 2970
Batch: 2971
Batch: 2972
Batch: 2973
Batch: 2974
Batch: 2975
Batch: 2976
Batch: 2977
Batch: 2978
Batch: 2979
Batch: 2980
Batch: 2981
Batch: 2982
Batch: 2983
Batch: 2984
Batch: 2985
Batch: 2986
Batch: 2987
Batch: 2988
Batch: 2989
Batch: 2990
Batch: 2991
Batch: 2992
Batch: 2993
Batch: 2994
Batch: 2995
Batch: 2996
Batch: 2997
Batch: 2998
Batch: 2999
Batch: 3000
Batch: 3001
Batch: 3002
Batch: 3003
Batch: 3004
Batch: 3005
Batch: 3006
Batch: 3007
Batch: 3008
Batch: 3009
Batch: 3010
Batch: 3011
Batch: 3012
Batch: 3013
Batch: 3014
Batch: 3015
Batch: 3016
Batch: 3017
Batch: 3018
Batch: 3019
Batch: 3020
Batch: 3021
Batch: 3022
Batch: 3023
Batch: 3024
Batch: 3025
Batch: 3026
Batch: 3027
Batch: 3028
Batch: 3029
Batch: 3030
Batch: 3031
Batch: 3032
Batch: 3033
Batch: 3034
Batch: 3035
Batch: 3036
Batch: 3037
Batch: 3038
Batch: 3039
Batch: 3040
Batch: 3041
Batch: 3042
Batch: 3043
Batch: 3044
Batch: 3045
Batch: 3046
Batch: 3047
Batch: 3048
Batch: 3049
Batch: 3050
Batch: 3051
Batch: 3052
Batch: 3053
Batch: 3054
Batch: 3055
Batch: 3056
Batch: 3057
Batch: 3058
Batch: 3059
Batch: 3060
Batch: 3061
Batch: 3062
Batch: 3063
Batch: 3064
Batch: 3065
Batch: 3066
Batch: 3067
Batch: 3068
Batch: 3069
Batch: 3070
Batch: 3071
Batch: 3072
Batch: 3073
Batch: 3074
Batch: 3075
Batch: 3076
Batch: 3077
Batch: 3078
Batch: 3079
Batch: 3080
Batch: 3081
Batch: 3082
Batch: 3083
Batch: 3084
Batch: 3085
Batch: 3086
Batch: 3087
Batch: 3088
Batch: 3089
Batch: 3090
Batch: 3091
Batch: 3092
Batch: 3093
Batch: 3094
Batch: 3095
Batch: 3096
Batch: 3097
Batch: 3098
Batch: 3099
Batch: 3100
Batch: 3101
Batch: 3102
Batch: 3103
Batch: 3104
Batch: 3105
Batch: 3106
Batch: 3107
Batch: 3108
Batch: 3109
Batch: 3110
Batch: 3111
Batch: 3112
Batch: 3113
Batch: 3114
Batch: 3115
Batch: 3116
Batch: 3117
Batch: 3118
Batch: 3119
Batch: 3120
Batch: 3121
Batch: 3122
Batch: 3123
Batch: 3124
Batch: 3125
Batch: 3126
Batch: 3127
Batch: 3128
Batch: 3129
Batch: 3130
Batch: 3131
Batch: 3132
Batch: 3133
Batch: 3134
Batch: 3135
Batch: 3136
Batch: 3137
Batch: 3138
Batch: 3139
Batch: 3140
Batch: 3141
Batch: 3142
Batch: 3143
Batch: 3144
Batch: 3145
Batch: 3146
Batch: 3147
Batch: 3148
Batch: 3149
Batch: 3150
Batch: 3151
Batch: 3152
Batch: 3153
Batch: 3154
Batch: 3155
Batch: 3156
Batch: 3157
Batch: 3158
Batch: 3159
Batch: 3160
Batch: 3161
Batch: 3162
Batch: 3163
Batch: 3164
Batch: 3165
Batch: 3166
Batch: 3167
Batch: 3168
Batch: 3169
Batch: 3170
Batch: 3171
Batch: 3172
Batch: 3173
Batch: 3174
Batch: 3175
Batch: 3176
Batch: 3177
Batch: 3178
Batch: 3179
Batch: 3180
Batch: 3181
Batch: 3182
Batch: 3183
Batch: 3184
Batch: 3185
Batch: 3186
Batch: 3187
Batch: 3188
Batch: 3189
Batch: 3190
Batch: 3191
Batch: 3192
Batch: 3193
Batch: 3194
Batch: 3195
Batch: 3196
Batch: 3197
Batch: 3198
Batch: 3199
Batch: 3200
Batch: 3201
Batch: 3202
Batch: 3203
Batch: 3204
Batch: 3205
Batch: 3206
Batch: 3207
Batch: 3208
Batch: 3209
Batch: 3210
Batch: 3211
Batch: 3212
Batch: 3213
Batch: 3214
Batch: 3215
Batch: 3216
Batch: 3217
Batch: 3218
Batch: 3219
Batch: 3220
Batch: 3221
Batch: 3222
Batch: 3223
Batch: 3224
Batch: 3225
Batch: 3226
Batch: 3227
Batch: 3228
Batch: 3229
Batch: 3230
Batch: 3231
Batch: 3232
Batch: 3233
Batch: 3234
Batch: 3235
Batch: 3236
Batch: 3237
Batch: 3238
Batch: 3239
Batch: 3240
Batch: 3241
Batch: 3242
Batch: 3243
Batch: 3244
Batch: 3245
Batch: 3246
Batch: 3247
Batch: 3248
Batch: 3249
Batch: 3250
Batch: 3251
Batch: 3252
Batch: 3253
Batch: 3254
Batch: 3255
Batch: 3256
Batch: 3257
Batch: 3258
Batch: 3259
Batch: 3260
Batch: 3261
Batch: 3262
Batch: 3263
Batch: 3264
Batch: 3265
Batch: 3266
Batch: 3267
Batch: 3268
Batch: 3269
Batch: 3270
Batch: 3271
Batch: 3272
Batch: 3273
Batch: 3274
Batch: 3275
Batch: 3276
Batch: 3277
Batch: 3278
Batch: 3279
Batch: 3280
Batch: 3281
Batch: 3282
Batch: 3283
Batch: 3284
Batch: 3285
Batch: 3286
Batch: 3287
Batch: 3288
Batch: 3289
Batch: 3290
Batch: 3291
Batch: 3292
Batch: 3293
Batch: 3294
Batch: 3295
Batch: 3296
Batch: 3297
Batch: 3298
Batch: 3299
Batch: 3300
Batch: 3301
Batch: 3302
Batch: 3303
Batch: 3304
Batch: 3305
Batch: 3306
Batch: 3307
Batch: 3308
Batch: 3309
Batch: 3310
Batch: 3311
Batch: 3312
Batch: 3313
Batch: 3314
Batch: 3315
Batch: 3316
Batch: 3317
Batch: 3318
Batch: 3319
Batch: 3320
Batch: 3321
Batch: 3322
Batch: 3323
Batch: 3324
Batch: 3325
Batch: 3326
Batch: 3327
Batch: 3328
Batch: 3329
Batch: 3330
Batch: 3331
Batch: 3332
Batch: 3333
Batch: 3334
Batch: 3335
Batch: 3336
Batch: 3337
Batch: 3338
Batch: 3339
Batch: 3340
Batch: 3341
Batch: 3342
Batch: 3343
Batch: 3344
Batch: 3345
Batch: 3346
Batch: 3347
Batch: 3348
Batch: 3349
Batch: 3350
Batch: 3351
Batch: 3352
Batch: 3353
Batch: 3354
Batch: 3355
Batch: 3356
Batch: 3357
Batch: 3358
Batch: 3359
Batch: 3360
Batch: 3361
Batch: 3362
Batch: 3363
Batch: 3364
Batch: 3365
Batch: 3366
Batch: 3367
Batch: 3368
Batch: 3369
Batch: 3370
Batch: 3371
Batch: 3372
Batch: 3373
Batch: 3374
Batch: 3375
Batch: 3376
Batch: 3377
Batch: 3378
Batch: 3379
Batch: 3380
Batch: 3381
Batch: 3382
Batch: 3383
Batch: 3384
Batch: 3385
Batch: 3386
Batch: 3387
Batch: 3388
Batch: 3389
Batch: 3390
Batch: 3391
Batch: 3392
Batch: 3393
Batch: 3394
Batch: 3395
Batch: 3396
Batch: 3397
Batch: 3398
Batch: 3399
Batch: 3400
Batch: 3401
Batch: 3402
Batch: 3403
Batch: 3404
Batch: 3405
Batch: 3406
Batch: 3407
Batch: 3408
Batch: 3409
Batch: 3410
Batch: 3411
Batch: 3412
Batch: 3413
Batch: 3414
Batch: 3415
Batch: 3416
Batch: 3417
Batch: 3418
Batch: 3419
Batch: 3420
Batch: 3421
Batch: 3422
Batch: 3423
Batch: 3424
Batch: 3425
Batch: 3426
Batch: 3427
Batch: 3428
Batch: 3429
Batch: 3430
Batch: 3431
Batch: 3432
Batch: 3433
Batch: 3434
Batch: 3435
Batch: 3436
Batch: 3437
Batch: 3438
Batch: 3439
Batch: 3440
Batch: 3441
Batch: 3442
Batch: 3443
Batch: 3444
Batch: 3445
Batch: 3446
Batch: 3447
Batch: 3448
Batch: 3449
Batch: 3450
Batch: 3451
Batch: 3452
Batch: 3453
Batch: 3454
Batch: 3455
Batch: 3456
Batch: 3457
Batch: 3458
Batch: 3459
Batch: 3460
Batch: 3461
Batch: 3462
Batch: 3463
Batch: 3464
Batch: 3465
Batch: 3466
Batch: 3467
Batch: 3468
Batch: 3469
Batch: 3470
Batch: 3471
Batch: 3472
Batch: 3473
Batch: 3474
Batch: 3475
Batch: 3476
Batch: 3477
Batch: 3478
Batch: 3479
Batch: 3480
Batch: 3481
Batch: 3482
Batch: 3483
Batch: 3484
Batch: 3485
Batch: 3486
Batch: 3487
Batch: 3488
Batch: 3489
Batch: 3490
Batch: 3491
Batch: 3492
Batch: 3493
Batch: 3494
Batch: 3495
Batch: 3496
Batch: 3497
Batch: 3498
Batch: 3499
Batch: 3500
Batch: 3501
Batch: 3502
Batch: 3503
Batch: 3504
Batch: 3505
Batch: 3506
Batch: 3507
Batch: 3508
Batch: 3509
Batch: 3510
Batch: 3511
Batch: 3512
Batch: 3513
Batch: 3514
Batch: 3515
Batch: 3516
Batch: 3517
Batch: 3518
Batch: 3519
Batch: 3520
Batch: 3521
Batch: 3522
Batch: 3523
Batch: 3524
Batch: 3525
Batch: 3526
Batch: 3527
Batch: 3528
Batch: 3529
Batch: 3530
Batch: 3531
Batch: 3532
Batch: 3533
Batch: 3534
Batch: 3535
Batch: 3536
Batch: 3537
Batch: 3538
Batch: 3539
Batch: 3540
Batch: 3541
Batch: 3542
Batch: 3543
Batch: 3544
Batch: 3545
Batch: 3546
Batch: 3547
Batch: 3548
Batch: 3549
Batch: 3550
Batch: 3551
Batch: 3552
Batch: 3553
Batch: 3554
Batch: 3555
Batch: 3556
Batch: 3557
Batch: 3558
Batch: 3559
Batch: 3560
Batch: 3561
Batch: 3562
Batch: 3563
Batch: 3564
Batch: 3565
Batch: 3566
Batch: 3567
Batch: 3568
Batch: 3569
Batch: 3570
Batch: 3571
Batch: 3572
Batch: 3573
Batch: 3574
Batch: 3575
Batch: 3576
Batch: 3577
Batch: 3578
Batch: 3579
Batch: 3580
Batch: 3581
Batch: 3582
Batch: 3583
Batch: 3584
Batch: 3585
Batch: 3586
Batch: 3587
Batch: 3588
Batch: 3589
Batch: 3590
Batch: 3591
Batch: 3592
Batch: 3593
Batch: 3594
Batch: 3595
Batch: 3596
Batch: 3597
Batch: 3598
Batch: 3599
Batch: 3600
Batch: 3601
Batch: 3602
Batch: 3603
Batch: 3604
Batch: 3605
Batch: 3606
Batch: 3607
Batch: 3608
Batch: 3609
Batch: 3610
Batch: 3611
Batch: 3612
Batch: 3613
Batch: 3614
Batch: 3615
Batch: 3616
Batch: 3617
Batch: 3618
Batch: 3619
Batch: 3620
Batch: 3621
Batch: 3622
Batch: 3623
Batch: 3624
Batch: 3625
Batch: 3626
Batch: 3627
Batch: 3628
Batch: 3629
Batch: 3630
Batch: 3631
Batch: 3632
Batch: 3633
Batch: 3634
Batch: 3635
Batch: 3636
Batch: 3637
Batch: 3638
Batch: 3639
Batch: 3640
Batch: 3641
Batch: 3642
Batch: 3643
Batch: 3644
Batch: 3645
Batch: 3646
Batch: 3647
Batch: 3648
Batch: 3649
Batch: 3650
Batch: 3651
Batch: 3652
Batch: 3653
Batch: 3654
Batch: 3655
Batch: 3656
Batch: 3657
Batch: 3658
Batch: 3659
Batch: 3660
Batch: 3661
Batch: 3662
Batch: 3663
Batch: 3664
Batch: 3665
Batch: 3666
Batch: 3667
Batch: 3668
Batch: 3669
Batch: 3670
Batch: 3671
Batch: 3672
Batch: 3673
Batch: 3674
Batch: 3675
Batch: 3676
Batch: 3677
Batch: 3678
Batch: 3679
Batch: 3680
Batch: 3681
Batch: 3682
Batch: 3683
Batch: 3684
Batch: 3685
Batch: 3686
Batch: 3687
Batch: 3688
Batch: 3689
Batch: 3690
Batch: 3691
Batch: 3692
Batch: 3693
Batch: 3694
Batch: 3695
Batch: 3696
Batch: 3697
Batch: 3698
Batch: 3699
Batch: 3700
Batch: 3701
Batch: 3702
Batch: 3703
Batch: 3704
Batch: 3705
Batch: 3706
Batch: 3707
Batch: 3708
Batch: 3709
Batch: 3710
Batch: 3711
Batch: 3712
Batch: 3713
Batch: 3714
Batch: 3715
Batch: 3716
Batch: 3717
Batch: 3718
Batch: 3719
Batch: 3720
Batch: 3721
Batch: 3722
Batch: 3723
Batch: 3724
Batch: 3725
Batch: 3726
Batch: 3727
Batch: 3728
Batch: 3729
Batch: 3730
Batch: 3731
Batch: 3732
Batch: 3733
Batch: 3734
Batch: 3735
Batch: 3736
Batch: 3737
Batch: 3738
Batch: 3739
Batch: 3740
Batch: 3741
Batch: 3742
Batch: 3743
Batch: 3744
Batch: 3745
Batch: 3746
Batch: 3747
Batch: 3748
Batch: 3749
Batch: 3750
Batch: 3751
Batch: 3752
Batch: 3753
Batch: 3754
Batch: 3755
Batch: 3756
Batch: 3757
Batch: 3758
Batch: 3759
Batch: 3760
Batch: 3761
Batch: 3762
Batch: 3763
Batch: 3764
Batch: 3765
Batch: 3766
Batch: 3767
Batch: 3768
Batch: 3769
Batch: 3770
Batch: 3771
Batch: 3772
Batch: 3773
Batch: 3774
Batch: 3775
Batch: 3776
Batch: 3777
Batch: 3778
Batch: 3779
Batch: 3780
Batch: 3781
Batch: 3782
Batch: 3783
Batch: 3784
Batch: 3785
Batch: 3786
Batch: 3787
Batch: 3788
Batch: 3789
Batch: 3790
Batch: 3791
Batch: 3792
Batch: 3793
Batch: 3794
Batch: 3795
Batch: 3796
Batch: 3797
Batch: 3798
Batch: 3799
Batch: 3800
Batch: 3801
Batch: 3802
Batch: 3803
Batch: 3804
Batch: 3805
Batch: 3806
Batch: 3807
Batch: 3808
Batch: 3809
Batch: 3810
Batch: 3811
Batch: 3812
Batch: 3813
Batch: 3814
Batch: 3815
Batch: 3816
Batch: 3817
Batch: 3818
Batch: 3819
Batch: 3820
Batch: 3821
Batch: 3822
Batch: 3823
Batch: 3824
Batch: 3825
Batch: 3826
Batch: 3827
Batch: 3828
Batch: 3829
Batch: 3830
Batch: 3831
Batch: 3832
Batch: 3833
Batch: 3834
Batch: 3835
Batch: 3836
Batch: 3837
Batch: 3838
Batch: 3839
Batch: 3840
Batch: 3841
Batch: 3842
Batch: 3843
Batch: 3844
Batch: 3845
Batch: 3846
Batch: 3847
Batch: 3848
Batch: 3849
Batch: 3850
Batch: 3851
Batch: 3852
Batch: 3853
Batch: 3854
Batch: 3855
Batch: 3856
Batch: 3857
Batch: 3858
Batch: 3859
Batch: 3860
Batch: 3861
Batch: 3862
Batch: 3863
Batch: 3864
Batch: 3865
Batch: 3866
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000002
	speed: 0.1724s/iter; left time: 6648.4068s
	iters: 200, epoch: 1 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 6537.4417s
	iters: 300, epoch: 1 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 6534.4327s
	iters: 400, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6506.4910s
	iters: 500, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6489.6267s
	iters: 600, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6473.1484s
	iters: 700, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6455.8560s
	iters: 800, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6438.4421s
	iters: 900, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6421.1932s
	iters: 1000, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6403.5310s
	iters: 1100, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6386.1177s
	iters: 1200, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6368.2972s
	iters: 1300, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6352.9885s
	iters: 1400, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6335.5619s
	iters: 1500, epoch: 1 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 6328.3078s
	iters: 1600, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 6307.3447s
	iters: 1700, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 6288.1625s
	iters: 1800, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 6270.6835s
	iters: 1900, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6252.8135s
	iters: 2000, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6233.1772s
	iters: 2100, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6217.2854s
	iters: 2200, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 6202.0617s
	iters: 2300, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 6185.6650s
	iters: 2400, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6166.2101s
	iters: 2500, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6147.1118s
	iters: 2600, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6130.8166s
	iters: 2700, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 6119.8092s
	iters: 2800, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6095.4440s
	iters: 2900, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6081.0630s
	iters: 3000, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6061.3519s
	iters: 3100, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6046.9541s
	iters: 3200, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6029.3979s
	iters: 3300, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6011.9595s
	iters: 3400, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5995.9994s
	iters: 3500, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5979.9895s
	iters: 3600, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5964.3227s
	iters: 3700, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5949.0039s
	iters: 3800, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5930.3218s
Epoch: 1 cost time: 657.7786712646484
Epoch: 1, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000000
	speed: 4.4460s/iter; left time: 154292.8780s
	iters: 200, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5888.4083s
	iters: 300, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5869.1740s
	iters: 400, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5851.2429s
	iters: 500, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5831.6309s
	iters: 600, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5817.5147s
	iters: 700, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5801.5566s
	iters: 800, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5783.9963s
	iters: 900, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5764.7810s
	iters: 1000, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5750.0760s
	iters: 1100, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5732.1235s
	iters: 1200, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5717.7988s
	iters: 1300, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5702.2531s
	iters: 1400, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5685.1772s
	iters: 1500, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5667.0942s
	iters: 1600, epoch: 2 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 5656.1428s
	iters: 1700, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5627.9076s
	iters: 1800, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5611.2051s
	iters: 1900, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5595.9222s
	iters: 2000, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5580.4792s
	iters: 2100, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5560.8681s
	iters: 2200, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5546.0624s
	iters: 2300, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5530.0177s
	iters: 2400, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5509.6409s
	iters: 2500, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5493.5476s
	iters: 2600, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5480.1815s
	iters: 2700, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5465.0613s
	iters: 2800, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5441.6483s
	iters: 2900, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5425.5827s
	iters: 3000, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5411.7462s
	iters: 3100, epoch: 2 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5394.6366s
	iters: 3200, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5377.3495s
	iters: 3300, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5359.2735s
	iters: 3400, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5341.8420s
	iters: 3500, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5323.3192s
	iters: 3600, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5304.0173s
	iters: 3700, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5287.1860s
	iters: 3800, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5270.1469s
Epoch: 2 cost time: 658.058379650116
Epoch: 2, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000000
	speed: 4.4411s/iter; left time: 136950.0646s
	iters: 200, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5229.2016s
	iters: 300, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5211.4900s
	iters: 400, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5191.8118s
	iters: 500, epoch: 3 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5178.9540s
	iters: 600, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5159.8128s
	iters: 700, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5141.1112s
	iters: 800, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5126.2760s
	iters: 900, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5105.3385s
	iters: 1000, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5089.2906s
	iters: 1100, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5071.2540s
	iters: 1200, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5052.7062s
	iters: 1300, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5036.5332s
	iters: 1400, epoch: 3 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 5027.6090s
	iters: 1500, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5002.2843s
	iters: 1600, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4984.7402s
	iters: 1700, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4967.9040s
	iters: 1800, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4953.0734s
	iters: 1900, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4938.3697s
	iters: 2000, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4920.0591s
	iters: 2100, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4902.0920s
	iters: 2200, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4883.7041s
	iters: 2300, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4869.8064s
	iters: 2400, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4852.7521s
	iters: 2500, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4836.7917s
	iters: 2600, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4821.3032s
	iters: 2700, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4800.3272s
	iters: 2800, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4785.1345s
	iters: 2900, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4767.8351s
	iters: 3000, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4751.4421s
	iters: 3100, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4732.9545s
	iters: 3200, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4717.0564s
	iters: 3300, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4700.0193s
	iters: 3400, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4680.9344s
	iters: 3500, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4661.7592s
	iters: 3600, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4644.1322s
	iters: 3700, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4625.9570s
	iters: 3800, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4613.6961s
Epoch: 3 cost time: 657.7577011585236
Epoch: 3, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000000
	speed: 4.4412s/iter; left time: 119778.6836s
	iters: 200, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4567.3114s
	iters: 300, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4550.6774s
	iters: 400, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4533.3896s
	iters: 500, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4516.6684s
	iters: 600, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4500.1647s
	iters: 700, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4481.9579s
	iters: 800, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4464.8736s
	iters: 900, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4446.6736s
	iters: 1000, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4429.7014s
	iters: 1100, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4413.0039s
	iters: 1200, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4394.7599s
	iters: 1300, epoch: 4 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 4385.9502s
	iters: 1400, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4362.1822s
	iters: 1500, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4345.1667s
	iters: 1600, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4328.1282s
	iters: 1700, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4314.8820s
	iters: 1800, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4296.3032s
	iters: 1900, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4279.1187s
	iters: 2000, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4261.5508s
	iters: 2100, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4244.9769s
	iters: 2200, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4226.4679s
	iters: 2300, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4209.2448s
	iters: 2400, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4191.7415s
	iters: 2500, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4179.3585s
	iters: 2600, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4158.0295s
	iters: 2700, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4140.0034s
	iters: 2800, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4123.3287s
	iters: 2900, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4109.3075s
	iters: 3000, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4091.3475s
	iters: 3100, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4075.6415s
	iters: 3200, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4057.5738s
	iters: 3300, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4041.7626s
	iters: 3400, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4023.9609s
	iters: 3500, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4007.0552s
	iters: 3600, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3990.7836s
	iters: 3700, epoch: 4 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 3978.4213s
	iters: 3800, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3957.5892s
Epoch: 4 cost time: 657.6300082206726
Epoch: 4, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000000
	speed: 4.4451s/iter; left time: 102694.1927s
	iters: 200, epoch: 5 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 3919.1096s
	iters: 300, epoch: 5 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 3899.3359s
	iters: 400, epoch: 5 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 3881.7669s
	iters: 500, epoch: 5 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 3866.4229s
	iters: 600, epoch: 5 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 3845.9232s
	iters: 700, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3827.3719s
	iters: 800, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3809.4432s
	iters: 900, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3794.1112s
	iters: 1000, epoch: 5 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 3781.9060s
	iters: 1100, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3758.5080s
	iters: 1200, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3741.1855s
	iters: 1300, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3724.4615s
	iters: 1400, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3706.4704s
	iters: 1500, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3689.3636s
	iters: 1600, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3674.2844s
	iters: 1700, epoch: 5 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 3659.9156s
	iters: 1800, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3641.3281s
	iters: 1900, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3623.7240s
	iters: 2000, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3606.7589s
	iters: 2100, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3589.9462s
	iters: 2200, epoch: 5 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 3575.8051s
	iters: 2300, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3555.1345s
	iters: 2400, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3537.3526s
	iters: 2500, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3520.8708s
	iters: 2600, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3503.1642s
	iters: 2700, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3486.8108s
	iters: 2800, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3470.8318s
	iters: 2900, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3453.8954s
	iters: 3000, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3436.9713s
	iters: 3100, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3415.4398s
	iters: 3200, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3397.7296s
	iters: 3300, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3379.6048s
	iters: 3400, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3368.4807s
	iters: 3500, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3348.3692s
	iters: 3600, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3331.0281s
	iters: 3700, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3315.2107s
	iters: 3800, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3298.3363s
Epoch: 5 cost time: 658.0339212417603
Epoch: 5, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000000
	speed: 4.4422s/iter; left time: 85450.5404s
	iters: 200, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3252.4093s
	iters: 300, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3235.1340s
	iters: 400, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3217.8765s
	iters: 500, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3201.4688s
	iters: 600, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3183.8619s
	iters: 700, epoch: 6 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 3171.7785s
	iters: 800, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3150.9885s
	iters: 900, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3133.6982s
	iters: 1000, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3117.0563s
	iters: 1100, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3100.8385s
	iters: 1200, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3082.9031s
	iters: 1300, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3066.7602s
	iters: 1400, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3048.8987s
	iters: 1500, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3031.9413s
	iters: 1600, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3013.8054s
	iters: 1700, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2997.2281s
	iters: 1800, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2982.1875s
	iters: 1900, epoch: 6 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2968.2495s
	iters: 2000, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2949.0683s
	iters: 2100, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2931.6060s
	iters: 2200, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2914.6437s
	iters: 2300, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2897.7142s
	iters: 2400, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2881.6396s
	iters: 2500, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2862.9770s
	iters: 2600, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2844.3296s
	iters: 2700, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2826.7691s
	iters: 2800, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2810.5039s
	iters: 2900, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2792.9332s
	iters: 3000, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2777.5945s
	iters: 3100, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2760.5410s
	iters: 3200, epoch: 6 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 2748.6778s
	iters: 3300, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2727.9946s
	iters: 3400, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2710.3022s
	iters: 3500, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2692.9918s
	iters: 3600, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2675.6105s
	iters: 3700, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2657.5906s
	iters: 3800, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2640.9864s
Epoch: 6 cost time: 657.7871332168579
Epoch: 6, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000000
	speed: 4.4415s/iter; left time: 68261.8411s
	iters: 200, epoch: 7 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2596.5133s
	iters: 300, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2577.9747s
	iters: 400, epoch: 7 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2564.7897s
	iters: 500, epoch: 7 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2543.7236s
	iters: 600, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2527.0387s
	iters: 700, epoch: 7 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2509.8565s
	iters: 800, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2493.1304s
	iters: 900, epoch: 7 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2475.6278s
	iters: 1000, epoch: 7 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2458.4625s
	iters: 1100, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2442.4874s
	iters: 1200, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2425.9974s
	iters: 1300, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2409.3608s
	iters: 1400, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2391.5122s
	iters: 1500, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2374.8431s
	iters: 1600, epoch: 7 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2360.8685s
	iters: 1700, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2340.4521s
	iters: 1800, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2323.2252s
	iters: 1900, epoch: 7 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2307.5496s
	iters: 2000, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2289.8597s
	iters: 2100, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2272.9111s
	iters: 2200, epoch: 7 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2255.0308s
	iters: 2300, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2238.2068s
	iters: 2400, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2221.9731s
	iters: 2500, epoch: 7 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2206.1139s
	iters: 2600, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2188.2586s
	iters: 2700, epoch: 7 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2171.8127s
	iters: 2800, epoch: 7 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 2156.8095s
	iters: 2900, epoch: 7 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2137.9594s
	iters: 3000, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2119.7332s
	iters: 3100, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2102.3786s
	iters: 3200, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2086.0952s
	iters: 3300, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2068.5583s
	iters: 3400, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2052.1198s
	iters: 3500, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2034.9127s
	iters: 3600, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2018.2031s
	iters: 3700, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2000.4434s
	iters: 3800, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1983.5288s
Epoch: 7 cost time: 657.758193731308
Epoch: 7, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000000
	speed: 4.4432s/iter; left time: 51105.8926s
	iters: 200, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1938.6955s
	iters: 300, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1922.1547s
	iters: 400, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1904.8076s
	iters: 500, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1888.2297s
	iters: 600, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1869.9136s
	iters: 700, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1853.6174s
	iters: 800, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1835.4443s
	iters: 900, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1819.5779s
	iters: 1000, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1801.5847s
	iters: 1100, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1784.7010s
	iters: 1200, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1768.1392s
	iters: 1300, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1750.5928s
	iters: 1400, epoch: 8 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 1737.1792s
	iters: 1500, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1718.1228s
	iters: 1600, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1701.1205s
	iters: 1700, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1684.0851s
	iters: 1800, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1666.3979s
	iters: 1900, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1649.8847s
	iters: 2000, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1632.9044s
	iters: 2100, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1614.7045s
	iters: 2200, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1598.4658s
	iters: 2300, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1581.6568s
	iters: 2400, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1564.9076s
	iters: 2500, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1548.0334s
	iters: 2600, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1531.1744s
	iters: 2700, epoch: 8 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 1516.1328s
	iters: 2800, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1497.3622s
	iters: 2900, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1480.1059s
	iters: 3000, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1462.6633s
	iters: 3100, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1445.3801s
	iters: 3200, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1428.1418s
	iters: 3300, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1411.1506s
	iters: 3400, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1394.4575s
	iters: 3500, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1377.4366s
	iters: 3600, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1360.4521s
	iters: 3700, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1343.1915s
	iters: 3800, epoch: 8 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 1327.9067s
Epoch: 8 cost time: 657.8178973197937
Epoch: 8, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000000
	speed: 4.4438s/iter; left time: 33928.7776s
	iters: 200, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1280.6049s
	iters: 300, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1263.5210s
	iters: 400, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1246.6495s
	iters: 500, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1229.6318s
	iters: 600, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1212.8464s
	iters: 700, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1196.2101s
	iters: 800, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1179.1493s
	iters: 900, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1161.8427s
	iters: 1000, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1144.7050s
	iters: 1100, epoch: 9 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 1129.5298s
	iters: 1200, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1110.6995s
	iters: 1300, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1093.4539s
	iters: 1400, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1076.8190s
	iters: 1500, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1059.6531s
	iters: 1600, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1042.4795s
	iters: 1700, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1025.3742s
	iters: 1800, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1008.5014s
	iters: 1900, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 991.8190s
	iters: 2000, epoch: 9 | loss: 0.0000000
	speed: 0.1678s/iter; left time: 962.3572s
	iters: 2100, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 956.6370s
	iters: 2200, epoch: 9 | loss: 0.0000000
	speed: 0.1708s/iter; left time: 945.2735s
	iters: 2300, epoch: 9 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 925.7095s
	iters: 2400, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 906.6195s
	iters: 2500, epoch: 9 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 890.2251s
	iters: 2600, epoch: 9 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 873.2620s
	iters: 2700, epoch: 9 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 857.8926s
	iters: 2800, epoch: 9 | loss: 0.0000000
	speed: 0.1707s/iter; left time: 842.3460s
	iters: 2900, epoch: 9 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 823.1744s
	iters: 3000, epoch: 9 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 805.6558s
	iters: 3100, epoch: 9 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 788.6426s
	iters: 3200, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 771.1623s
	iters: 3300, epoch: 9 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 754.1987s
	iters: 3400, epoch: 9 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 737.2712s
	iters: 3500, epoch: 9 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 720.9121s
	iters: 3600, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 703.1322s
	iters: 3700, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 686.0696s
	iters: 3800, epoch: 9 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 669.4056s
Epoch: 9 cost time: 657.7146196365356
Epoch: 9, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000000
	speed: 4.4436s/iter; left time: 16743.3840s
	iters: 200, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 623.3889s
	iters: 300, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 606.3481s
	iters: 400, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 589.3446s
	iters: 500, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 572.6914s
	iters: 600, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 555.6780s
	iters: 700, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 538.6454s
	iters: 800, epoch: 10 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 522.4088s
	iters: 900, epoch: 10 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 504.7843s
	iters: 1000, epoch: 10 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 487.7554s
	iters: 1100, epoch: 10 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 470.8528s
	iters: 1200, epoch: 10 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 453.8149s
	iters: 1300, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 436.5948s
	iters: 1400, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 419.2662s
	iters: 1500, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 402.2132s
	iters: 1600, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 385.2372s
	iters: 1700, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 368.2799s
	iters: 1800, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 351.3412s
	iters: 1900, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 334.5409s
	iters: 2000, epoch: 10 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 317.9259s
	iters: 2100, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 300.5355s
	iters: 2200, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 283.3802s
	iters: 2300, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 266.3938s
	iters: 2400, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 249.4904s
	iters: 2500, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 232.5316s
	iters: 2600, epoch: 10 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 215.6253s
	iters: 2700, epoch: 10 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 198.7726s
	iters: 2800, epoch: 10 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 181.6292s
	iters: 2900, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 164.6053s
	iters: 3000, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 147.5895s
	iters: 3100, epoch: 10 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 130.7090s
	iters: 3200, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 113.5777s
	iters: 3300, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 96.5801s
	iters: 3400, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 79.5602s
	iters: 3500, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 62.5679s
	iters: 3600, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 45.5555s
	iters: 3700, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 28.5612s
	iters: 3800, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 11.5615s
Epoch: 10 cost time: 657.7212467193604
Epoch: 10, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 449820
test: (449919, 51)
train: (495000, 51)
train 494901
Threshold : 3.569705597783468e-08
pred:    (44982000,)
gt:      (44982000,)
pred:  (44982000,)
gt:    (44982000,)
Accuracy : 0.9660, Precision : 0.9098, Recall : 0.7997, F-score : 0.8512, AUC : 0.8943
  File "run.py", line 191
    return
    ^
SyntaxError: 'return' outside function
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
Traceback (most recent call last):
  File "run.py", line 152, in <module>
    exp = Exp(args)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 25, in __init__
    super(Exp_Anomaly_Detection, self).__init__(args)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_basic.py", line 28, in __init__
    self.model, self.feature_encoder = self._build_model()
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 28, in _build_model
    model = self.model_dict[self.args.model].Model(self.args).float()
  File "/root/taowei/Project/LLM/MADLLM/models/GPT4TS.py", line 69, in __init__
    self.gpt2.to(device=device)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/transformers/modeling_utils.py", line 1902, in to
    return super().to(*args, **kwargs)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 673, in to
    return self._apply(convert)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 409, in _apply
    param_applied = fn(param)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 671, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
KeyboardInterrupt
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
Traceback (most recent call last):
  File "run.py", line 169, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 68, in train
    train_data, train_loader = self._get_data(flag='train')
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 36, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_factory.py", line 45, in data_provider
    data_set = Data(
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_loader.py", line 940, in __init__
    file = args.data.strip('SMD')
NameError: name 'args' is not defined
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
Traceback (most recent call last):
  File "run.py", line 169, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 68, in train
    train_data, train_loader = self._get_data(flag='train')
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 36, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_factory.py", line 45, in data_provider
    data_set = Data(
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_loader.py", line 940, in __init__
    file = self.args.data.strip('SMD')
AttributeError: 'SMDSegLoader' object has no attribute 'args'
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
Traceback (most recent call last):
  File "run.py", line 169, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 68, in train
    train_data, train_loader = self._get_data(flag='train')
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 36, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_factory.py", line 47, in data_provider
    data_set = Data(
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_loader.py", line 941, in __init__
    data = np.load(os.path.join(root_path, "train", file))
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/numpy/lib/npyio.py", line 435, in load
    raise ValueError("Cannot load file containing pickled data "
ValueError: Cannot load file containing pickled data when allow_pickle=False
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
Traceback (most recent call last):
  File "run.py", line 169, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 68, in train
    train_data, train_loader = self._get_data(flag='train')
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 36, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_factory.py", line 47, in data_provider
    data_set = Data(
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_loader.py", line 941, in __init__
    data = np.load(os.path.join(root_path, file))
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/numpy/lib/npyio.py", line 407, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: './all_datasets/SMD/machine-1-1_train.txt'
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 284
val 56
Traceback (most recent call last):
  File "run.py", line 169, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 70, in train
    test_data, test_loader = self._get_data(flag='test')
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 36, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_factory.py", line 47, in data_provider
    data_set = Data(
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_loader.py", line 957, in __init__
    self.test_labels = np.load(os.path.join(root_path, "labels", file))
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/numpy/lib/npyio.py", line 407, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: './all_datasets/SMD/labels/machine-1-1_test.npy'
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 284
val 56
Traceback (most recent call last):
  File "run.py", line 169, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 70, in train
    test_data, test_loader = self._get_data(flag='test')
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 36, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_factory.py", line 47, in data_provider
    data_set = Data(
  File "/root/taowei/Project/LLM/MADLLM/data_provider/data_loader.py", line 958, in __init__
    self.test_labels = np.load(os.path.join(root_path, file))
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/numpy/lib/npyio.py", line 407, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: './all_datasets/SMD/machine-machine-1-1_test.npy_labels.npy'
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 284
val 56
test 284
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7637374401092529
Epoch: 1, Steps: 3 | Train Loss: 0.1638082 Vali Loss: 0.1746712 Test Loss: 0.2176657
Validation loss decreased (inf --> 0.174671).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7667539119720459
Epoch: 2, Steps: 3 | Train Loss: 0.1578649 Vali Loss: 0.1737433 Test Loss: 0.2160930
Validation loss decreased (0.174671 --> 0.173743).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7619538307189941
Epoch: 3, Steps: 3 | Train Loss: 0.1724578 Vali Loss: 0.1733990 Test Loss: 0.2155019
Validation loss decreased (0.173743 --> 0.173399).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7653322219848633
Epoch: 4, Steps: 3 | Train Loss: 0.1730213 Vali Loss: 0.1732537 Test Loss: 0.2152414
Validation loss decreased (0.173399 --> 0.173254).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7575957775115967
Epoch: 5, Steps: 3 | Train Loss: 0.1579719 Vali Loss: 0.1731861 Test Loss: 0.2151210
Validation loss decreased (0.173254 --> 0.173186).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 284
train 284
Threshold : 2.1669196593761435
pred:    (28400,)
gt:      (1079200,)
Traceback (most recent call last):
  File "run.py", line 172, in <module>
    _, precision, recall, f_score, auc_score = exp.test(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 196, in test
    gt, pred = adjustment(gt, pred)
  File "/root/taowei/Project/LLM/MADLLM/utils/tools.py", line 116, in adjustment
    if gt[i] == 1 and pred[i] == 1 and not anomaly_state:
IndexError: index 602224 is out of bounds for axis 0 with size 28400
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 284
val 56
test 284
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7585554122924805
Epoch: 1, Steps: 3 | Train Loss: 0.1638086 Vali Loss: 0.1746707 Test Loss: 0.2176741
Validation loss decreased (inf --> 0.174671).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7580888271331787
Epoch: 2, Steps: 3 | Train Loss: 0.1578641 Vali Loss: 0.1737405 Test Loss: 0.2160925
Validation loss decreased (0.174671 --> 0.173741).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7549302577972412
Epoch: 3, Steps: 3 | Train Loss: 0.1728037 Vali Loss: 0.1733985 Test Loss: 0.2155020
Validation loss decreased (0.173741 --> 0.173399).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7587883472442627
Epoch: 4, Steps: 3 | Train Loss: 0.1730249 Vali Loss: 0.1732552 Test Loss: 0.2152428
Validation loss decreased (0.173399 --> 0.173255).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7620136737823486
Epoch: 5, Steps: 3 | Train Loss: 0.1579734 Vali Loss: 0.1731888 Test Loss: 0.2151228
Validation loss decreased (0.173255 --> 0.173189).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 284
train 284
Threshold : 2.1671776032447814
pred:    (28400,)
gt:      (28400,)
pred:  (28400,)
gt:    (28400,)
Accuracy : 0.9981, Precision : 0.9807, Recall : 1.0000, F-score : 0.9903, AUC : 0.9990
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7168722152709961
Epoch: 1, Steps: 2 | Train Loss: 0.6256219 Vali Loss: 0.6517218 Test Loss: 0.5980072
Validation loss decreased (inf --> 0.651722).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7279965877532959
Epoch: 2, Steps: 2 | Train Loss: 0.6182787 Vali Loss: 0.6470931 Test Loss: 0.5935168
Validation loss decreased (0.651722 --> 0.647093).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7230827808380127
Epoch: 3, Steps: 2 | Train Loss: 0.6073167 Vali Loss: 0.6449986 Test Loss: 0.5914730
Validation loss decreased (0.647093 --> 0.644999).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7322075366973877
Epoch: 4, Steps: 2 | Train Loss: 0.6054408 Vali Loss: 0.6440082 Test Loss: 0.5905123
Validation loss decreased (0.644999 --> 0.644008).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7330319881439209
Epoch: 5, Steps: 2 | Train Loss: 0.6120374 Vali Loss: 0.6435357 Test Loss: 0.5900528
Validation loss decreased (0.644008 --> 0.643536).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 12.415246725082358
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9935, Precision : 0.8530, Recall : 0.8672, F-score : 0.8600, AUC : 0.9318
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7425203323364258
Epoch: 1, Steps: 2 | Train Loss: 0.3200116 Vali Loss: 1.3369868 Test Loss: 0.2391961
Validation loss decreased (inf --> 1.336987).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7438836097717285
Epoch: 2, Steps: 2 | Train Loss: 0.3183649 Vali Loss: 1.3303331 Test Loss: 0.2381273
Validation loss decreased (1.336987 --> 1.330333).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.745844841003418
Epoch: 3, Steps: 2 | Train Loss: 0.2999343 Vali Loss: 1.3270607 Test Loss: 0.2376201
Validation loss decreased (1.330333 --> 1.327061).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7379202842712402
Epoch: 4, Steps: 2 | Train Loss: 0.2845749 Vali Loss: 1.3255171 Test Loss: 0.2373755
Validation loss decreased (1.327061 --> 1.325517).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7446002960205078
Epoch: 5, Steps: 2 | Train Loss: 0.2838484 Vali Loss: 1.3248215 Test Loss: 0.2372621
Validation loss decreased (1.325517 --> 1.324821).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 5.0809999346732555
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9926, Precision : 0.9735, Recall : 0.8078, F-score : 0.8829, AUC : 0.9035
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7542209625244141
Epoch: 1, Steps: 2 | Train Loss: 0.3176978 Vali Loss: 1.3680809 Test Loss: 0.2428879
Validation loss decreased (inf --> 1.368081).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7481868267059326
Epoch: 2, Steps: 2 | Train Loss: 0.2981763 Vali Loss: 1.3617516 Test Loss: 0.2414951
Validation loss decreased (1.368081 --> 1.361752).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7468345165252686
Epoch: 3, Steps: 2 | Train Loss: 0.3097106 Vali Loss: 1.3590759 Test Loss: 0.2408830
Validation loss decreased (1.361752 --> 1.359076).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.754920482635498
Epoch: 4, Steps: 2 | Train Loss: 0.3280299 Vali Loss: 1.3579109 Test Loss: 0.2406120
Validation loss decreased (1.359076 --> 1.357911).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7533879280090332
Epoch: 5, Steps: 2 | Train Loss: 0.3292591 Vali Loss: 1.3573394 Test Loss: 0.2404808
Validation loss decreased (1.357911 --> 1.357339).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 4.193379602432195
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9965, Precision : 0.9690, Recall : 0.9125, F-score : 0.9399, AUC : 0.9558
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7386503219604492
Epoch: 1, Steps: 2 | Train Loss: 0.4325780 Vali Loss: 0.6064325 Test Loss: 0.5204808
Validation loss decreased (inf --> 0.606432).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7447488307952881
Epoch: 2, Steps: 2 | Train Loss: 0.4235954 Vali Loss: 0.6032187 Test Loss: 0.5172788
Validation loss decreased (0.606432 --> 0.603219).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.743492603302002
Epoch: 3, Steps: 2 | Train Loss: 0.4267464 Vali Loss: 0.6018248 Test Loss: 0.5158960
Validation loss decreased (0.603219 --> 0.601825).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7486097812652588
Epoch: 4, Steps: 2 | Train Loss: 0.4217999 Vali Loss: 0.6011655 Test Loss: 0.5152360
Validation loss decreased (0.601825 --> 0.601166).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7417261600494385
Epoch: 5, Steps: 2 | Train Loss: 0.4190730 Vali Loss: 0.6008477 Test Loss: 0.5149212
Validation loss decreased (0.601166 --> 0.600848).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 7.609002947807229
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9972, Precision : 0.6024, Recall : 1.0000, F-score : 0.7519, AUC : 0.9986
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7563912868499756
Epoch: 1, Steps: 2 | Train Loss: 0.3335689 Vali Loss: 0.4308428 Test Loss: 0.2534448
Validation loss decreased (inf --> 0.430843).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7492961883544922
Epoch: 2, Steps: 2 | Train Loss: 0.3381926 Vali Loss: 0.4293892 Test Loss: 0.2520932
Validation loss decreased (0.430843 --> 0.429389).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7506179809570312
Epoch: 3, Steps: 2 | Train Loss: 0.3399202 Vali Loss: 0.4287767 Test Loss: 0.2515031
Validation loss decreased (0.429389 --> 0.428777).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7527031898498535
Epoch: 4, Steps: 2 | Train Loss: 0.3336567 Vali Loss: 0.4284970 Test Loss: 0.2512188
Validation loss decreased (0.428777 --> 0.428497).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7461631298065186
Epoch: 5, Steps: 2 | Train Loss: 0.3309348 Vali Loss: 0.4283653 Test Loss: 0.2510830
Validation loss decreased (0.428497 --> 0.428365).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.198485050201384
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9976, Precision : 0.9901, Recall : 0.9946, F-score : 0.9923, AUC : 0.9964
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7347831726074219
Epoch: 1, Steps: 2 | Train Loss: 0.3655419 Vali Loss: 0.3121459 Test Loss: 0.3030939
Validation loss decreased (inf --> 0.312146).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7370655536651611
Epoch: 2, Steps: 2 | Train Loss: 0.3481280 Vali Loss: 0.3103969 Test Loss: 0.3012598
Validation loss decreased (0.312146 --> 0.310397).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7411925792694092
Epoch: 3, Steps: 2 | Train Loss: 0.3457399 Vali Loss: 0.3096193 Test Loss: 0.3004386
Validation loss decreased (0.310397 --> 0.309619).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7512848377227783
Epoch: 4, Steps: 2 | Train Loss: 0.3593947 Vali Loss: 0.3092574 Test Loss: 0.3000577
Validation loss decreased (0.309619 --> 0.309257).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7392723560333252
Epoch: 5, Steps: 2 | Train Loss: 0.3475076 Vali Loss: 0.3090689 Test Loss: 0.2998700
Validation loss decreased (0.309257 --> 0.309069).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 10.603599963188108
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9984, Precision : 0.9876, Recall : 0.9971, F-score : 0.9923, AUC : 0.9978
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7405359745025635
Epoch: 1, Steps: 2 | Train Loss: 0.2547250 Vali Loss: 0.1946808 Test Loss: 0.1881684
Validation loss decreased (inf --> 0.194681).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.738776683807373
Epoch: 2, Steps: 2 | Train Loss: 0.2538039 Vali Loss: 0.1940295 Test Loss: 0.1876330
Validation loss decreased (0.194681 --> 0.194029).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7444617748260498
Epoch: 3, Steps: 2 | Train Loss: 0.2565915 Vali Loss: 0.1937542 Test Loss: 0.1874189
Validation loss decreased (0.194029 --> 0.193754).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7422704696655273
Epoch: 4, Steps: 2 | Train Loss: 0.2518676 Vali Loss: 0.1936308 Test Loss: 0.1873276
Validation loss decreased (0.193754 --> 0.193631).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7395954132080078
Epoch: 5, Steps: 2 | Train Loss: 0.2531573 Vali Loss: 0.1935711 Test Loss: 0.1872844
Validation loss decreased (0.193631 --> 0.193571).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 3.284129308462054
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9993, Precision : 0.9795, Recall : 1.0000, F-score : 0.9896, AUC : 0.9996
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7332377433776855
Epoch: 1, Steps: 2 | Train Loss: 0.5034005 Vali Loss: 0.4261346 Test Loss: 0.5128196
Validation loss decreased (inf --> 0.426135).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7455599308013916
Epoch: 2, Steps: 2 | Train Loss: 0.4953008 Vali Loss: 0.4226608 Test Loss: 0.5083823
Validation loss decreased (0.426135 --> 0.422661).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7518956661224365
Epoch: 3, Steps: 2 | Train Loss: 0.4978312 Vali Loss: 0.4210666 Test Loss: 0.5063746
Validation loss decreased (0.422661 --> 0.421067).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7487034797668457
Epoch: 4, Steps: 2 | Train Loss: 0.4810438 Vali Loss: 0.4203019 Test Loss: 0.5054030
Validation loss decreased (0.421067 --> 0.420302).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7390697002410889
Epoch: 5, Steps: 2 | Train Loss: 0.4863037 Vali Loss: 0.4199294 Test Loss: 0.5049243
Validation loss decreased (0.420302 --> 0.419929).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.9584540247916475
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9955, Precision : 0.9396, Recall : 0.9709, F-score : 0.9550, AUC : 0.9838
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7484798431396484
Epoch: 1, Steps: 2 | Train Loss: 0.5214776 Vali Loss: 0.6717181 Test Loss: 0.6079009
Validation loss decreased (inf --> 0.671718).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7455601692199707
Epoch: 2, Steps: 2 | Train Loss: 0.5131958 Vali Loss: 0.6664577 Test Loss: 0.6030633
Validation loss decreased (0.671718 --> 0.666458).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.742767333984375
Epoch: 3, Steps: 2 | Train Loss: 0.5116726 Vali Loss: 0.6641267 Test Loss: 0.6009009
Validation loss decreased (0.666458 --> 0.664127).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7432124614715576
Epoch: 4, Steps: 2 | Train Loss: 0.5083334 Vali Loss: 0.6630284 Test Loss: 0.5998813
Validation loss decreased (0.664127 --> 0.663028).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7500896453857422
Epoch: 5, Steps: 2 | Train Loss: 0.5083821 Vali Loss: 0.6624995 Test Loss: 0.5993854
Validation loss decreased (0.663028 --> 0.662500).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 236
Threshold : 7.992051873207067
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9957, Precision : 0.9649, Recall : 1.0000, F-score : 0.9821, AUC : 0.9975
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7391495704650879
Epoch: 1, Steps: 2 | Train Loss: 0.4384702 Vali Loss: 0.1200708 Test Loss: 0.4816616
Validation loss decreased (inf --> 0.120071).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7497639656066895
Epoch: 2, Steps: 2 | Train Loss: 0.4247075 Vali Loss: 0.1194151 Test Loss: 0.4790528
Validation loss decreased (0.120071 --> 0.119415).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7420914173126221
Epoch: 3, Steps: 2 | Train Loss: 0.4318210 Vali Loss: 0.1191332 Test Loss: 0.4779039
Validation loss decreased (0.119415 --> 0.119133).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.755791187286377
Epoch: 4, Steps: 2 | Train Loss: 0.3923392 Vali Loss: 0.1190025 Test Loss: 0.4773376
Validation loss decreased (0.119133 --> 0.119002).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7449500560760498
Epoch: 5, Steps: 2 | Train Loss: 0.3930135 Vali Loss: 0.1189395 Test Loss: 0.4770757
Validation loss decreased (0.119002 --> 0.118939).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 6.513231863975449
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9985, Precision : 0.8820, Recall : 1.0000, F-score : 0.9373, AUC : 0.9992
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7430965900421143
Epoch: 1, Steps: 2 | Train Loss: 0.2806472 Vali Loss: 0.2160667 Test Loss: 0.2837396
Validation loss decreased (inf --> 0.216067).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7507882118225098
Epoch: 2, Steps: 2 | Train Loss: 0.2740177 Vali Loss: 0.2137466 Test Loss: 0.2813051
Validation loss decreased (0.216067 --> 0.213747).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7515273094177246
Epoch: 3, Steps: 2 | Train Loss: 0.2754477 Vali Loss: 0.2126990 Test Loss: 0.2802140
Validation loss decreased (0.213747 --> 0.212699).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7426056861877441
Epoch: 4, Steps: 2 | Train Loss: 0.2835445 Vali Loss: 0.2121881 Test Loss: 0.2796869
Validation loss decreased (0.212699 --> 0.212188).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7498283386230469
Epoch: 5, Steps: 2 | Train Loss: 0.2872090 Vali Loss: 0.2119389 Test Loss: 0.2794321
Validation loss decreased (0.212188 --> 0.211939).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 3.4847001278399454
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9993, Precision : 0.9906, Recall : 1.0000, F-score : 0.9953, AUC : 0.9996
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7405185699462891
Epoch: 1, Steps: 2 | Train Loss: 0.3113314 Vali Loss: 0.3128843 Test Loss: 0.2656308
Validation loss decreased (inf --> 0.312884).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7482187747955322
Epoch: 2, Steps: 2 | Train Loss: 0.3127956 Vali Loss: 0.3106263 Test Loss: 0.2637468
Validation loss decreased (0.312884 --> 0.310626).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7510924339294434
Epoch: 3, Steps: 2 | Train Loss: 0.3224912 Vali Loss: 0.3095688 Test Loss: 0.2628010
Validation loss decreased (0.310626 --> 0.309569).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7447032928466797
Epoch: 4, Steps: 2 | Train Loss: 0.3173490 Vali Loss: 0.3090680 Test Loss: 0.2623533
Validation loss decreased (0.309569 --> 0.309068).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7487919330596924
Epoch: 5, Steps: 2 | Train Loss: 0.3053089 Vali Loss: 0.3088254 Test Loss: 0.2621264
Validation loss decreased (0.309068 --> 0.308825).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 4.568082721233346
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9977, Precision : 0.9667, Recall : 0.9786, F-score : 0.9726, AUC : 0.9886
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8210551738739014
Epoch: 1, Steps: 3 | Train Loss: 0.2876988 Vali Loss: 0.2275623 Test Loss: 0.4817427
Validation loss decreased (inf --> 0.227562).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8284173011779785
Epoch: 2, Steps: 3 | Train Loss: 0.2519869 Vali Loss: 0.2251859 Test Loss: 0.4763169
Validation loss decreased (0.227562 --> 0.225186).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8248655796051025
Epoch: 3, Steps: 3 | Train Loss: 0.2997243 Vali Loss: 0.2241555 Test Loss: 0.4742501
Validation loss decreased (0.225186 --> 0.224156).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8179993629455566
Epoch: 4, Steps: 3 | Train Loss: 0.2640272 Vali Loss: 0.2236776 Test Loss: 0.4737541
Validation loss decreased (0.224156 --> 0.223678).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.8226289749145508
Epoch: 5, Steps: 3 | Train Loss: 0.2449603 Vali Loss: 0.2234448 Test Loss: 0.4732955
Validation loss decreased (0.223678 --> 0.223445).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.259007289409621
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9970, Precision : 0.8532, Recall : 0.9599, F-score : 0.9034, AUC : 0.9787
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7406439781188965
Epoch: 1, Steps: 2 | Train Loss: 0.2725383 Vali Loss: 0.3143403 Test Loss: 0.2568907
Validation loss decreased (inf --> 0.314340).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7463889122009277
Epoch: 2, Steps: 2 | Train Loss: 0.2681524 Vali Loss: 0.3121785 Test Loss: 0.2552969
Validation loss decreased (0.314340 --> 0.312178).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7593543529510498
Epoch: 3, Steps: 2 | Train Loss: 0.2737024 Vali Loss: 0.3111991 Test Loss: 0.2545786
Validation loss decreased (0.312178 --> 0.311199).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.755465030670166
Epoch: 4, Steps: 2 | Train Loss: 0.2615677 Vali Loss: 0.3107308 Test Loss: 0.2542377
Validation loss decreased (0.311199 --> 0.310731).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7422492504119873
Epoch: 5, Steps: 2 | Train Loss: 0.2607835 Vali Loss: 0.3105075 Test Loss: 0.2540700
Validation loss decreased (0.310731 --> 0.310508).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.837525622844662
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9984, Precision : 0.9260, Recall : 0.9904, F-score : 0.9571, AUC : 0.9945
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7465903759002686
Epoch: 1, Steps: 2 | Train Loss: 0.1938443 Vali Loss: 0.2811816 Test Loss: 0.2279437
Validation loss decreased (inf --> 0.281182).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7484660148620605
Epoch: 2, Steps: 2 | Train Loss: 0.1909865 Vali Loss: 0.2784774 Test Loss: 0.2256448
Validation loss decreased (0.281182 --> 0.278477).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.762394905090332
Epoch: 3, Steps: 2 | Train Loss: 0.1911237 Vali Loss: 0.2772230 Test Loss: 0.2245810
Validation loss decreased (0.278477 --> 0.277223).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7437150478363037
Epoch: 4, Steps: 2 | Train Loss: 0.1881810 Vali Loss: 0.2766216 Test Loss: 0.2240720
Validation loss decreased (0.277223 --> 0.276622).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7545802593231201
Epoch: 5, Steps: 2 | Train Loss: 0.1891616 Vali Loss: 0.2763283 Test Loss: 0.2238246
Validation loss decreased (0.276622 --> 0.276328).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 3.0741313457488944
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9995, Precision : 0.9360, Recall : 1.0000, F-score : 0.9670, AUC : 0.9998
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8219118118286133
Epoch: 1, Steps: 3 | Train Loss: 0.2656197 Vali Loss: 0.1908239 Test Loss: 0.2811588
Validation loss decreased (inf --> 0.190824).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8225264549255371
Epoch: 2, Steps: 3 | Train Loss: 0.2710502 Vali Loss: 0.1890881 Test Loss: 0.2792617
Validation loss decreased (0.190824 --> 0.189088).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8263630867004395
Epoch: 3, Steps: 3 | Train Loss: 0.2678093 Vali Loss: 0.1883457 Test Loss: 0.2784625
Validation loss decreased (0.189088 --> 0.188346).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.826977014541626
Epoch: 4, Steps: 3 | Train Loss: 0.3334694 Vali Loss: 0.1880044 Test Loss: 0.2780958
Validation loss decreased (0.188346 --> 0.188004).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.828728437423706
Epoch: 5, Steps: 3 | Train Loss: 0.2699101 Vali Loss: 0.1878473 Test Loss: 0.2779268
Validation loss decreased (0.188004 --> 0.187847).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.945809907913179
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9985, Precision : 0.9761, Recall : 1.0000, F-score : 0.9879, AUC : 0.9992
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8306045532226562
Epoch: 1, Steps: 3 | Train Loss: 0.3390395 Vali Loss: 0.2674246 Test Loss: 0.3547639
Validation loss decreased (inf --> 0.267425).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8206627368927002
Epoch: 2, Steps: 3 | Train Loss: 0.3343784 Vali Loss: 0.2643506 Test Loss: 0.3511046
Validation loss decreased (0.267425 --> 0.264351).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8321571350097656
Epoch: 3, Steps: 3 | Train Loss: 0.2961439 Vali Loss: 0.2630461 Test Loss: 0.3495429
Validation loss decreased (0.264351 --> 0.263046).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8277261257171631
Epoch: 4, Steps: 3 | Train Loss: 0.3042244 Vali Loss: 0.2624448 Test Loss: 0.3488275
Validation loss decreased (0.263046 --> 0.262445).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.8290920257568359
Epoch: 5, Steps: 3 | Train Loss: 0.2961076 Vali Loss: 0.2621593 Test Loss: 0.3484875
Validation loss decreased (0.262445 --> 0.262159).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.162578361034391
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9941, Precision : 0.6583, Recall : 0.9318, F-score : 0.7715, AUC : 0.9633
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7463181018829346
Epoch: 1, Steps: 2 | Train Loss: 0.5121786 Vali Loss: 0.6114725 Test Loss: 0.5069865
Validation loss decreased (inf --> 0.611472).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7498764991760254
Epoch: 2, Steps: 2 | Train Loss: 0.5077961 Vali Loss: 0.6076810 Test Loss: 0.5036790
Validation loss decreased (0.611472 --> 0.607681).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7470271587371826
Epoch: 3, Steps: 2 | Train Loss: 0.5055789 Vali Loss: 0.6060438 Test Loss: 0.5021883
Validation loss decreased (0.607681 --> 0.606044).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7469310760498047
Epoch: 4, Steps: 2 | Train Loss: 0.5018575 Vali Loss: 0.6052050 Test Loss: 0.5014714
Validation loss decreased (0.606044 --> 0.605205).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7515788078308105
Epoch: 5, Steps: 2 | Train Loss: 0.5059943 Vali Loss: 0.6048274 Test Loss: 0.5011376
Validation loss decreased (0.605205 --> 0.604827).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 9.512114992141708
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9962, Precision : 0.9277, Recall : 0.9955, F-score : 0.9604, AUC : 0.9958
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7508425712585449
Epoch: 1, Steps: 2 | Train Loss: 0.3652771 Vali Loss: 0.1686011 Test Loss: 0.2918420
Validation loss decreased (inf --> 0.168601).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7436108589172363
Epoch: 2, Steps: 2 | Train Loss: 0.3666909 Vali Loss: 0.1671476 Test Loss: 0.2894627
Validation loss decreased (0.168601 --> 0.167148).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7484321594238281
Epoch: 3, Steps: 2 | Train Loss: 0.3498196 Vali Loss: 0.1664803 Test Loss: 0.2885008
Validation loss decreased (0.167148 --> 0.166480).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.745305061340332
Epoch: 4, Steps: 2 | Train Loss: 0.3638920 Vali Loss: 0.1661676 Test Loss: 0.2879670
Validation loss decreased (0.166480 --> 0.166168).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7511816024780273
Epoch: 5, Steps: 2 | Train Loss: 0.3573403 Vali Loss: 0.1660124 Test Loss: 0.2876997
Validation loss decreased (0.166168 --> 0.166012).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 3.351909745931625
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9972, Precision : 0.9248, Recall : 0.9731, F-score : 0.9483, AUC : 0.9855
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7502517700195312
Epoch: 1, Steps: 2 | Train Loss: 0.3598076 Vali Loss: 0.3706328 Test Loss: 0.3318915
Validation loss decreased (inf --> 0.370633).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7495701313018799
Epoch: 2, Steps: 2 | Train Loss: 0.3587905 Vali Loss: 0.3683002 Test Loss: 0.3300394
Validation loss decreased (0.370633 --> 0.368300).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7529411315917969
Epoch: 3, Steps: 2 | Train Loss: 0.3539111 Vali Loss: 0.3672639 Test Loss: 0.3292109
Validation loss decreased (0.368300 --> 0.367264).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7454299926757812
Epoch: 4, Steps: 2 | Train Loss: 0.3539211 Vali Loss: 0.3665330 Test Loss: 0.3288180
Validation loss decreased (0.367264 --> 0.366533).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7471702098846436
Epoch: 5, Steps: 2 | Train Loss: 0.3548693 Vali Loss: 0.3663001 Test Loss: 0.3286334
Validation loss decreased (0.366533 --> 0.366300).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 2.9655325245857114
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9967, Precision : 0.9252, Recall : 1.0000, F-score : 0.9611, AUC : 0.9983
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7491598129272461
Epoch: 1, Steps: 2 | Train Loss: 0.3291502 Vali Loss: 0.4150116 Test Loss: 0.3343876
Validation loss decreased (inf --> 0.415012).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.749889612197876
Epoch: 2, Steps: 2 | Train Loss: 0.3263817 Vali Loss: 0.4122742 Test Loss: 0.3323348
Validation loss decreased (0.415012 --> 0.412274).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7464323043823242
Epoch: 3, Steps: 2 | Train Loss: 0.3242490 Vali Loss: 0.4109887 Test Loss: 0.3313942
Validation loss decreased (0.412274 --> 0.410989).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7454729080200195
Epoch: 4, Steps: 2 | Train Loss: 0.3247795 Vali Loss: 0.4103678 Test Loss: 0.3309479
Validation loss decreased (0.410989 --> 0.410368).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7495441436767578
Epoch: 5, Steps: 2 | Train Loss: 0.3190114 Vali Loss: 0.4100686 Test Loss: 0.3307339
Validation loss decreased (0.410368 --> 0.410069).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.668881192207217
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9959, Precision : 0.8313, Recall : 0.9718, F-score : 0.8961, AUC : 0.9841
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8309757709503174
Epoch: 1, Steps: 3 | Train Loss: 0.2284597 Vali Loss: 0.2181589 Test Loss: 0.4232440
Validation loss decreased (inf --> 0.218159).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8407573699951172
Epoch: 2, Steps: 3 | Train Loss: 0.2169042 Vali Loss: 0.2149655 Test Loss: 0.4171194
Validation loss decreased (0.218159 --> 0.214965).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8292868137359619
Epoch: 3, Steps: 3 | Train Loss: 0.2043777 Vali Loss: 0.2135455 Test Loss: 0.4141915
Validation loss decreased (0.214965 --> 0.213545).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8313326835632324
Epoch: 4, Steps: 3 | Train Loss: 0.1797907 Vali Loss: 0.2128861 Test Loss: 0.4129390
Validation loss decreased (0.213545 --> 0.212886).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.831540584564209
Epoch: 5, Steps: 3 | Train Loss: 0.1824309 Vali Loss: 0.2125774 Test Loss: 0.4122400
Validation loss decreased (0.212886 --> 0.212577).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 5.470910313129411
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9972, Precision : 0.9379, Recall : 1.0000, F-score : 0.9680, AUC : 0.9986
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.850165843963623
Epoch: 1, Steps: 3 | Train Loss: 0.4306871 Vali Loss: 0.2828832 Test Loss: 0.2329675
Validation loss decreased (inf --> 0.282883).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8319735527038574
Epoch: 2, Steps: 3 | Train Loss: 0.5316614 Vali Loss: 0.2802423 Test Loss: 0.2308678
Validation loss decreased (0.282883 --> 0.280242).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8523581027984619
Epoch: 3, Steps: 3 | Train Loss: 0.4074241 Vali Loss: 0.2791155 Test Loss: 0.2300035
Validation loss decreased (0.280242 --> 0.279116).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8367016315460205
Epoch: 4, Steps: 3 | Train Loss: 0.4026501 Vali Loss: 0.2785968 Test Loss: 0.2296057
Validation loss decreased (0.279116 --> 0.278597).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.8275437355041504
Epoch: 5, Steps: 3 | Train Loss: 0.4247617 Vali Loss: 0.2783494 Test Loss: 0.2294202
Validation loss decreased (0.278597 --> 0.278349).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.889771487712844
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9995, Precision : 0.9709, Recall : 1.0000, F-score : 0.9852, AUC : 0.9998
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8207719326019287
Epoch: 1, Steps: 3 | Train Loss: 0.3894267 Vali Loss: 0.2252647 Test Loss: 0.3849653
Validation loss decreased (inf --> 0.225265).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8362774848937988
Epoch: 2, Steps: 3 | Train Loss: 0.3056823 Vali Loss: 0.2240622 Test Loss: 0.3828766
Validation loss decreased (0.225265 --> 0.224062).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8231692314147949
Epoch: 3, Steps: 3 | Train Loss: 0.3205933 Vali Loss: 0.2235694 Test Loss: 0.3820115
Validation loss decreased (0.224062 --> 0.223569).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8251361846923828
Epoch: 4, Steps: 3 | Train Loss: 0.3798941 Vali Loss: 0.2233541 Test Loss: 0.3816195
Validation loss decreased (0.223569 --> 0.223354).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.8281242847442627
Epoch: 5, Steps: 3 | Train Loss: 0.3221899 Vali Loss: 0.2232523 Test Loss: 0.3814342
Validation loss decreased (0.223354 --> 0.223252).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.429553875923107
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9952, Precision : 0.9092, Recall : 1.0000, F-score : 0.9524, AUC : 0.9975
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8236238956451416
Epoch: 1, Steps: 3 | Train Loss: 0.3039810 Vali Loss: 0.2808018 Test Loss: 0.3557783
Validation loss decreased (inf --> 0.280802).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.831514835357666
Epoch: 2, Steps: 3 | Train Loss: 0.3037809 Vali Loss: 0.2778525 Test Loss: 0.3520809
Validation loss decreased (0.280802 --> 0.277853).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8312313556671143
Epoch: 3, Steps: 3 | Train Loss: 0.2967011 Vali Loss: 0.2766702 Test Loss: 0.3505566
Validation loss decreased (0.277853 --> 0.276670).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.834857702255249
Epoch: 4, Steps: 3 | Train Loss: 0.3078576 Vali Loss: 0.2761115 Test Loss: 0.3498571
Validation loss decreased (0.276670 --> 0.276111).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.8289816379547119
Epoch: 5, Steps: 3 | Train Loss: 0.2951961 Vali Loss: 0.2757841 Test Loss: 0.3495232
Validation loss decreased (0.276111 --> 0.275784).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 3.4654903864860422
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9967, Precision : 0.7613, Recall : 1.0000, F-score : 0.8645, AUC : 0.9983
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-10_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7768630981445312
Epoch: 1, Steps: 2 | Train Loss: 0.1986690 Vali Loss: 0.0532458 Test Loss: 0.1652527
Validation loss decreased (inf --> 0.053246).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7499089241027832
Epoch: 2, Steps: 2 | Train Loss: 0.1935885 Vali Loss: 0.0530224 Test Loss: 0.1645469
Validation loss decreased (0.053246 --> 0.053022).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7725682258605957
Epoch: 3, Steps: 2 | Train Loss: 0.1975857 Vali Loss: 0.0529464 Test Loss: 0.1642325
Validation loss decreased (0.053022 --> 0.052946).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7480556964874268
Epoch: 4, Steps: 2 | Train Loss: 0.1959171 Vali Loss: 0.0529118 Test Loss: 0.1640862
Validation loss decreased (0.052946 --> 0.052912).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7523212432861328
Epoch: 5, Steps: 2 | Train Loss: 0.1922603 Vali Loss: 0.0528950 Test Loss: 0.1640106
Validation loss decreased (0.052912 --> 0.052895).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-10_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 2.3371075522899076
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9970, Precision : 0.9365, Recall : 1.0000, F-score : 0.9672, AUC : 0.9984
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-11_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 286
val 57
test 286
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8219420909881592
Epoch: 1, Steps: 3 | Train Loss: 0.2794441 Vali Loss: 0.1642873 Test Loss: 0.4675855
Validation loss decreased (inf --> 0.164287).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8244261741638184
Epoch: 2, Steps: 3 | Train Loss: 0.2804284 Vali Loss: 0.1633432 Test Loss: 0.4651566
Validation loss decreased (0.164287 --> 0.163343).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8190655708312988
Epoch: 3, Steps: 3 | Train Loss: 0.2683683 Vali Loss: 0.1629685 Test Loss: 0.4643361
Validation loss decreased (0.163343 --> 0.162969).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8263802528381348
Epoch: 4, Steps: 3 | Train Loss: 0.2748507 Vali Loss: 0.1628013 Test Loss: 0.4639728
Validation loss decreased (0.162969 --> 0.162801).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.8198013305664062
Epoch: 5, Steps: 3 | Train Loss: 0.2674224 Vali Loss: 0.1627230 Test Loss: 0.4637922
Validation loss decreased (0.162801 --> 0.162723).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-11_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 286
train 286
Threshold : 1.673153670430091
pred:    (28600,)
gt:      (28600,)
pred:  (28600,)
gt:    (28600,)
Accuracy : 0.9965, Precision : 0.6622, Recall : 1.0000, F-score : 0.7968, AUC : 0.9982
Mean Value:
Traceback (most recent call last):
  File "run.py", line 181, in <module>
    accuracy, precision,
NameError: name 'accuracy' is not defined
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 284
val 56
test 284
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7346727848052979
Epoch: 1, Steps: 3 | Train Loss: 0.1638081 Vali Loss: 0.1746697 Test Loss: 0.2176736
Validation loss decreased (inf --> 0.174670).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7412722110748291
Epoch: 2, Steps: 3 | Train Loss: 0.1578614 Vali Loss: 0.1737390 Test Loss: 0.2160902
Validation loss decreased (0.174670 --> 0.173739).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7435417175292969
Epoch: 3, Steps: 3 | Train Loss: 0.1727907 Vali Loss: 0.1733962 Test Loss: 0.2155000
Validation loss decreased (0.173739 --> 0.173396).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7427666187286377
Epoch: 4, Steps: 3 | Train Loss: 0.1730202 Vali Loss: 0.1732521 Test Loss: 0.2152399
Validation loss decreased (0.173396 --> 0.173252).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7375016212463379
Epoch: 5, Steps: 3 | Train Loss: 0.1579712 Vali Loss: 0.1731856 Test Loss: 0.2151192
Validation loss decreased (0.173252 --> 0.173186).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 284
train 284
Threshold : 2.1673807716369624
pred:    (28400,)
gt:      (28400,)
pred:  (28400,)
gt:    (28400,)
Accuracy : 0.9981, Precision : 0.9803, Recall : 1.0000, F-score : 0.9901, AUC : 0.9989
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6978049278259277
Epoch: 1, Steps: 2 | Train Loss: 0.6256256 Vali Loss: 0.6517221 Test Loss: 0.5980081
Validation loss decreased (inf --> 0.651722).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7089834213256836
Epoch: 2, Steps: 2 | Train Loss: 0.6182883 Vali Loss: 0.6470879 Test Loss: 0.5935132
Validation loss decreased (0.651722 --> 0.647088).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7168474197387695
Epoch: 3, Steps: 2 | Train Loss: 0.6073194 Vali Loss: 0.6449934 Test Loss: 0.5914712
Validation loss decreased (0.647088 --> 0.644993).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7025830745697021
Epoch: 4, Steps: 2 | Train Loss: 0.6054415 Vali Loss: 0.6440065 Test Loss: 0.5905077
Validation loss decreased (0.644993 --> 0.644006).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7012438774108887
Epoch: 5, Steps: 2 | Train Loss: 0.6120449 Vali Loss: 0.6435324 Test Loss: 0.5900473
Validation loss decreased (0.644006 --> 0.643532).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 12.415070466995202
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9935, Precision : 0.8530, Recall : 0.8672, F-score : 0.8600, AUC : 0.9318
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7042360305786133
Epoch: 1, Steps: 2 | Train Loss: 0.3200120 Vali Loss: 1.3369793 Test Loss: 0.2391960
Validation loss decreased (inf --> 1.336979).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7059063911437988
Epoch: 2, Steps: 2 | Train Loss: 0.3183676 Vali Loss: 1.3303159 Test Loss: 0.2381254
Validation loss decreased (1.336979 --> 1.330316).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.708383321762085
Epoch: 3, Steps: 2 | Train Loss: 0.2999384 Vali Loss: 1.3270562 Test Loss: 0.2376235
Validation loss decreased (1.330316 --> 1.327056).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7118930816650391
Epoch: 4, Steps: 2 | Train Loss: 0.2845696 Vali Loss: 1.3255132 Test Loss: 0.2373752
Validation loss decreased (1.327056 --> 1.325513).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7092936038970947
Epoch: 5, Steps: 2 | Train Loss: 0.2838438 Vali Loss: 1.3248031 Test Loss: 0.2372636
Validation loss decreased (1.325513 --> 1.324803).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 5.080320167541446
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9926, Precision : 0.9735, Recall : 0.8078, F-score : 0.8829, AUC : 0.9035
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7143690586090088
Epoch: 1, Steps: 2 | Train Loss: 0.3177030 Vali Loss: 1.3680774 Test Loss: 0.2428881
Validation loss decreased (inf --> 1.368077).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7231235504150391
Epoch: 2, Steps: 2 | Train Loss: 0.2981747 Vali Loss: 1.3617491 Test Loss: 0.2414948
Validation loss decreased (1.368077 --> 1.361749).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7191855907440186
Epoch: 3, Steps: 2 | Train Loss: 0.3097093 Vali Loss: 1.3590661 Test Loss: 0.2408848
Validation loss decreased (1.361749 --> 1.359066).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7231640815734863
Epoch: 4, Steps: 2 | Train Loss: 0.3280306 Vali Loss: 1.3579159 Test Loss: 0.2406136
Validation loss decreased (1.359066 --> 1.357916).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7279341220855713
Epoch: 5, Steps: 2 | Train Loss: 0.3292584 Vali Loss: 1.3573406 Test Loss: 0.2404799
Validation loss decreased (1.357916 --> 1.357341).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 4.193416359424535
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9965, Precision : 0.9690, Recall : 0.9125, F-score : 0.9399, AUC : 0.9558
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7210731506347656
Epoch: 1, Steps: 2 | Train Loss: 0.4325778 Vali Loss: 0.6064327 Test Loss: 0.5204819
Validation loss decreased (inf --> 0.606433).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7201881408691406
Epoch: 2, Steps: 2 | Train Loss: 0.4236028 Vali Loss: 0.6032202 Test Loss: 0.5172800
Validation loss decreased (0.606433 --> 0.603220).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.721951961517334
Epoch: 3, Steps: 2 | Train Loss: 0.4267676 Vali Loss: 0.6018277 Test Loss: 0.5158975
Validation loss decreased (0.603220 --> 0.601828).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7217705249786377
Epoch: 4, Steps: 2 | Train Loss: 0.4218047 Vali Loss: 0.6011688 Test Loss: 0.5152398
Validation loss decreased (0.601828 --> 0.601169).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7190980911254883
Epoch: 5, Steps: 2 | Train Loss: 0.4190774 Vali Loss: 0.6008503 Test Loss: 0.5149221
Validation loss decreased (0.601169 --> 0.600850).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 7.609128985404887
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9972, Precision : 0.6024, Recall : 1.0000, F-score : 0.7519, AUC : 0.9986
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7450268268585205
Epoch: 1, Steps: 2 | Train Loss: 0.3335695 Vali Loss: 0.4308420 Test Loss: 0.2534437
Validation loss decreased (inf --> 0.430842).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7537643909454346
Epoch: 2, Steps: 2 | Train Loss: 0.3381922 Vali Loss: 0.4293886 Test Loss: 0.2520918
Validation loss decreased (0.430842 --> 0.429389).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7196285724639893
Epoch: 3, Steps: 2 | Train Loss: 0.3399220 Vali Loss: 0.4287746 Test Loss: 0.2515029
Validation loss decreased (0.429389 --> 0.428775).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7164266109466553
Epoch: 4, Steps: 2 | Train Loss: 0.3336551 Vali Loss: 0.4284976 Test Loss: 0.2512183
Validation loss decreased (0.428775 --> 0.428498).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7145183086395264
Epoch: 5, Steps: 2 | Train Loss: 0.3309336 Vali Loss: 0.4283655 Test Loss: 0.2510823
Validation loss decreased (0.428498 --> 0.428365).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.196518099307976
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9976, Precision : 0.9901, Recall : 0.9946, F-score : 0.9923, AUC : 0.9964
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6914730072021484
Epoch: 1, Steps: 2 | Train Loss: 0.3655428 Vali Loss: 0.3121474 Test Loss: 0.3030954
Validation loss decreased (inf --> 0.312147).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7000346183776855
Epoch: 2, Steps: 2 | Train Loss: 0.3481274 Vali Loss: 0.3103938 Test Loss: 0.3012604
Validation loss decreased (0.312147 --> 0.310394).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6994955539703369
Epoch: 3, Steps: 2 | Train Loss: 0.3457408 Vali Loss: 0.3096187 Test Loss: 0.3004381
Validation loss decreased (0.310394 --> 0.309619).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7003655433654785
Epoch: 4, Steps: 2 | Train Loss: 0.3593938 Vali Loss: 0.3092465 Test Loss: 0.3000562
Validation loss decreased (0.309619 --> 0.309246).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.701923131942749
Epoch: 5, Steps: 2 | Train Loss: 0.3475068 Vali Loss: 0.3090676 Test Loss: 0.2998672
Validation loss decreased (0.309246 --> 0.309068).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 10.603335261344842
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9984, Precision : 0.9876, Recall : 0.9971, F-score : 0.9923, AUC : 0.9978
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7035260200500488
Epoch: 1, Steps: 2 | Train Loss: 0.2547241 Vali Loss: 0.1946812 Test Loss: 0.1881683
Validation loss decreased (inf --> 0.194681).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7092881202697754
Epoch: 2, Steps: 2 | Train Loss: 0.2538037 Vali Loss: 0.1940289 Test Loss: 0.1876331
Validation loss decreased (0.194681 --> 0.194029).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7054762840270996
Epoch: 3, Steps: 2 | Train Loss: 0.2565918 Vali Loss: 0.1937544 Test Loss: 0.1874178
Validation loss decreased (0.194029 --> 0.193754).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7031979560852051
Epoch: 4, Steps: 2 | Train Loss: 0.2518665 Vali Loss: 0.1936305 Test Loss: 0.1873269
Validation loss decreased (0.193754 --> 0.193630).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7091283798217773
Epoch: 5, Steps: 2 | Train Loss: 0.2531577 Vali Loss: 0.1935707 Test Loss: 0.1872845
Validation loss decreased (0.193630 --> 0.193571).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 3.284052869081408
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9993, Precision : 0.9795, Recall : 1.0000, F-score : 0.9896, AUC : 0.9996
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7039890289306641
Epoch: 1, Steps: 2 | Train Loss: 0.5034029 Vali Loss: 0.4261355 Test Loss: 0.5128226
Validation loss decreased (inf --> 0.426135).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7042183876037598
Epoch: 2, Steps: 2 | Train Loss: 0.4953003 Vali Loss: 0.4226635 Test Loss: 0.5083854
Validation loss decreased (0.426135 --> 0.422664).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7052173614501953
Epoch: 3, Steps: 2 | Train Loss: 0.4978351 Vali Loss: 0.4210701 Test Loss: 0.5063773
Validation loss decreased (0.422664 --> 0.421070).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7075855731964111
Epoch: 4, Steps: 2 | Train Loss: 0.4810410 Vali Loss: 0.4203052 Test Loss: 0.5054036
Validation loss decreased (0.421070 --> 0.420305).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7064850330352783
Epoch: 5, Steps: 2 | Train Loss: 0.4863086 Vali Loss: 0.4199333 Test Loss: 0.5049272
Validation loss decreased (0.420305 --> 0.419933).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.958748946189811
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9955, Precision : 0.9396, Recall : 0.9709, F-score : 0.9550, AUC : 0.9838
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7145631313323975
Epoch: 1, Steps: 2 | Train Loss: 0.5214756 Vali Loss: 0.6717157 Test Loss: 0.6079006
Validation loss decreased (inf --> 0.671716).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7135410308837891
Epoch: 2, Steps: 2 | Train Loss: 0.5131941 Vali Loss: 0.6664556 Test Loss: 0.6030605
Validation loss decreased (0.671716 --> 0.666456).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7156510353088379
Epoch: 3, Steps: 2 | Train Loss: 0.5116719 Vali Loss: 0.6641230 Test Loss: 0.6008980
Validation loss decreased (0.666456 --> 0.664123).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7218747138977051
Epoch: 4, Steps: 2 | Train Loss: 0.5083279 Vali Loss: 0.6630263 Test Loss: 0.5998777
Validation loss decreased (0.664123 --> 0.663026).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7111222743988037
Epoch: 5, Steps: 2 | Train Loss: 0.5083809 Vali Loss: 0.6624964 Test Loss: 0.5993817
Validation loss decreased (0.663026 --> 0.662496).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 236
Threshold : 7.992178354263278
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9957, Precision : 0.9649, Recall : 1.0000, F-score : 0.9821, AUC : 0.9975
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7131474018096924
Epoch: 1, Steps: 2 | Train Loss: 0.4384674 Vali Loss: 0.1200702 Test Loss: 0.4816620
Validation loss decreased (inf --> 0.120070).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7107512950897217
Epoch: 2, Steps: 2 | Train Loss: 0.4247060 Vali Loss: 0.1194149 Test Loss: 0.4790525
Validation loss decreased (0.120070 --> 0.119415).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7196199893951416
Epoch: 3, Steps: 2 | Train Loss: 0.4318220 Vali Loss: 0.1191333 Test Loss: 0.4779029
Validation loss decreased (0.119415 --> 0.119133).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7157180309295654
Epoch: 4, Steps: 2 | Train Loss: 0.3923404 Vali Loss: 0.1190023 Test Loss: 0.4773378
Validation loss decreased (0.119133 --> 0.119002).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7139954566955566
Epoch: 5, Steps: 2 | Train Loss: 0.3930136 Vali Loss: 0.1189393 Test Loss: 0.4770741
Validation loss decreased (0.119002 --> 0.118939).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 6.5129697299002824
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9985, Precision : 0.8820, Recall : 1.0000, F-score : 0.9373, AUC : 0.9992
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7082874774932861
Epoch: 1, Steps: 2 | Train Loss: 0.2806484 Vali Loss: 0.2160666 Test Loss: 0.2837395
Validation loss decreased (inf --> 0.216067).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7207858562469482
Epoch: 2, Steps: 2 | Train Loss: 0.2740178 Vali Loss: 0.2137472 Test Loss: 0.2813061
Validation loss decreased (0.216067 --> 0.213747).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7179701328277588
Epoch: 3, Steps: 2 | Train Loss: 0.2753725 Vali Loss: 0.2126765 Test Loss: 0.2801923
Validation loss decreased (0.213747 --> 0.212677).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7164812088012695
Epoch: 4, Steps: 2 | Train Loss: 0.2835213 Vali Loss: 0.2121563 Test Loss: 0.2796568
Validation loss decreased (0.212677 --> 0.212156).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7182769775390625
Epoch: 5, Steps: 2 | Train Loss: 0.2871588 Vali Loss: 0.2119025 Test Loss: 0.2793974
Validation loss decreased (0.212156 --> 0.211902).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 3.4816951763628867
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9993, Precision : 0.9906, Recall : 1.0000, F-score : 0.9953, AUC : 0.9996
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7042176723480225
Epoch: 1, Steps: 2 | Train Loss: 0.3113346 Vali Loss: 0.3128849 Test Loss: 0.2656317
Validation loss decreased (inf --> 0.312885).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.707115888595581
Epoch: 2, Steps: 2 | Train Loss: 0.3127947 Vali Loss: 0.3106239 Test Loss: 0.2637465
Validation loss decreased (0.312885 --> 0.310624).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7099411487579346
Epoch: 3, Steps: 2 | Train Loss: 0.3224880 Vali Loss: 0.3095671 Test Loss: 0.2628000
Validation loss decreased (0.310624 --> 0.309567).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7067513465881348
Epoch: 4, Steps: 2 | Train Loss: 0.3173488 Vali Loss: 0.3090671 Test Loss: 0.2623530
Validation loss decreased (0.309567 --> 0.309067).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7135283946990967
Epoch: 5, Steps: 2 | Train Loss: 0.3053038 Vali Loss: 0.3088228 Test Loss: 0.2621252
Validation loss decreased (0.309067 --> 0.308823).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 4.5680196857452176
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9977, Precision : 0.9667, Recall : 0.9786, F-score : 0.9726, AUC : 0.9886
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7900962829589844
Epoch: 1, Steps: 3 | Train Loss: 0.2876985 Vali Loss: 0.2275612 Test Loss: 0.4817446
Validation loss decreased (inf --> 0.227561).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8004813194274902
Epoch: 2, Steps: 3 | Train Loss: 0.2519858 Vali Loss: 0.2251853 Test Loss: 0.4763209
Validation loss decreased (0.227561 --> 0.225185).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7973816394805908
Epoch: 3, Steps: 3 | Train Loss: 0.2997260 Vali Loss: 0.2241556 Test Loss: 0.4742537
Validation loss decreased (0.225185 --> 0.224156).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7969226837158203
Epoch: 4, Steps: 3 | Train Loss: 0.2640267 Vali Loss: 0.2236778 Test Loss: 0.4737569
Validation loss decreased (0.224156 --> 0.223678).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7941081523895264
Epoch: 5, Steps: 3 | Train Loss: 0.2449618 Vali Loss: 0.2234464 Test Loss: 0.4733007
Validation loss decreased (0.223678 --> 0.223446).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.2588871121406395
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9970, Precision : 0.8532, Recall : 0.9599, F-score : 0.9034, AUC : 0.9787
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7211148738861084
Epoch: 1, Steps: 2 | Train Loss: 0.2725383 Vali Loss: 0.3143394 Test Loss: 0.2568907
Validation loss decreased (inf --> 0.314339).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7182273864746094
Epoch: 2, Steps: 2 | Train Loss: 0.2681525 Vali Loss: 0.3121774 Test Loss: 0.2552974
Validation loss decreased (0.314339 --> 0.312177).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7176179885864258
Epoch: 3, Steps: 2 | Train Loss: 0.2737065 Vali Loss: 0.3111995 Test Loss: 0.2545793
Validation loss decreased (0.312177 --> 0.311199).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7137861251831055
Epoch: 4, Steps: 2 | Train Loss: 0.2615741 Vali Loss: 0.3107331 Test Loss: 0.2542384
Validation loss decreased (0.311199 --> 0.310733).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7220008373260498
Epoch: 5, Steps: 2 | Train Loss: 0.2607901 Vali Loss: 0.3105091 Test Loss: 0.2540725
Validation loss decreased (0.310733 --> 0.310509).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.837513294219938
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9984, Precision : 0.9260, Recall : 0.9904, F-score : 0.9571, AUC : 0.9945
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7497432231903076
Epoch: 1, Steps: 2 | Train Loss: 0.1938434 Vali Loss: 0.2811847 Test Loss: 0.2279535
Validation loss decreased (inf --> 0.281185).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7217698097229004
Epoch: 2, Steps: 2 | Train Loss: 0.1910037 Vali Loss: 0.2784409 Test Loss: 0.2256528
Validation loss decreased (0.281185 --> 0.278441).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7347099781036377
Epoch: 3, Steps: 2 | Train Loss: 0.1911151 Vali Loss: 0.2771747 Test Loss: 0.2245799
Validation loss decreased (0.278441 --> 0.277175).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7301738262176514
Epoch: 4, Steps: 2 | Train Loss: 0.1881818 Vali Loss: 0.2765723 Test Loss: 0.2240680
Validation loss decreased (0.277175 --> 0.276572).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7263846397399902
Epoch: 5, Steps: 2 | Train Loss: 0.1891600 Vali Loss: 0.2762790 Test Loss: 0.2238199
Validation loss decreased (0.276572 --> 0.276279).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 3.0570307612418923
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9995, Precision : 0.9360, Recall : 1.0000, F-score : 0.9670, AUC : 0.9998
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7961618900299072
Epoch: 1, Steps: 3 | Train Loss: 0.2656212 Vali Loss: 0.1908242 Test Loss: 0.2811584
Validation loss decreased (inf --> 0.190824).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7940399646759033
Epoch: 2, Steps: 3 | Train Loss: 0.2710493 Vali Loss: 0.1890896 Test Loss: 0.2792628
Validation loss decreased (0.190824 --> 0.189090).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8068335056304932
Epoch: 3, Steps: 3 | Train Loss: 0.2678127 Vali Loss: 0.1883474 Test Loss: 0.2784633
Validation loss decreased (0.189090 --> 0.188347).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7964494228363037
Epoch: 4, Steps: 3 | Train Loss: 0.3334779 Vali Loss: 0.1880058 Test Loss: 0.2780972
Validation loss decreased (0.188347 --> 0.188006).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.8037302494049072
Epoch: 5, Steps: 3 | Train Loss: 0.2700008 Vali Loss: 0.1878493 Test Loss: 0.2779287
Validation loss decreased (0.188006 --> 0.187849).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.945773596763582
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9985, Precision : 0.9761, Recall : 1.0000, F-score : 0.9879, AUC : 0.9992
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.790848970413208
Epoch: 1, Steps: 3 | Train Loss: 0.3390404 Vali Loss: 0.2674210 Test Loss: 0.3547623
Validation loss decreased (inf --> 0.267421).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7934155464172363
Epoch: 2, Steps: 3 | Train Loss: 0.3343693 Vali Loss: 0.2643416 Test Loss: 0.3510963
Validation loss decreased (0.267421 --> 0.264342).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7936313152313232
Epoch: 3, Steps: 3 | Train Loss: 0.2961321 Vali Loss: 0.2630363 Test Loss: 0.3495327
Validation loss decreased (0.264342 --> 0.263036).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7982137203216553
Epoch: 4, Steps: 3 | Train Loss: 0.3042095 Vali Loss: 0.2624350 Test Loss: 0.3488164
Validation loss decreased (0.263036 --> 0.262435).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7981200218200684
Epoch: 5, Steps: 3 | Train Loss: 0.2960952 Vali Loss: 0.2621493 Test Loss: 0.3486218
Validation loss decreased (0.262435 --> 0.262149).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.162654824256895
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9941, Precision : 0.6583, Recall : 0.9318, F-score : 0.7715, AUC : 0.9633
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7329764366149902
Epoch: 1, Steps: 2 | Train Loss: 0.5121790 Vali Loss: 0.6114770 Test Loss: 0.5069876
Validation loss decreased (inf --> 0.611477).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7223458290100098
Epoch: 2, Steps: 2 | Train Loss: 0.5077986 Vali Loss: 0.6076883 Test Loss: 0.5036799
Validation loss decreased (0.611477 --> 0.607688).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7295482158660889
Epoch: 3, Steps: 2 | Train Loss: 0.5055848 Vali Loss: 0.6060541 Test Loss: 0.5021902
Validation loss decreased (0.607688 --> 0.606054).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7193403244018555
Epoch: 4, Steps: 2 | Train Loss: 0.5018642 Vali Loss: 0.6052179 Test Loss: 0.5014729
Validation loss decreased (0.606054 --> 0.605218).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7237424850463867
Epoch: 5, Steps: 2 | Train Loss: 0.5060009 Vali Loss: 0.6048320 Test Loss: 0.5011380
Validation loss decreased (0.605218 --> 0.604832).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 9.51184023857115
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9962, Precision : 0.9277, Recall : 0.9955, F-score : 0.9604, AUC : 0.9958
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.722482442855835
Epoch: 1, Steps: 2 | Train Loss: 0.3652779 Vali Loss: 0.1686009 Test Loss: 0.2920475
Validation loss decreased (inf --> 0.168601).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7200493812561035
Epoch: 2, Steps: 2 | Train Loss: 0.3666901 Vali Loss: 0.1671472 Test Loss: 0.2896501
Validation loss decreased (0.168601 --> 0.167147).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7235982418060303
Epoch: 3, Steps: 2 | Train Loss: 0.3498206 Vali Loss: 0.1664788 Test Loss: 0.2884996
Validation loss decreased (0.167147 --> 0.166479).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7280688285827637
Epoch: 4, Steps: 2 | Train Loss: 0.3638933 Vali Loss: 0.1661673 Test Loss: 0.2879674
Validation loss decreased (0.166479 --> 0.166167).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7223362922668457
Epoch: 5, Steps: 2 | Train Loss: 0.3573395 Vali Loss: 0.1660116 Test Loss: 0.2876996
Validation loss decreased (0.166167 --> 0.166012).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 3.35188978075981
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9972, Precision : 0.9248, Recall : 0.9731, F-score : 0.9483, AUC : 0.9855
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7215993404388428
Epoch: 1, Steps: 2 | Train Loss: 0.3598072 Vali Loss: 0.3706329 Test Loss: 0.3318894
Validation loss decreased (inf --> 0.370633).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7212460041046143
Epoch: 2, Steps: 2 | Train Loss: 0.3587873 Vali Loss: 0.3682973 Test Loss: 0.3300330
Validation loss decreased (0.370633 --> 0.368297).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7270543575286865
Epoch: 3, Steps: 2 | Train Loss: 0.3539036 Vali Loss: 0.3670181 Test Loss: 0.3292073
Validation loss decreased (0.368297 --> 0.367018).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.720102071762085
Epoch: 4, Steps: 2 | Train Loss: 0.3539170 Vali Loss: 0.3665321 Test Loss: 0.3288150
Validation loss decreased (0.367018 --> 0.366532).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7243149280548096
Epoch: 5, Steps: 2 | Train Loss: 0.3548657 Vali Loss: 0.3663015 Test Loss: 0.3286289
Validation loss decreased (0.366532 --> 0.366302).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 2.9655757260322444
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9967, Precision : 0.9252, Recall : 1.0000, F-score : 0.9611, AUC : 0.9983
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7212584018707275
Epoch: 1, Steps: 2 | Train Loss: 0.3291524 Vali Loss: 0.4150097 Test Loss: 0.3343876
Validation loss decreased (inf --> 0.415010).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7358493804931641
Epoch: 2, Steps: 2 | Train Loss: 0.3263800 Vali Loss: 0.4122785 Test Loss: 0.3323342
Validation loss decreased (0.415010 --> 0.412278).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7235872745513916
Epoch: 3, Steps: 2 | Train Loss: 0.3242497 Vali Loss: 0.4109859 Test Loss: 0.3313947
Validation loss decreased (0.412278 --> 0.410986).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7181766033172607
Epoch: 4, Steps: 2 | Train Loss: 0.3247809 Vali Loss: 0.4103666 Test Loss: 0.3309483
Validation loss decreased (0.410986 --> 0.410367).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7209265232086182
Epoch: 5, Steps: 2 | Train Loss: 0.3190117 Vali Loss: 0.4100727 Test Loss: 0.3307337
Validation loss decreased (0.410367 --> 0.410073).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.668892467021823
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9959, Precision : 0.8313, Recall : 0.9718, F-score : 0.8961, AUC : 0.9841
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8028790950775146
Epoch: 1, Steps: 3 | Train Loss: 0.2284648 Vali Loss: 0.2181597 Test Loss: 0.4232481
Validation loss decreased (inf --> 0.218160).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8026766777038574
Epoch: 2, Steps: 3 | Train Loss: 0.2169067 Vali Loss: 0.2149674 Test Loss: 0.4171237
Validation loss decreased (0.218160 --> 0.214967).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7991085052490234
Epoch: 3, Steps: 3 | Train Loss: 0.2043810 Vali Loss: 0.2135493 Test Loss: 0.4141957
Validation loss decreased (0.214967 --> 0.213549).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.79878830909729
Epoch: 4, Steps: 3 | Train Loss: 0.1797931 Vali Loss: 0.2128893 Test Loss: 0.4129401
Validation loss decreased (0.213549 --> 0.212889).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7989249229431152
Epoch: 5, Steps: 3 | Train Loss: 0.1824333 Vali Loss: 0.2125826 Test Loss: 0.4122491
Validation loss decreased (0.212889 --> 0.212583).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 5.4702044868469075
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9972, Precision : 0.9379, Recall : 1.0000, F-score : 0.9680, AUC : 0.9986
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7982659339904785
Epoch: 1, Steps: 3 | Train Loss: 0.4306858 Vali Loss: 0.2828830 Test Loss: 0.2329645
Validation loss decreased (inf --> 0.282883).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8126382827758789
Epoch: 2, Steps: 3 | Train Loss: 0.5316489 Vali Loss: 0.2802462 Test Loss: 0.2308705
Validation loss decreased (0.282883 --> 0.280246).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7907295227050781
Epoch: 3, Steps: 3 | Train Loss: 0.4074347 Vali Loss: 0.2791202 Test Loss: 0.2300079
Validation loss decreased (0.280246 --> 0.279120).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.795602560043335
Epoch: 4, Steps: 3 | Train Loss: 0.4026659 Vali Loss: 0.2786004 Test Loss: 0.2296118
Validation loss decreased (0.279120 --> 0.278600).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7951536178588867
Epoch: 5, Steps: 3 | Train Loss: 0.4247845 Vali Loss: 0.2783529 Test Loss: 0.2294251
Validation loss decreased (0.278600 --> 0.278353).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.889917643070206
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9995, Precision : 0.9709, Recall : 1.0000, F-score : 0.9852, AUC : 0.9998
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8004410266876221
Epoch: 1, Steps: 3 | Train Loss: 0.3894230 Vali Loss: 0.2252640 Test Loss: 0.3849610
Validation loss decreased (inf --> 0.225264).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8028874397277832
Epoch: 2, Steps: 3 | Train Loss: 0.3056821 Vali Loss: 0.2240623 Test Loss: 0.3828747
Validation loss decreased (0.225264 --> 0.224062).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.8154265880584717
Epoch: 3, Steps: 3 | Train Loss: 0.3205960 Vali Loss: 0.2235686 Test Loss: 0.3820094
Validation loss decreased (0.224062 --> 0.223569).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7988166809082031
Epoch: 4, Steps: 3 | Train Loss: 0.3798876 Vali Loss: 0.2233534 Test Loss: 0.3816178
Validation loss decreased (0.223569 --> 0.223353).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7946281433105469
Epoch: 5, Steps: 3 | Train Loss: 0.3221861 Vali Loss: 0.2232515 Test Loss: 0.3814303
Validation loss decreased (0.223353 --> 0.223252).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.429378278255412
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9952, Precision : 0.9092, Recall : 1.0000, F-score : 0.9524, AUC : 0.9975
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.786005973815918
Epoch: 1, Steps: 3 | Train Loss: 0.3039811 Vali Loss: 0.2808014 Test Loss: 0.3557774
Validation loss decreased (inf --> 0.280801).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8050980567932129
Epoch: 2, Steps: 3 | Train Loss: 0.3037790 Vali Loss: 0.2778513 Test Loss: 0.3520806
Validation loss decreased (0.280801 --> 0.277851).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7999026775360107
Epoch: 3, Steps: 3 | Train Loss: 0.2966902 Vali Loss: 0.2766699 Test Loss: 0.3505544
Validation loss decreased (0.277851 --> 0.276670).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8026506900787354
Epoch: 4, Steps: 3 | Train Loss: 0.3078567 Vali Loss: 0.2760905 Test Loss: 0.3498566
Validation loss decreased (0.276670 --> 0.276091).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7979192733764648
Epoch: 5, Steps: 3 | Train Loss: 0.2951946 Vali Loss: 0.2757384 Test Loss: 0.3495224
Validation loss decreased (0.276091 --> 0.275738).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 3.465465396642674
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9967, Precision : 0.7613, Recall : 1.0000, F-score : 0.8645, AUC : 0.9983
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-10_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7347903251647949
Epoch: 1, Steps: 2 | Train Loss: 0.1986691 Vali Loss: 0.0532457 Test Loss: 0.1652527
Validation loss decreased (inf --> 0.053246).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7153182029724121
Epoch: 2, Steps: 2 | Train Loss: 0.1935898 Vali Loss: 0.0530224 Test Loss: 0.1645471
Validation loss decreased (0.053246 --> 0.053022).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7103884220123291
Epoch: 3, Steps: 2 | Train Loss: 0.1975842 Vali Loss: 0.0529463 Test Loss: 0.1642319
Validation loss decreased (0.053022 --> 0.052946).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7111544609069824
Epoch: 4, Steps: 2 | Train Loss: 0.1959151 Vali Loss: 0.0529116 Test Loss: 0.1640851
Validation loss decreased (0.052946 --> 0.052912).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7098205089569092
Epoch: 5, Steps: 2 | Train Loss: 0.1922564 Vali Loss: 0.0528949 Test Loss: 0.1640099
Validation loss decreased (0.052912 --> 0.052895).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-10_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 2.337150381803458
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9970, Precision : 0.9365, Recall : 1.0000, F-score : 0.9672, AUC : 0.9984
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-11_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 286
val 57
test 286
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7876186370849609
Epoch: 1, Steps: 3 | Train Loss: 0.2794439 Vali Loss: 0.1642870 Test Loss: 0.4675794
Validation loss decreased (inf --> 0.164287).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.784963846206665
Epoch: 2, Steps: 3 | Train Loss: 0.2804298 Vali Loss: 0.1633432 Test Loss: 0.4651519
Validation loss decreased (0.164287 --> 0.163343).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7952446937561035
Epoch: 3, Steps: 3 | Train Loss: 0.2683687 Vali Loss: 0.1629682 Test Loss: 0.4643353
Validation loss decreased (0.163343 --> 0.162968).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7867460250854492
Epoch: 4, Steps: 3 | Train Loss: 0.2748524 Vali Loss: 0.1628010 Test Loss: 0.4639664
Validation loss decreased (0.162968 --> 0.162801).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7851748466491699
Epoch: 5, Steps: 3 | Train Loss: 0.2674236 Vali Loss: 0.1627230 Test Loss: 0.4637890
Validation loss decreased (0.162801 --> 0.162723).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-11_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 286
train 286
Threshold : 1.6731629765032798
pred:    (28600,)
gt:      (28600,)
pred:  (28600,)
gt:    (28600,)
Accuracy : 0.9965, Precision : 0.6622, Recall : 1.0000, F-score : 0.7968, AUC : 0.9982
Mean Value:
Accuracy : 0.9970, Precision : 0.9006, Recall : 0.9768, F-score : 0.9332, AUC : 0.9872
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.5, batch_size=128, c_out=38, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMD', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=38, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMD', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=25, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMD', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 284
val 56
test 284
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.6710350513458252
Epoch: 1, Steps: 3 | Train Loss: 0.1638082 Vali Loss: 0.1746707 Test Loss: 0.2176675
Validation loss decreased (inf --> 0.174671).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6554896831512451
Epoch: 2, Steps: 3 | Train Loss: 0.1578646 Vali Loss: 0.1737431 Test Loss: 0.2160937
Validation loss decreased (0.174671 --> 0.173743).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6612668037414551
Epoch: 3, Steps: 3 | Train Loss: 0.1724574 Vali Loss: 0.1733988 Test Loss: 0.2155024
Validation loss decreased (0.173743 --> 0.173399).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6525073051452637
Epoch: 4, Steps: 3 | Train Loss: 0.1730216 Vali Loss: 0.1732535 Test Loss: 0.2152412
Validation loss decreased (0.173399 --> 0.173253).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6620128154754639
Epoch: 5, Steps: 3 | Train Loss: 0.1579716 Vali Loss: 0.1731862 Test Loss: 0.2151202
Validation loss decreased (0.173253 --> 0.173186).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 284
train 284
Threshold : 2.166967582702636
pred:    (28400,)
gt:      (28400,)
pred:  (28400,)
gt:    (28400,)
Accuracy : 0.9981, Precision : 0.9807, Recall : 1.0000, F-score : 0.9903, AUC : 0.9990
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.625361442565918
Epoch: 1, Steps: 2 | Train Loss: 0.6256352 Vali Loss: 0.6517208 Test Loss: 0.5980074
Validation loss decreased (inf --> 0.651721).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6287095546722412
Epoch: 2, Steps: 2 | Train Loss: 0.6182863 Vali Loss: 0.6470871 Test Loss: 0.5935143
Validation loss decreased (0.651721 --> 0.647087).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6358225345611572
Epoch: 3, Steps: 2 | Train Loss: 0.6073124 Vali Loss: 0.6449927 Test Loss: 0.5914682
Validation loss decreased (0.647087 --> 0.644993).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.678044319152832
Epoch: 4, Steps: 2 | Train Loss: 0.6054352 Vali Loss: 0.6440065 Test Loss: 0.5905077
Validation loss decreased (0.644993 --> 0.644006).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6287708282470703
Epoch: 5, Steps: 2 | Train Loss: 0.6120405 Vali Loss: 0.6435309 Test Loss: 0.5900465
Validation loss decreased (0.644006 --> 0.643531).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 12.415082035064659
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9935, Precision : 0.8530, Recall : 0.8672, F-score : 0.8600, AUC : 0.9318
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6605017185211182
Epoch: 1, Steps: 2 | Train Loss: 0.3200167 Vali Loss: 1.3369700 Test Loss: 0.2391933
Validation loss decreased (inf --> 1.336970).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7147712707519531
Epoch: 2, Steps: 2 | Train Loss: 0.3183675 Vali Loss: 1.3303242 Test Loss: 0.2381236
Validation loss decreased (1.336970 --> 1.330324).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6486399173736572
Epoch: 3, Steps: 2 | Train Loss: 0.2999387 Vali Loss: 1.3270488 Test Loss: 0.2376133
Validation loss decreased (1.330324 --> 1.327049).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6502270698547363
Epoch: 4, Steps: 2 | Train Loss: 0.2845729 Vali Loss: 1.3255129 Test Loss: 0.2373714
Validation loss decreased (1.327049 --> 1.325513).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6548411846160889
Epoch: 5, Steps: 2 | Train Loss: 0.2838471 Vali Loss: 1.3247827 Test Loss: 0.2372558
Validation loss decreased (1.325513 --> 1.324783).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 5.081838309764808
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9926, Precision : 0.9735, Recall : 0.8078, F-score : 0.8829, AUC : 0.9035
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6787593364715576
Epoch: 1, Steps: 2 | Train Loss: 0.3177036 Vali Loss: 1.3680779 Test Loss: 0.2428855
Validation loss decreased (inf --> 1.368078).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6719648838043213
Epoch: 2, Steps: 2 | Train Loss: 0.2981748 Vali Loss: 1.3617637 Test Loss: 0.2414934
Validation loss decreased (1.368078 --> 1.361764).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6933658123016357
Epoch: 3, Steps: 2 | Train Loss: 0.3097072 Vali Loss: 1.3590678 Test Loss: 0.2408824
Validation loss decreased (1.361764 --> 1.359068).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6812593936920166
Epoch: 4, Steps: 2 | Train Loss: 0.3280298 Vali Loss: 1.3579113 Test Loss: 0.2406135
Validation loss decreased (1.359068 --> 1.357911).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6783411502838135
Epoch: 5, Steps: 2 | Train Loss: 0.3292589 Vali Loss: 1.3573323 Test Loss: 0.2404786
Validation loss decreased (1.357911 --> 1.357332).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 4.193317689895573
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9965, Precision : 0.9690, Recall : 0.9125, F-score : 0.9399, AUC : 0.9558
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6420161724090576
Epoch: 1, Steps: 2 | Train Loss: 0.4325774 Vali Loss: 0.6064323 Test Loss: 0.5204799
Validation loss decreased (inf --> 0.606432).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6497986316680908
Epoch: 2, Steps: 2 | Train Loss: 0.4235985 Vali Loss: 0.6032202 Test Loss: 0.5172782
Validation loss decreased (0.606432 --> 0.603220).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6489148139953613
Epoch: 3, Steps: 2 | Train Loss: 0.4267555 Vali Loss: 0.6018264 Test Loss: 0.5158994
Validation loss decreased (0.603220 --> 0.601826).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6493713855743408
Epoch: 4, Steps: 2 | Train Loss: 0.4218042 Vali Loss: 0.6011690 Test Loss: 0.5152403
Validation loss decreased (0.601826 --> 0.601169).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6435196399688721
Epoch: 5, Steps: 2 | Train Loss: 0.4190792 Vali Loss: 0.6008518 Test Loss: 0.5149239
Validation loss decreased (0.601169 --> 0.600852).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 7.6091001462935575
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9972, Precision : 0.6024, Recall : 1.0000, F-score : 0.7519, AUC : 0.9986
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7095589637756348
Epoch: 1, Steps: 2 | Train Loss: 0.3335696 Vali Loss: 0.4308421 Test Loss: 0.2534444
Validation loss decreased (inf --> 0.430842).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.670874834060669
Epoch: 2, Steps: 2 | Train Loss: 0.3381926 Vali Loss: 0.4293843 Test Loss: 0.2520926
Validation loss decreased (0.430842 --> 0.429384).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6779794692993164
Epoch: 3, Steps: 2 | Train Loss: 0.3399222 Vali Loss: 0.4287743 Test Loss: 0.2515029
Validation loss decreased (0.429384 --> 0.428774).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6786668300628662
Epoch: 4, Steps: 2 | Train Loss: 0.3336561 Vali Loss: 0.4285001 Test Loss: 0.2512175
Validation loss decreased (0.428774 --> 0.428500).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6609923839569092
Epoch: 5, Steps: 2 | Train Loss: 0.3309333 Vali Loss: 0.4283651 Test Loss: 0.2510827
Validation loss decreased (0.428500 --> 0.428365).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.196027469634972
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9976, Precision : 0.9901, Recall : 0.9946, F-score : 0.9923, AUC : 0.9964
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6935889720916748
Epoch: 1, Steps: 2 | Train Loss: 0.3655429 Vali Loss: 0.3121475 Test Loss: 0.3030936
Validation loss decreased (inf --> 0.312148).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6658871173858643
Epoch: 2, Steps: 2 | Train Loss: 0.3481288 Vali Loss: 0.3104000 Test Loss: 0.3012586
Validation loss decreased (0.312148 --> 0.310400).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6626849174499512
Epoch: 3, Steps: 2 | Train Loss: 0.3457427 Vali Loss: 0.3096180 Test Loss: 0.3004378
Validation loss decreased (0.310400 --> 0.309618).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6580102443695068
Epoch: 4, Steps: 2 | Train Loss: 0.3593957 Vali Loss: 0.3092494 Test Loss: 0.3000563
Validation loss decreased (0.309618 --> 0.309249).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6746847629547119
Epoch: 5, Steps: 2 | Train Loss: 0.3475062 Vali Loss: 0.3090673 Test Loss: 0.2998667
Validation loss decreased (0.309249 --> 0.309067).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 10.60250163078301
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9984, Precision : 0.9876, Recall : 0.9971, F-score : 0.9923, AUC : 0.9978
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD1-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6482133865356445
Epoch: 1, Steps: 2 | Train Loss: 0.2547239 Vali Loss: 0.1946810 Test Loss: 0.1881690
Validation loss decreased (inf --> 0.194681).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.65557861328125
Epoch: 2, Steps: 2 | Train Loss: 0.2538024 Vali Loss: 0.1940288 Test Loss: 0.1876325
Validation loss decreased (0.194681 --> 0.194029).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6443986892700195
Epoch: 3, Steps: 2 | Train Loss: 0.2565906 Vali Loss: 0.1937540 Test Loss: 0.1874192
Validation loss decreased (0.194029 --> 0.193754).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6519050598144531
Epoch: 4, Steps: 2 | Train Loss: 0.2518650 Vali Loss: 0.1936299 Test Loss: 0.1873274
Validation loss decreased (0.193754 --> 0.193630).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6539907455444336
Epoch: 5, Steps: 2 | Train Loss: 0.2531546 Vali Loss: 0.1935707 Test Loss: 0.1872835
Validation loss decreased (0.193630 --> 0.193571).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD1-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 3.2840683627127714
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9993, Precision : 0.9795, Recall : 1.0000, F-score : 0.9896, AUC : 0.9996
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6752216815948486
Epoch: 1, Steps: 2 | Train Loss: 0.5034025 Vali Loss: 0.4261344 Test Loss: 0.5128191
Validation loss decreased (inf --> 0.426134).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6514413356781006
Epoch: 2, Steps: 2 | Train Loss: 0.4953034 Vali Loss: 0.4226611 Test Loss: 0.5083838
Validation loss decreased (0.426134 --> 0.422661).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6442813873291016
Epoch: 3, Steps: 2 | Train Loss: 0.4978333 Vali Loss: 0.4210681 Test Loss: 0.5063779
Validation loss decreased (0.422661 --> 0.421068).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.656691312789917
Epoch: 4, Steps: 2 | Train Loss: 0.4810426 Vali Loss: 0.4203033 Test Loss: 0.5054035
Validation loss decreased (0.421068 --> 0.420303).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6614625453948975
Epoch: 5, Steps: 2 | Train Loss: 0.4863081 Vali Loss: 0.4199308 Test Loss: 0.5049266
Validation loss decreased (0.420303 --> 0.419931).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.958229610919881
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9955, Precision : 0.9396, Recall : 0.9709, F-score : 0.9550, AUC : 0.9838
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6846926212310791
Epoch: 1, Steps: 2 | Train Loss: 0.5214776 Vali Loss: 0.6717175 Test Loss: 0.6079024
Validation loss decreased (inf --> 0.671717).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6841201782226562
Epoch: 2, Steps: 2 | Train Loss: 0.5131952 Vali Loss: 0.6664610 Test Loss: 0.6030653
Validation loss decreased (0.671717 --> 0.666461).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6711509227752686
Epoch: 3, Steps: 2 | Train Loss: 0.5116658 Vali Loss: 0.6641287 Test Loss: 0.6009018
Validation loss decreased (0.666461 --> 0.664129).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7106528282165527
Epoch: 4, Steps: 2 | Train Loss: 0.5083289 Vali Loss: 0.6630269 Test Loss: 0.5998830
Validation loss decreased (0.664129 --> 0.663027).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6994500160217285
Epoch: 5, Steps: 2 | Train Loss: 0.5083817 Vali Loss: 0.6625005 Test Loss: 0.5993863
Validation loss decreased (0.663027 --> 0.662501).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 236
Threshold : 7.99492696523665
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9957, Precision : 0.9649, Recall : 1.0000, F-score : 0.9821, AUC : 0.9975
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6940858364105225
Epoch: 1, Steps: 2 | Train Loss: 0.4384703 Vali Loss: 0.1200702 Test Loss: 0.4816619
Validation loss decreased (inf --> 0.120070).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6823303699493408
Epoch: 2, Steps: 2 | Train Loss: 0.4247068 Vali Loss: 0.1194148 Test Loss: 0.4790526
Validation loss decreased (0.120070 --> 0.119415).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6867637634277344
Epoch: 3, Steps: 2 | Train Loss: 0.4318211 Vali Loss: 0.1191333 Test Loss: 0.4779039
Validation loss decreased (0.119415 --> 0.119133).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6913824081420898
Epoch: 4, Steps: 2 | Train Loss: 0.3923421 Vali Loss: 0.1190022 Test Loss: 0.4773370
Validation loss decreased (0.119133 --> 0.119002).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6799726486206055
Epoch: 5, Steps: 2 | Train Loss: 0.3930128 Vali Loss: 0.1189395 Test Loss: 0.4770750
Validation loss decreased (0.119002 --> 0.118940).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 6.513416519164963
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9985, Precision : 0.8820, Recall : 1.0000, F-score : 0.9373, AUC : 0.9992
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6792945861816406
Epoch: 1, Steps: 2 | Train Loss: 0.2806481 Vali Loss: 0.2160659 Test Loss: 0.2837397
Validation loss decreased (inf --> 0.216066).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6900277137756348
Epoch: 2, Steps: 2 | Train Loss: 0.2740166 Vali Loss: 0.2137465 Test Loss: 0.2813054
Validation loss decreased (0.216066 --> 0.213747).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6896655559539795
Epoch: 3, Steps: 2 | Train Loss: 0.2754430 Vali Loss: 0.2126981 Test Loss: 0.2802127
Validation loss decreased (0.213747 --> 0.212698).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6915836334228516
Epoch: 4, Steps: 2 | Train Loss: 0.2835426 Vali Loss: 0.2121865 Test Loss: 0.2796860
Validation loss decreased (0.212698 --> 0.212187).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6813130378723145
Epoch: 5, Steps: 2 | Train Loss: 0.2872055 Vali Loss: 0.2119384 Test Loss: 0.2794315
Validation loss decreased (0.212187 --> 0.211938).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 3.4847111999987592
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9993, Precision : 0.9906, Recall : 1.0000, F-score : 0.9953, AUC : 0.9996
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7034850120544434
Epoch: 1, Steps: 2 | Train Loss: 0.3113317 Vali Loss: 0.3128855 Test Loss: 0.2656308
Validation loss decreased (inf --> 0.312885).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.696718692779541
Epoch: 2, Steps: 2 | Train Loss: 0.3127945 Vali Loss: 0.3106247 Test Loss: 0.2637458
Validation loss decreased (0.312885 --> 0.310625).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6920313835144043
Epoch: 3, Steps: 2 | Train Loss: 0.3224887 Vali Loss: 0.3095669 Test Loss: 0.2628005
Validation loss decreased (0.310625 --> 0.309567).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6818287372589111
Epoch: 4, Steps: 2 | Train Loss: 0.3173471 Vali Loss: 0.3090671 Test Loss: 0.2623521
Validation loss decreased (0.309567 --> 0.309067).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6926400661468506
Epoch: 5, Steps: 2 | Train Loss: 0.3053047 Vali Loss: 0.3088225 Test Loss: 0.2621253
Validation loss decreased (0.309067 --> 0.308822).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 4.567829926013925
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9977, Precision : 0.9667, Recall : 0.9786, F-score : 0.9726, AUC : 0.9886
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.8341336250305176
Epoch: 1, Steps: 3 | Train Loss: 0.2877002 Vali Loss: 0.2275632 Test Loss: 0.4817444
Validation loss decreased (inf --> 0.227563).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7672841548919678
Epoch: 2, Steps: 3 | Train Loss: 0.2519880 Vali Loss: 0.2251864 Test Loss: 0.4763235
Validation loss decreased (0.227563 --> 0.225186).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.767333984375
Epoch: 3, Steps: 3 | Train Loss: 0.2997233 Vali Loss: 0.2241560 Test Loss: 0.4742558
Validation loss decreased (0.225186 --> 0.224156).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7714323997497559
Epoch: 4, Steps: 3 | Train Loss: 0.2640299 Vali Loss: 0.2236779 Test Loss: 0.4737638
Validation loss decreased (0.224156 --> 0.223678).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7630801200866699
Epoch: 5, Steps: 3 | Train Loss: 0.2449635 Vali Loss: 0.2234464 Test Loss: 0.4733018
Validation loss decreased (0.223678 --> 0.223446).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.258960111141189
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9970, Precision : 0.8532, Recall : 0.9599, F-score : 0.9034, AUC : 0.9787
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6825804710388184
Epoch: 1, Steps: 2 | Train Loss: 0.2725400 Vali Loss: 0.3143391 Test Loss: 0.2568906
Validation loss decreased (inf --> 0.314339).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6883304119110107
Epoch: 2, Steps: 2 | Train Loss: 0.2681568 Vali Loss: 0.3121773 Test Loss: 0.2552970
Validation loss decreased (0.314339 --> 0.312177).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6876003742218018
Epoch: 3, Steps: 2 | Train Loss: 0.2737065 Vali Loss: 0.3112016 Test Loss: 0.2545798
Validation loss decreased (0.312177 --> 0.311202).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6808440685272217
Epoch: 4, Steps: 2 | Train Loss: 0.2615732 Vali Loss: 0.3107309 Test Loss: 0.2542379
Validation loss decreased (0.311202 --> 0.310731).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6809396743774414
Epoch: 5, Steps: 2 | Train Loss: 0.2607910 Vali Loss: 0.3105073 Test Loss: 0.2540717
Validation loss decreased (0.310731 --> 0.310507).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.837603087425199
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9984, Precision : 0.9260, Recall : 0.9904, F-score : 0.9571, AUC : 0.9945
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7096002101898193
Epoch: 1, Steps: 2 | Train Loss: 0.1938446 Vali Loss: 0.2811801 Test Loss: 0.2279421
Validation loss decreased (inf --> 0.281180).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6892166137695312
Epoch: 2, Steps: 2 | Train Loss: 0.1909797 Vali Loss: 0.2784757 Test Loss: 0.2256427
Validation loss decreased (0.281180 --> 0.278476).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6895534992218018
Epoch: 3, Steps: 2 | Train Loss: 0.1911208 Vali Loss: 0.2772210 Test Loss: 0.2245784
Validation loss decreased (0.278476 --> 0.277221).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6835625171661377
Epoch: 4, Steps: 2 | Train Loss: 0.1881787 Vali Loss: 0.2766198 Test Loss: 0.2240686
Validation loss decreased (0.277221 --> 0.276620).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7174432277679443
Epoch: 5, Steps: 2 | Train Loss: 0.1891568 Vali Loss: 0.2763264 Test Loss: 0.2238215
Validation loss decreased (0.276620 --> 0.276326).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 3.074036169052117
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9995, Precision : 0.9360, Recall : 1.0000, F-score : 0.9670, AUC : 0.9998
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD2-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7581343650817871
Epoch: 1, Steps: 3 | Train Loss: 0.2656241 Vali Loss: 0.1908240 Test Loss: 0.2811587
Validation loss decreased (inf --> 0.190824).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7836349010467529
Epoch: 2, Steps: 3 | Train Loss: 0.2710476 Vali Loss: 0.1890893 Test Loss: 0.2792629
Validation loss decreased (0.190824 --> 0.189089).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7802982330322266
Epoch: 3, Steps: 3 | Train Loss: 0.2678120 Vali Loss: 0.1883471 Test Loss: 0.2784630
Validation loss decreased (0.189089 --> 0.188347).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8056697845458984
Epoch: 4, Steps: 3 | Train Loss: 0.3334781 Vali Loss: 0.1880055 Test Loss: 0.2780963
Validation loss decreased (0.188347 --> 0.188005).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7854499816894531
Epoch: 5, Steps: 3 | Train Loss: 0.2700013 Vali Loss: 0.1878490 Test Loss: 0.2779285
Validation loss decreased (0.188005 --> 0.187849).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD2-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.945789968967409
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9985, Precision : 0.9761, Recall : 1.0000, F-score : 0.9879, AUC : 0.9992
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7595109939575195
Epoch: 1, Steps: 3 | Train Loss: 0.3390384 Vali Loss: 0.2674258 Test Loss: 0.3547676
Validation loss decreased (inf --> 0.267426).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7559561729431152
Epoch: 2, Steps: 3 | Train Loss: 0.3343854 Vali Loss: 0.2643544 Test Loss: 0.3511101
Validation loss decreased (0.267426 --> 0.264354).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7588260173797607
Epoch: 3, Steps: 3 | Train Loss: 0.2961507 Vali Loss: 0.2630509 Test Loss: 0.3495475
Validation loss decreased (0.264354 --> 0.263051).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7610597610473633
Epoch: 4, Steps: 3 | Train Loss: 0.3042331 Vali Loss: 0.2624499 Test Loss: 0.3488318
Validation loss decreased (0.263051 --> 0.262450).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7524914741516113
Epoch: 5, Steps: 3 | Train Loss: 0.2961137 Vali Loss: 0.2621644 Test Loss: 0.3484950
Validation loss decreased (0.262450 --> 0.262164).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-1_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.162609834671018
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9941, Precision : 0.6583, Recall : 0.9318, F-score : 0.7715, AUC : 0.9633
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6838793754577637
Epoch: 1, Steps: 2 | Train Loss: 0.5121810 Vali Loss: 0.6114672 Test Loss: 0.5069870
Validation loss decreased (inf --> 0.611467).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6778926849365234
Epoch: 2, Steps: 2 | Train Loss: 0.5079844 Vali Loss: 0.6076858 Test Loss: 0.5036836
Validation loss decreased (0.611467 --> 0.607686).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6816587448120117
Epoch: 3, Steps: 2 | Train Loss: 0.5055825 Vali Loss: 0.6060475 Test Loss: 0.5022023
Validation loss decreased (0.607686 --> 0.606048).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6886813640594482
Epoch: 4, Steps: 2 | Train Loss: 0.5018619 Vali Loss: 0.6052191 Test Loss: 0.5014840
Validation loss decreased (0.606048 --> 0.605219).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6807069778442383
Epoch: 5, Steps: 2 | Train Loss: 0.5060070 Vali Loss: 0.6048362 Test Loss: 0.5011515
Validation loss decreased (0.605219 --> 0.604836).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-2_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 9.519178042411802
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9962, Precision : 0.9285, Recall : 0.9955, F-score : 0.9608, AUC : 0.9959
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 237
val 47
test 237
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6836833953857422
Epoch: 1, Steps: 2 | Train Loss: 0.3652752 Vali Loss: 0.1686016 Test Loss: 0.2920481
Validation loss decreased (inf --> 0.168602).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.695857048034668
Epoch: 2, Steps: 2 | Train Loss: 0.3666948 Vali Loss: 0.1671477 Test Loss: 0.2896525
Validation loss decreased (0.168602 --> 0.167148).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6901443004608154
Epoch: 3, Steps: 2 | Train Loss: 0.3498222 Vali Loss: 0.1664813 Test Loss: 0.2885024
Validation loss decreased (0.167148 --> 0.166481).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.705474853515625
Epoch: 4, Steps: 2 | Train Loss: 0.3638951 Vali Loss: 0.1661684 Test Loss: 0.2879692
Validation loss decreased (0.166481 --> 0.166168).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7109231948852539
Epoch: 5, Steps: 2 | Train Loss: 0.3573396 Vali Loss: 0.1660128 Test Loss: 0.2877015
Validation loss decreased (0.166168 --> 0.166013).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-3_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
train 237
Threshold : 3.351856516599655
pred:    (23700,)
gt:      (23700,)
pred:  (23700,)
gt:    (23700,)
Accuracy : 0.9972, Precision : 0.9248, Recall : 0.9731, F-score : 0.9483, AUC : 0.9855
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6990325450897217
Epoch: 1, Steps: 2 | Train Loss: 0.3598077 Vali Loss: 0.3706329 Test Loss: 0.3318907
Validation loss decreased (inf --> 0.370633).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7233049869537354
Epoch: 2, Steps: 2 | Train Loss: 0.3587881 Vali Loss: 0.3682995 Test Loss: 0.3300350
Validation loss decreased (0.370633 --> 0.368299).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.705390453338623
Epoch: 3, Steps: 2 | Train Loss: 0.3539107 Vali Loss: 0.3672615 Test Loss: 0.3292099
Validation loss decreased (0.368299 --> 0.367261).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6829829216003418
Epoch: 4, Steps: 2 | Train Loss: 0.3539193 Vali Loss: 0.3665320 Test Loss: 0.3288169
Validation loss decreased (0.367261 --> 0.366532).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6957213878631592
Epoch: 5, Steps: 2 | Train Loss: 0.3548676 Vali Loss: 0.3663022 Test Loss: 0.3286296
Validation loss decreased (0.366532 --> 0.366302).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-4_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 2.965577692985522
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9967, Precision : 0.9252, Recall : 1.0000, F-score : 0.9611, AUC : 0.9983
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.6819307804107666
Epoch: 1, Steps: 2 | Train Loss: 0.3291646 Vali Loss: 0.4150103 Test Loss: 0.3343861
Validation loss decreased (inf --> 0.415010).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.700019359588623
Epoch: 2, Steps: 2 | Train Loss: 0.3263810 Vali Loss: 0.4122750 Test Loss: 0.3323314
Validation loss decreased (0.415010 --> 0.412275).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6859297752380371
Epoch: 3, Steps: 2 | Train Loss: 0.3242446 Vali Loss: 0.4109835 Test Loss: 0.3313918
Validation loss decreased (0.412275 --> 0.410983).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6787755489349365
Epoch: 4, Steps: 2 | Train Loss: 0.3247746 Vali Loss: 0.4103621 Test Loss: 0.3309454
Validation loss decreased (0.410983 --> 0.410362).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6850850582122803
Epoch: 5, Steps: 2 | Train Loss: 0.3190074 Vali Loss: 0.4100640 Test Loss: 0.3307316
Validation loss decreased (0.410362 --> 0.410064).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-5_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 5.668813898563266
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9959, Precision : 0.8313, Recall : 0.9718, F-score : 0.8961, AUC : 0.9841
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7679808139801025
Epoch: 1, Steps: 3 | Train Loss: 0.2284623 Vali Loss: 0.2181597 Test Loss: 0.4232393
Validation loss decreased (inf --> 0.218160).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7878251075744629
Epoch: 2, Steps: 3 | Train Loss: 0.2169054 Vali Loss: 0.2149650 Test Loss: 0.4171226
Validation loss decreased (0.218160 --> 0.214965).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7512896060943604
Epoch: 3, Steps: 3 | Train Loss: 0.2043783 Vali Loss: 0.2135473 Test Loss: 0.4141938
Validation loss decreased (0.214965 --> 0.213547).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7550601959228516
Epoch: 4, Steps: 3 | Train Loss: 0.1797911 Vali Loss: 0.2128865 Test Loss: 0.4129384
Validation loss decreased (0.213547 --> 0.212886).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7857606410980225
Epoch: 5, Steps: 3 | Train Loss: 0.1824310 Vali Loss: 0.2125806 Test Loss: 0.4122415
Validation loss decreased (0.212886 --> 0.212581).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-6_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 5.470866687297807
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9972, Precision : 0.9379, Recall : 1.0000, F-score : 0.9680, AUC : 0.9986
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7849888801574707
Epoch: 1, Steps: 3 | Train Loss: 0.4306910 Vali Loss: 0.2828813 Test Loss: 0.2329725
Validation loss decreased (inf --> 0.282881).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7694504261016846
Epoch: 2, Steps: 3 | Train Loss: 0.5316575 Vali Loss: 0.2802296 Test Loss: 0.2308545
Validation loss decreased (0.282881 --> 0.280230).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7751114368438721
Epoch: 3, Steps: 3 | Train Loss: 0.4073724 Vali Loss: 0.2790919 Test Loss: 0.2299841
Validation loss decreased (0.280230 --> 0.279092).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7871782779693604
Epoch: 4, Steps: 3 | Train Loss: 0.4025779 Vali Loss: 0.2785693 Test Loss: 0.2295840
Validation loss decreased (0.279092 --> 0.278569).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7647175788879395
Epoch: 5, Steps: 3 | Train Loss: 0.4246903 Vali Loss: 0.2783211 Test Loss: 0.2293962
Validation loss decreased (0.278569 --> 0.278321).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-7_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.8799489808082175
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9995, Precision : 0.9709, Recall : 1.0000, F-score : 0.9852, AUC : 0.9998
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7616374492645264
Epoch: 1, Steps: 3 | Train Loss: 0.3894271 Vali Loss: 0.2252647 Test Loss: 0.3849664
Validation loss decreased (inf --> 0.225265).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7678735256195068
Epoch: 2, Steps: 3 | Train Loss: 0.3056853 Vali Loss: 0.2240633 Test Loss: 0.3828780
Validation loss decreased (0.225265 --> 0.224063).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7609593868255615
Epoch: 3, Steps: 3 | Train Loss: 0.3204977 Vali Loss: 0.2235698 Test Loss: 0.3820136
Validation loss decreased (0.224063 --> 0.223570).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7571284770965576
Epoch: 4, Steps: 3 | Train Loss: 0.3798904 Vali Loss: 0.2233540 Test Loss: 0.3816225
Validation loss decreased (0.223570 --> 0.223354).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7627606391906738
Epoch: 5, Steps: 3 | Train Loss: 0.3221847 Vali Loss: 0.2232524 Test Loss: 0.3814367
Validation loss decreased (0.223354 --> 0.223252).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-8_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 4.4296348047255965
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9952, Precision : 0.9092, Recall : 1.0000, F-score : 0.9524, AUC : 0.9975
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 287
val 57
test 287
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7784948348999023
Epoch: 1, Steps: 3 | Train Loss: 0.3039816 Vali Loss: 0.2808023 Test Loss: 0.3557782
Validation loss decreased (inf --> 0.280802).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7625417709350586
Epoch: 2, Steps: 3 | Train Loss: 0.3037816 Vali Loss: 0.2778535 Test Loss: 0.3520805
Validation loss decreased (0.280802 --> 0.277854).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7644450664520264
Epoch: 3, Steps: 3 | Train Loss: 0.2966928 Vali Loss: 0.2766735 Test Loss: 0.3505578
Validation loss decreased (0.277854 --> 0.276673).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7919988632202148
Epoch: 4, Steps: 3 | Train Loss: 0.3078588 Vali Loss: 0.2760777 Test Loss: 0.3498583
Validation loss decreased (0.276673 --> 0.276078).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7658517360687256
Epoch: 5, Steps: 3 | Train Loss: 0.2951987 Vali Loss: 0.2757454 Test Loss: 0.3495239
Validation loss decreased (0.276078 --> 0.275745).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-9_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 287
train 287
Threshold : 3.4656278717517743
pred:    (28700,)
gt:      (28700,)
pred:  (28700,)
gt:    (28700,)
Accuracy : 0.9967, Precision : 0.7613, Recall : 1.0000, F-score : 0.8645, AUC : 0.9983
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-10_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 236
val 47
test 236
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Save Encoder Model...
Epoch: 1 cost time: 0.7273032665252686
Epoch: 1, Steps: 2 | Train Loss: 0.1986688 Vali Loss: 0.0532457 Test Loss: 0.1652531
Validation loss decreased (inf --> 0.053246).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.6652235984802246
Epoch: 2, Steps: 2 | Train Loss: 0.1935891 Vali Loss: 0.0530223 Test Loss: 0.1645466
Validation loss decreased (0.053246 --> 0.053022).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.6919488906860352
Epoch: 3, Steps: 2 | Train Loss: 0.1975847 Vali Loss: 0.0529464 Test Loss: 0.1642321
Validation loss decreased (0.053022 --> 0.052946).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.6849098205566406
Epoch: 4, Steps: 2 | Train Loss: 0.1959157 Vali Loss: 0.0529117 Test Loss: 0.1640860
Validation loss decreased (0.052946 --> 0.052912).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.6683118343353271
Epoch: 5, Steps: 2 | Train Loss: 0.1922569 Vali Loss: 0.0528949 Test Loss: 0.1640100
Validation loss decreased (0.052912 --> 0.052895).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-10_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
train 236
Threshold : 2.3371370363234925
pred:    (23600,)
gt:      (23600,)
pred:  (23600,)
gt:    (23600,)
Accuracy : 0.9970, Precision : 0.9365, Recall : 1.0000, F-score : 0.9672, AUC : 0.9984
Use GPU: cuda:0
>>>>>>>start training : SMD_GPT4TS_SMD3-11_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
train 286
val 57
test 286
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Save Encoder Model...
Epoch: 1 cost time: 0.7947704792022705
Epoch: 1, Steps: 3 | Train Loss: 0.2794450 Vali Loss: 0.1642877 Test Loss: 0.4675821
Validation loss decreased (inf --> 0.164288).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.7625906467437744
Epoch: 2, Steps: 3 | Train Loss: 0.2804330 Vali Loss: 0.1633439 Test Loss: 0.4651577
Validation loss decreased (0.164288 --> 0.163344).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.7494359016418457
Epoch: 3, Steps: 3 | Train Loss: 0.2683703 Vali Loss: 0.1629689 Test Loss: 0.4643385
Validation loss decreased (0.163344 --> 0.162969).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.7618362903594971
Epoch: 4, Steps: 3 | Train Loss: 0.2748549 Vali Loss: 0.1628019 Test Loss: 0.4639688
Validation loss decreased (0.162969 --> 0.162802).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.7545602321624756
Epoch: 5, Steps: 3 | Train Loss: 0.2674269 Vali Loss: 0.1627237 Test Loss: 0.4637935
Validation loss decreased (0.162802 --> 0.162724).  Saving model ...
Updating learning rate to 6.25e-06
>>>>>>>testing : SMD_GPT4TS_SMD3-11_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 286
train 286
Threshold : 1.673110460042861
pred:    (28600,)
gt:      (28600,)
pred:  (28600,)
gt:    (28600,)
Accuracy : 0.9965, Precision : 0.6622, Recall : 1.0000, F-score : 0.7968, AUC : 0.9982
Mean Value:
Accuracy : 0.9970, Precision : 0.9006, Recall : 0.9768, F-score : 0.9332, AUC : 0.9872
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=25, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='SMAP', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=25, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0005, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SMAP', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SMAP', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=5, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SMAP_GPT4TS_SMAP_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (427617, 25)
train: (135183, 25)
train 135084
test: (427617, 25)
train: (135183, 25)
val 26938
test: (427617, 25)
train: (135183, 25)
test 427518
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Batch: 455
Batch: 456
Batch: 457
Batch: 458
Batch: 459
Batch: 460
Batch: 461
Batch: 462
Batch: 463
Batch: 464
Batch: 465
Batch: 466
Batch: 467
Batch: 468
Batch: 469
Batch: 470
Batch: 471
Batch: 472
Batch: 473
Batch: 474
Batch: 475
Batch: 476
Batch: 477
Batch: 478
Batch: 479
Batch: 480
Batch: 481
Batch: 482
Batch: 483
Batch: 484
Batch: 485
Batch: 486
Batch: 487
Batch: 488
Batch: 489
Batch: 490
Batch: 491
Batch: 492
Batch: 493
Batch: 494
Batch: 495
Batch: 496
Batch: 497
Batch: 498
Batch: 499
Batch: 500
Batch: 501
Batch: 502
Batch: 503
Batch: 504
Batch: 505
Batch: 506
Batch: 507
Batch: 508
Batch: 509
Batch: 510
Batch: 511
Batch: 512
Batch: 513
Batch: 514
Batch: 515
Batch: 516
Batch: 517
Batch: 518
Batch: 519
Batch: 520
Batch: 521
Batch: 522
Batch: 523
Batch: 524
Batch: 525
Batch: 526
Batch: 527
Batch: 528
Batch: 529
Batch: 530
Batch: 531
Batch: 532
Batch: 533
Batch: 534
Batch: 535
Batch: 536
Batch: 537
Batch: 538
Batch: 539
Batch: 540
Batch: 541
Batch: 542
Batch: 543
Batch: 544
Batch: 545
Batch: 546
Batch: 547
Batch: 548
Batch: 549
Batch: 550
Batch: 551
Batch: 552
Batch: 553
Batch: 554
Batch: 555
Batch: 556
Batch: 557
Batch: 558
Batch: 559
Batch: 560
Batch: 561
Batch: 562
Batch: 563
Batch: 564
Batch: 565
Batch: 566
Batch: 567
Batch: 568
Batch: 569
Batch: 570
Batch: 571
Batch: 572
Batch: 573
Batch: 574
Batch: 575
Batch: 576
Batch: 577
Batch: 578
Batch: 579
Batch: 580
Batch: 581
Batch: 582
Batch: 583
Batch: 584
Batch: 585
Batch: 586
Batch: 587
Batch: 588
Batch: 589
Batch: 590
Batch: 591
Batch: 592
Batch: 593
Batch: 594
Batch: 595
Batch: 596
Batch: 597
Batch: 598
Batch: 599
Batch: 600
Batch: 601
Batch: 602
Batch: 603
Batch: 604
Batch: 605
Batch: 606
Batch: 607
Batch: 608
Batch: 609
Batch: 610
Batch: 611
Batch: 612
Batch: 613
Batch: 614
Batch: 615
Batch: 616
Batch: 617
Batch: 618
Batch: 619
Batch: 620
Batch: 621
Batch: 622
Batch: 623
Batch: 624
Batch: 625
Batch: 626
Batch: 627
Batch: 628
Batch: 629
Batch: 630
Batch: 631
Batch: 632
Batch: 633
Batch: 634
Batch: 635
Batch: 636
Batch: 637
Batch: 638
Batch: 639
Batch: 640
Batch: 641
Batch: 642
Batch: 643
Batch: 644
Batch: 645
Batch: 646
Batch: 647
Batch: 648
Batch: 649
Batch: 650
Batch: 651
Batch: 652
Batch: 653
Batch: 654
Batch: 655
Batch: 656
Batch: 657
Batch: 658
Batch: 659
Batch: 660
Batch: 661
Batch: 662
Batch: 663
Batch: 664
Batch: 665
Batch: 666
Batch: 667
Batch: 668
Batch: 669
Batch: 670
Batch: 671
Batch: 672
Batch: 673
Batch: 674
Batch: 675
Batch: 676
Batch: 677
Batch: 678
Batch: 679
Batch: 680
Batch: 681
Batch: 682
Batch: 683
Batch: 684
Batch: 685
Batch: 686
Batch: 687
Batch: 688
Batch: 689
Batch: 690
Batch: 691
Batch: 692
Batch: 693
Batch: 694
Batch: 695
Batch: 696
Batch: 697
Batch: 698
Batch: 699
Batch: 700
Batch: 701
Batch: 702
Batch: 703
Batch: 704
Batch: 705
Batch: 706
Batch: 707
Batch: 708
Batch: 709
Batch: 710
Batch: 711
Batch: 712
Batch: 713
Batch: 714
Batch: 715
Batch: 716
Batch: 717
Batch: 718
Batch: 719
Batch: 720
Batch: 721
Batch: 722
Batch: 723
Batch: 724
Batch: 725
Batch: 726
Batch: 727
Batch: 728
Batch: 729
Batch: 730
Batch: 731
Batch: 732
Batch: 733
Batch: 734
Batch: 735
Batch: 736
Batch: 737
Batch: 738
Batch: 739
Batch: 740
Batch: 741
Batch: 742
Batch: 743
Batch: 744
Batch: 745
Batch: 746
Batch: 747
Batch: 748
Batch: 749
Batch: 750
Batch: 751
Batch: 752
Batch: 753
Batch: 754
Batch: 755
Batch: 756
Batch: 757
Batch: 758
Batch: 759
Batch: 760
Batch: 761
Batch: 762
Batch: 763
Batch: 764
Batch: 765
Batch: 766
Batch: 767
Batch: 768
Batch: 769
Batch: 770
Batch: 771
Batch: 772
Batch: 773
Batch: 774
Batch: 775
Batch: 776
Batch: 777
Batch: 778
Batch: 779
Batch: 780
Batch: 781
Batch: 782
Batch: 783
Batch: 784
Batch: 785
Batch: 786
Batch: 787
Batch: 788
Batch: 789
Batch: 790
Batch: 791
Batch: 792
Batch: 793
Batch: 794
Batch: 795
Batch: 796
Batch: 797
Batch: 798
Batch: 799
Batch: 800
Batch: 801
Batch: 802
Batch: 803
Batch: 804
Batch: 805
Batch: 806
Batch: 807
Batch: 808
Batch: 809
Batch: 810
Batch: 811
Batch: 812
Batch: 813
Batch: 814
Batch: 815
Batch: 816
Batch: 817
Batch: 818
Batch: 819
Batch: 820
Batch: 821
Batch: 822
Batch: 823
Batch: 824
Batch: 825
Batch: 826
Batch: 827
Batch: 828
Batch: 829
Batch: 830
Batch: 831
Batch: 832
Batch: 833
Batch: 834
Batch: 835
Batch: 836
Batch: 837
Batch: 838
Batch: 839
Batch: 840
Batch: 841
Batch: 842
Batch: 843
Batch: 844
Batch: 845
Batch: 846
Batch: 847
Batch: 848
Batch: 849
Batch: 850
Batch: 851
Batch: 852
Batch: 853
Batch: 854
Batch: 855
Batch: 856
Batch: 857
Batch: 858
Batch: 859
Batch: 860
Batch: 861
Batch: 862
Batch: 863
Batch: 864
Batch: 865
Batch: 866
Batch: 867
Batch: 868
Batch: 869
Batch: 870
Batch: 871
Batch: 872
Batch: 873
Batch: 874
Batch: 875
Batch: 876
Batch: 877
Batch: 878
Batch: 879
Batch: 880
Batch: 881
Batch: 882
Batch: 883
Batch: 884
Batch: 885
Batch: 886
Batch: 887
Batch: 888
Batch: 889
Batch: 890
Batch: 891
Batch: 892
Batch: 893
Batch: 894
Batch: 895
Batch: 896
Batch: 897
Batch: 898
Batch: 899
Batch: 900
Batch: 901
Batch: 902
Batch: 903
Batch: 904
Batch: 905
Batch: 906
Batch: 907
Batch: 908
Batch: 909
Batch: 910
Batch: 911
Batch: 912
Batch: 913
Batch: 914
Batch: 915
Batch: 916
Batch: 917
Batch: 918
Batch: 919
Batch: 920
Batch: 921
Batch: 922
Batch: 923
Batch: 924
Batch: 925
Batch: 926
Batch: 927
Batch: 928
Batch: 929
Batch: 930
Batch: 931
Batch: 932
Batch: 933
Batch: 934
Batch: 935
Batch: 936
Batch: 937
Batch: 938
Batch: 939
Batch: 940
Batch: 941
Batch: 942
Batch: 943
Batch: 944
Batch: 945
Batch: 946
Batch: 947
Batch: 948
Batch: 949
Batch: 950
Batch: 951
Batch: 952
Batch: 953
Batch: 954
Batch: 955
Batch: 956
Batch: 957
Batch: 958
Batch: 959
Batch: 960
Batch: 961
Batch: 962
Batch: 963
Batch: 964
Batch: 965
Batch: 966
Batch: 967
Batch: 968
Batch: 969
Batch: 970
Batch: 971
Batch: 972
Batch: 973
Batch: 974
Batch: 975
Batch: 976
Batch: 977
Batch: 978
Batch: 979
Batch: 980
Batch: 981
Batch: 982
Batch: 983
Batch: 984
Batch: 985
Batch: 986
Batch: 987
Batch: 988
Batch: 989
Batch: 990
Batch: 991
Batch: 992
Batch: 993
Batch: 994
Batch: 995
Batch: 996
Batch: 997
Batch: 998
Batch: 999
Batch: 1000
Batch: 1001
Batch: 1002
Batch: 1003
Batch: 1004
Batch: 1005
Batch: 1006
Batch: 1007
Batch: 1008
Batch: 1009
Batch: 1010
Batch: 1011
Batch: 1012
Batch: 1013
Batch: 1014
Batch: 1015
Batch: 1016
Batch: 1017
Batch: 1018
Batch: 1019
Batch: 1020
Batch: 1021
Batch: 1022
Batch: 1023
Batch: 1024
Batch: 1025
Batch: 1026
Batch: 1027
Batch: 1028
Batch: 1029
Batch: 1030
Batch: 1031
Batch: 1032
Batch: 1033
Batch: 1034
Batch: 1035
Batch: 1036
Batch: 1037
Batch: 1038
Batch: 1039
Batch: 1040
Batch: 1041
Batch: 1042
Batch: 1043
Batch: 1044
Batch: 1045
Batch: 1046
Batch: 1047
Batch: 1048
Batch: 1049
Batch: 1050
Batch: 1051
Batch: 1052
Batch: 1053
Batch: 1054
Batch: 1055
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000000
	speed: 0.1721s/iter; left time: 891.7905s
	iters: 200, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 862.6240s
	iters: 300, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 846.5368s
	iters: 400, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 828.2028s
	iters: 500, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 811.1455s
	iters: 600, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 794.2579s
	iters: 700, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 777.4073s
	iters: 800, epoch: 1 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 759.9380s
	iters: 900, epoch: 1 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 742.8000s
	iters: 1000, epoch: 1 | loss: 0.0000000
	speed: 0.1695s/iter; left time: 725.7264s
Epoch: 1 cost time: 179.45820307731628
Epoch: 1, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.0000000
	speed: 2.3502s/iter; left time: 9694.5242s
	iters: 200, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 683.1096s
	iters: 300, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 666.2040s
	iters: 400, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 649.1698s
	iters: 500, epoch: 2 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 632.9261s
	iters: 600, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 615.0971s
	iters: 700, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 597.7959s
	iters: 800, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 580.8067s
	iters: 900, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 564.1681s
	iters: 1000, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 547.0770s
Epoch: 2 cost time: 179.4820466041565
Epoch: 2, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 3 | loss: 0.0000000
	speed: 2.3506s/iter; left time: 7214.1091s
	iters: 200, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 503.5060s
	iters: 300, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 486.5420s
	iters: 400, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 469.6777s
	iters: 500, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 452.5549s
	iters: 600, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 435.6066s
	iters: 700, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 419.2153s
	iters: 800, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 401.7519s
	iters: 900, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 384.7857s
	iters: 1000, epoch: 3 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 367.5106s
Epoch: 3 cost time: 179.37657737731934
Epoch: 3, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 0.000125
	iters: 100, epoch: 4 | loss: 0.0000000
	speed: 2.3491s/iter; left time: 4728.7479s
	iters: 200, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 324.5140s
	iters: 300, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 307.4648s
	iters: 400, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 290.4666s
	iters: 500, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 273.5027s
	iters: 600, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 256.6004s
	iters: 700, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 239.9118s
	iters: 800, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 222.6799s
	iters: 900, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 205.7147s
	iters: 1000, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 188.7953s
Epoch: 4 cost time: 179.39546513557434
Epoch: 4, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-05
	iters: 100, epoch: 5 | loss: 0.0000000
	speed: 2.3512s/iter; left time: 2250.0729s
	iters: 200, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 145.4260s
	iters: 300, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 128.4540s
	iters: 400, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 111.5159s
	iters: 500, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 94.5080s
	iters: 600, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 77.5626s
	iters: 700, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 60.6008s
	iters: 800, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 43.6235s
	iters: 900, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 26.6873s
	iters: 1000, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 9.6751s
Epoch: 5 cost time: 179.5251340866089
Epoch: 5, Steps: 1056 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-05
>>>>>>>testing : SMAP_GPT4TS_SMAP_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (427617, 25)
train: (135183, 25)
test 427518
test: (427617, 25)
train: (135183, 25)
train 135084
Threshold : 8.520937871958617e-10
pred:    (42751800,)
gt:      (42751800,)
pred:  (42751800,)
gt:    (42751800,)
Accuracy : 0.9824, Precision : 0.9192, Recall : 0.9458, F-score : 0.9323, AUC : 0.9668
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000005
	speed: 0.1725s/iter; left time: 767.9226s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1701s/iter; left time: 739.9999s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1703s/iter; left time: 723.9212s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1700s/iter; left time: 705.6869s
Epoch: 1 cost time: 77.7552056312561
Epoch: 1, Steps: 455 | Train Loss: 0.0000012 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6702s/iter; left time: 2678.2989s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1702s/iter; left time: 663.1917s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1702s/iter; left time: 646.0060s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1701s/iter; left time: 628.8077s
Epoch: 2 cost time: 77.78454399108887
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000001 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6706s/iter; left time: 2374.4317s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 585.2566s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 568.1235s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 551.0484s
Epoch: 3 cost time: 77.73707318305969
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6711s/iter; left time: 2070.9260s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 507.7615s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 490.8506s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 473.8377s
Epoch: 4 cost time: 77.78385925292969
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6704s/iter; left time: 1763.9066s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 430.5050s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 413.5113s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 396.6784s
Epoch: 5 cost time: 77.76060009002686
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6707s/iter; left time: 1459.4365s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 353.0363s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 336.2882s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 318.7818s
Epoch: 6 cost time: 77.7214081287384
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6704s/iter; left time: 1153.7179s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 275.8058s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 258.8077s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 241.7555s
Epoch: 7 cost time: 77.78491568565369
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6710s/iter; left time: 849.5000s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 198.2787s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 181.3649s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 164.3000s
Epoch: 8 cost time: 77.76038074493408
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6711s/iter; left time: 544.2670s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 120.9281s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 103.9493s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 86.8914s
Epoch: 9 cost time: 77.77650618553162
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6712s/iter; left time: 238.9405s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 43.5777s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 26.5465s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 9.5257s
Epoch: 10 cost time: 77.79634165763855
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.0158640222357461e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9679, Precision : 0.8142, Recall : 0.9007, F-score : 0.8553, AUC : 0.9382
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Traceback (most recent call last):
  File "run.py", line 209, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 77, in train
    self.feature_encoder.fit(train_loader, setting)
  File "/root/taowei/Project/LLM/MADLLM/models/FeatureEncoder.py", line 71, in fit
    loss = self.loss(
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/layers/triple_loss.py", line 75, in forward
    negative_representation = encoder(input)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/layers/casual_cnn.py", line 210, in forward
    x = self.causal_cnn(x)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/layers/casual_cnn.py", line 177, in forward
    return self.network(x)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/layers/casual_cnn.py", line 132, in forward
    out_causal = self.causal(x)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 714, in forward
    return F.leaky_relu(input, self.negative_slope, self.inplace)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/functional.py", line 1378, in leaky_relu
    result = torch._C._nn.leaky_relu(input, negative_slope)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.64 GiB total capacity; 21.34 GiB already allocated; 9.56 MiB free; 21.43 GiB reserved in total by PyTorch)
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Traceback (most recent call last):
  File "run.py", line 209, in <module>
    exp.train(setting)
  File "/root/taowei/Project/LLM/MADLLM/exp/exp_anomaly_detection.py", line 77, in train
    self.feature_encoder.fit(train_loader, setting)
  File "/root/taowei/Project/LLM/MADLLM/models/FeatureEncoder.py", line 71, in fit
    loss = self.loss(
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/layers/triple_loss.py", line 75, in forward
    negative_representation = encoder(input)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/layers/casual_cnn.py", line 210, in forward
    x = self.causal_cnn(x)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/layers/casual_cnn.py", line 177, in forward
    return self.network(x)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/taowei/Project/LLM/MADLLM/models/layers/casual_cnn.py", line 132, in forward
    out_causal = self.causal(x)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 714, in forward
    return F.leaky_relu(input, self.negative_slope, self.inplace)
  File "/root/Downloads/yes/envs/onefits/lib/python3.8/site-packages/torch/nn/functional.py", line 1378, in leaky_relu
    result = torch._C._nn.leaky_relu(input, negative_slope)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.64 GiB total capacity; 21.34 GiB already allocated; 9.56 MiB free; 21.43 GiB reserved in total by PyTorch)
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Epoch: 1
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Epoch: 2
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000004
	speed: 0.1724s/iter; left time: 767.3667s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1699s/iter; left time: 739.3627s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1702s/iter; left time: 723.3579s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1699s/iter; left time: 705.1324s
Epoch: 1 cost time: 77.69381880760193
Epoch: 1, Steps: 455 | Train Loss: 0.0000011 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6710s/iter; left time: 2681.3732s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1700s/iter; left time: 662.1887s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1700s/iter; left time: 645.2579s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1700s/iter; left time: 628.3089s
Epoch: 2 cost time: 77.72354173660278
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6705s/iter; left time: 2374.1500s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 584.7174s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 567.6481s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 550.6675s
Epoch: 3 cost time: 77.68046116828918
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6711s/iter; left time: 2071.0032s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 507.5269s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 490.6252s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 473.5707s
Epoch: 4 cost time: 77.72553300857544
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6703s/iter; left time: 1763.6016s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 430.4756s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 413.2677s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 396.1901s
Epoch: 5 cost time: 77.71129631996155
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6705s/iter; left time: 1458.9622s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 352.9528s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 335.9101s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 319.3344s
Epoch: 6 cost time: 77.73331999778748
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6704s/iter; left time: 1153.7110s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 275.5180s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 258.5620s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 241.6390s
Epoch: 7 cost time: 77.71905994415283
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6705s/iter; left time: 848.8562s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 198.1954s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 181.2199s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 164.1731s
Epoch: 8 cost time: 77.69754958152771
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6709s/iter; left time: 544.0662s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1703s/iter; left time: 121.0644s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 103.8799s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 86.8357s
Epoch: 9 cost time: 77.73528456687927
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6709s/iter; left time: 238.8315s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 43.4987s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 26.4960s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 9.5193s
Epoch: 10 cost time: 77.69604825973511
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 9.805507545479485e-08
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9616, Precision : 0.7993, Recall : 0.8489, F-score : 0.8234, AUC : 0.9119
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Epoch: 1
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Epoch: 2
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000002
	speed: 0.1723s/iter; left time: 766.9622s
	iters: 200, epoch: 1 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 739.1894s
	iters: 300, epoch: 1 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 723.4465s
	iters: 400, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 705.3111s
Epoch: 1 cost time: 77.69153094291687
Epoch: 1, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.0000000
	speed: 0.6710s/iter; left time: 2681.1992s
	iters: 200, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 662.8010s
	iters: 300, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 645.6229s
	iters: 400, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 628.1937s
Epoch: 2 cost time: 77.7330846786499
Epoch: 2, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.0000000
	speed: 0.6706s/iter; left time: 2374.4628s
	iters: 200, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 585.2791s
	iters: 300, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 568.4613s
	iters: 400, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 551.3164s
Epoch: 3 cost time: 77.75856137275696
Epoch: 3, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.0000000
	speed: 0.6711s/iter; left time: 2071.0355s
	iters: 200, epoch: 4 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 508.3294s
	iters: 300, epoch: 4 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 491.1934s
	iters: 400, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 474.0148s
Epoch: 4 cost time: 77.82286858558655
Epoch: 4, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 0.000125
	iters: 100, epoch: 5 | loss: 0.0000000
	speed: 0.6710s/iter; left time: 1765.3237s
	iters: 200, epoch: 5 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 430.7180s
	iters: 300, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 413.5095s
	iters: 400, epoch: 5 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 396.7202s
Epoch: 5 cost time: 77.77560925483704
Epoch: 5, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-05
	iters: 100, epoch: 6 | loss: 0.0000000
	speed: 0.6707s/iter; left time: 1459.3547s
	iters: 200, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 352.9650s
	iters: 300, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 335.9907s
	iters: 400, epoch: 6 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 319.3653s
Epoch: 6 cost time: 77.73945355415344
Epoch: 6, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-05
	iters: 100, epoch: 7 | loss: 0.0000000
	speed: 0.6710s/iter; left time: 1154.7233s
	iters: 200, epoch: 7 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 275.8093s
	iters: 300, epoch: 7 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 258.7310s
	iters: 400, epoch: 7 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 241.7256s
Epoch: 7 cost time: 77.77884364128113
Epoch: 7, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-05
	iters: 100, epoch: 8 | loss: 0.0000000
	speed: 0.6708s/iter; left time: 849.1743s
	iters: 200, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 198.2957s
	iters: 300, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 181.2043s
	iters: 400, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 164.2710s
Epoch: 8 cost time: 77.73027896881104
Epoch: 8, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-06
	iters: 100, epoch: 9 | loss: 0.0000000
	speed: 0.6672s/iter; left time: 541.1151s
	iters: 200, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 120.8233s
	iters: 300, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 103.8313s
	iters: 400, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 86.8494s
Epoch: 9 cost time: 77.7102906703949
Epoch: 9, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-06
	iters: 100, epoch: 10 | loss: 0.0000000
	speed: 0.6704s/iter; left time: 238.6711s
	iters: 200, epoch: 10 | loss: 0.0000000
	speed: 0.1702s/iter; left time: 43.5688s
	iters: 300, epoch: 10 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 26.5413s
	iters: 400, epoch: 10 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 9.5228s
Epoch: 10 cost time: 77.75388669967651
Epoch: 10, Steps: 455 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-06
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 7.145743659009438e-09
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9512, Precision : 0.7422, Recall : 0.8228, F-score : 0.7805, AUC : 0.8946
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000005
	speed: 0.1723s/iter; left time: 766.9478s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1698s/iter; left time: 738.9707s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1701s/iter; left time: 723.0722s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1698s/iter; left time: 704.9160s
Epoch: 1 cost time: 77.68090724945068
Epoch: 1, Steps: 455 | Train Loss: 0.0000011 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6702s/iter; left time: 2678.1239s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1700s/iter; left time: 662.1275s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1700s/iter; left time: 645.2855s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1700s/iter; left time: 628.3496s
Epoch: 2 cost time: 77.68744897842407
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000001 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6703s/iter; left time: 2373.6064s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 584.6750s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 567.7865s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 550.8028s
Epoch: 3 cost time: 77.6909248828888
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6710s/iter; left time: 2070.7305s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 507.4886s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 490.5314s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 473.4584s
Epoch: 4 cost time: 77.71632742881775
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6707s/iter; left time: 1764.4858s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 430.4230s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 413.3898s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 396.2171s
Epoch: 5 cost time: 77.74447321891785
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6705s/iter; left time: 1459.0772s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 352.8972s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 335.9119s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 319.3491s
Epoch: 6 cost time: 77.7283525466919
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6705s/iter; left time: 1153.9649s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 275.6227s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 258.6148s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 241.5555s
Epoch: 7 cost time: 77.73163104057312
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6703s/iter; left time: 848.5806s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 198.1181s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 181.1825s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 164.1758s
Epoch: 8 cost time: 77.69804382324219
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6704s/iter; left time: 543.6963s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 120.8759s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 103.9968s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 86.8540s
Epoch: 9 cost time: 77.73980808258057
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6708s/iter; left time: 238.7977s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 43.5282s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 26.5256s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 9.5216s
Epoch: 10 cost time: 77.73013758659363
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.0747260247967458e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9620, Precision : 0.7919, Recall : 0.8676, F-score : 0.8280, AUC : 0.9203
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=5, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=15, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Epoch: 1
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Epoch: 2
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Epoch: 3
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Epoch: 4
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000004
	speed: 0.1725s/iter; left time: 767.8430s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1699s/iter; left time: 739.0428s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1701s/iter; left time: 723.2156s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1699s/iter; left time: 705.3454s
Epoch: 1 cost time: 77.70372462272644
Epoch: 1, Steps: 455 | Train Loss: 0.0000010 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6723s/iter; left time: 2686.4453s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1698s/iter; left time: 661.6812s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1698s/iter; left time: 644.6083s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1698s/iter; left time: 627.7257s
Epoch: 2 cost time: 77.64960813522339
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000001 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6727s/iter; left time: 2381.9410s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 584.2824s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 567.2399s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 550.3570s
Epoch: 3 cost time: 77.63907074928284
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6728s/iter; left time: 2076.3992s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 507.5160s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 490.5178s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 473.5691s
Epoch: 4 cost time: 77.72723078727722
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6728s/iter; left time: 1770.1991s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 430.1552s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 413.0539s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 396.1177s
Epoch: 5 cost time: 77.69803214073181
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6727s/iter; left time: 1463.7208s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 352.7242s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 336.1715s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 318.7222s
Epoch: 6 cost time: 77.69641184806824
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6727s/iter; left time: 1157.7094s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 275.4088s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 258.4633s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 241.5069s
Epoch: 7 cost time: 77.67099952697754
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6728s/iter; left time: 851.7357s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 198.0790s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 180.9742s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 164.0022s
Epoch: 8 cost time: 77.63977098464966
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6741s/iter; left time: 546.6745s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 120.8276s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 103.8322s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 86.8704s
Epoch: 9 cost time: 77.73117136955261
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6761s/iter; left time: 240.6754s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 43.5273s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 26.5018s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 9.5140s
Epoch: 10 cost time: 77.69586181640625
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.1577930408179775e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9654, Precision : 0.8262, Recall : 0.8511, F-score : 0.8385, AUC : 0.9150
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=20, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs20>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000005
	speed: 0.1744s/iter; left time: 776.0833s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1699s/iter; left time: 739.0238s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1701s/iter; left time: 723.0541s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1698s/iter; left time: 705.0244s
Epoch: 1 cost time: 77.8388557434082
Epoch: 1, Steps: 455 | Train Loss: 0.0000013 Vali Loss: 0.0000002 Test Loss: 0.0000003
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6648s/iter; left time: 2656.6249s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1700s/iter; left time: 662.1787s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 644.3462s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 627.3589s
Epoch: 2 cost time: 77.41245675086975
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000002
	speed: 0.6656s/iter; left time: 2356.8185s
	iters: 200, epoch: 3 | loss: 0.0000002
	speed: 0.1698s/iter; left time: 584.1366s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 567.1684s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 550.1871s
Epoch: 3 cost time: 77.42411255836487
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6660s/iter; left time: 2055.2363s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 506.6306s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 489.8405s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 472.9196s
Epoch: 4 cost time: 77.41959881782532
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6663s/iter; left time: 1753.1445s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 429.7614s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 412.4374s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 395.5273s
Epoch: 5 cost time: 77.40029001235962
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6655s/iter; left time: 1448.0532s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 352.1363s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 335.4212s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 318.8031s
Epoch: 6 cost time: 77.40920543670654
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6660s/iter; left time: 1146.2636s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 275.1011s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 258.0816s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 241.0828s
Epoch: 7 cost time: 77.40147233009338
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6661s/iter; left time: 843.3333s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 198.0433s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 181.0269s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 164.0403s
Epoch: 8 cost time: 77.44467401504517
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6662s/iter; left time: 540.3065s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 120.9079s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 103.7236s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 86.7229s
Epoch: 9 cost time: 77.43840169906616
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6661s/iter; left time: 237.1180s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 43.4397s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 26.4738s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 9.5026s
Epoch: 10 cost time: 77.40426898002625
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs20<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.1469739547464991e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9517, Precision : 0.7755, Recall : 0.7630, F-score : 0.7692, AUC : 0.8685
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=15, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000004
	speed: 0.1704s/iter; left time: 758.4094s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1695s/iter; left time: 737.5168s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1697s/iter; left time: 721.4360s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1695s/iter; left time: 703.6380s
Epoch: 1 cost time: 77.31797194480896
Epoch: 1, Steps: 455 | Train Loss: 0.0000010 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6660s/iter; left time: 2661.3547s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 661.1982s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 644.2667s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 627.2719s
Epoch: 2 cost time: 77.40864872932434
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000001 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6662s/iter; left time: 2359.0422s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 584.1352s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 567.4501s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 550.3278s
Epoch: 3 cost time: 77.43698525428772
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6664s/iter; left time: 2056.4795s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 506.9088s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 490.0403s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 473.3042s
Epoch: 4 cost time: 77.46590995788574
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6665s/iter; left time: 1753.6033s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 429.7596s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 412.7661s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 395.6764s
Epoch: 5 cost time: 77.43866777420044
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6663s/iter; left time: 1449.8918s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 352.7453s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 336.1774s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 318.6364s
Epoch: 6 cost time: 77.4885151386261
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6672s/iter; left time: 1148.3325s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 275.3060s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 258.2395s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 241.1909s
Epoch: 7 cost time: 77.44471192359924
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6659s/iter; left time: 843.0742s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 197.8481s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 180.8626s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 163.9136s
Epoch: 8 cost time: 77.38985276222229
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6678s/iter; left time: 541.5884s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 120.8122s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 103.8645s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 86.7981s
Epoch: 9 cost time: 77.51291561126709
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6669s/iter; left time: 237.3991s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 43.4987s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 26.4932s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 9.5111s
Epoch: 10 cost time: 77.46034002304077
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.1346265566203326e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9577, Precision : 0.7957, Recall : 0.8054, F-score : 0.8005, AUC : 0.8905
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000004
	speed: 0.1706s/iter; left time: 759.2284s
	iters: 200, epoch: 1 | loss: 0.0000004
	speed: 0.1697s/iter; left time: 738.4539s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1700s/iter; left time: 722.6276s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1698s/iter; left time: 704.7536s
Epoch: 1 cost time: 77.44942331314087
Epoch: 1, Steps: 455 | Train Loss: 0.0000011 Vali Loss: 0.0000002 Test Loss: 0.0000003
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6659s/iter; left time: 2660.8390s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 661.1106s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 644.1501s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 627.1260s
Epoch: 2 cost time: 77.3949019908905
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6662s/iter; left time: 2359.1533s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 584.5146s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 567.1503s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 549.9490s
Epoch: 3 cost time: 77.4323251247406
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6664s/iter; left time: 2056.4300s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 506.6123s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 489.6324s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 472.7245s
Epoch: 4 cost time: 77.40390944480896
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6664s/iter; left time: 1753.2200s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 429.6236s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 412.6216s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 395.5698s
Epoch: 5 cost time: 77.42114615440369
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6664s/iter; left time: 1450.0290s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 352.4774s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 335.5545s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 318.7543s
Epoch: 6 cost time: 77.49493169784546
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6668s/iter; left time: 1147.5365s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 275.3728s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 258.3263s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 241.3456s
Epoch: 7 cost time: 77.4857108592987
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6664s/iter; left time: 843.6363s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 198.0942s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 181.1351s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 164.1298s
Epoch: 8 cost time: 77.49075412750244
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6668s/iter; left time: 540.7655s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 120.7388s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 103.7382s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 86.8362s
Epoch: 9 cost time: 77.45877313613892
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6663s/iter; left time: 237.2133s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 43.4635s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 26.4888s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 9.5085s
Epoch: 10 cost time: 77.44340300559998
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.241642493710059e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9474, Precision : 0.7668, Recall : 0.7197, F-score : 0.7425, AUC : 0.8469
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=15, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000004
	speed: 0.1706s/iter; left time: 759.3961s
	iters: 200, epoch: 1 | loss: 0.0000004
	speed: 0.1697s/iter; left time: 738.4780s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1699s/iter; left time: 722.4086s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1698s/iter; left time: 704.7632s
Epoch: 1 cost time: 77.42978739738464
Epoch: 1, Steps: 455 | Train Loss: 0.0000012 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6630s/iter; left time: 2649.3980s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 661.2360s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 644.3042s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 627.2910s
Epoch: 2 cost time: 77.38450241088867
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000002
	speed: 0.6634s/iter; left time: 2349.2580s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 583.8647s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 567.2892s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 550.0427s
Epoch: 3 cost time: 77.37750053405762
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6620s/iter; left time: 2043.0464s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 506.7776s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 489.4843s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 472.5228s
Epoch: 4 cost time: 77.34446263313293
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6629s/iter; left time: 1744.0989s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 429.9007s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 412.6776s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 395.7551s
Epoch: 5 cost time: 77.4232497215271
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6659s/iter; left time: 1448.9088s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 353.0496s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1679s/iter; left time: 331.8108s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 318.6965s
Epoch: 6 cost time: 77.30921912193298
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6653s/iter; left time: 1145.0550s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 275.5846s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 258.3885s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 241.1720s
Epoch: 7 cost time: 77.48043298721313
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6654s/iter; left time: 842.3425s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1691s/iter; left time: 197.1968s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1693s/iter; left time: 180.4339s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1694s/iter; left time: 163.6047s
Epoch: 8 cost time: 77.33356809616089
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6632s/iter; left time: 537.8926s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 120.8888s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 103.7639s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 86.7802s
Epoch: 9 cost time: 77.483633518219
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6641s/iter; left time: 236.4224s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 43.4923s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 26.4991s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 9.5213s
Epoch: 10 cost time: 77.47484803199768
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.109065414084398e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9697, Precision : 0.8340, Recall : 0.8894, F-score : 0.8608, AUC : 0.9343
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=15, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000004
	speed: 0.1708s/iter; left time: 760.3490s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1697s/iter; left time: 738.5131s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1700s/iter; left time: 722.5715s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1697s/iter; left time: 704.3345s
Epoch: 1 cost time: 77.4360613822937
Epoch: 1, Steps: 455 | Train Loss: 0.0000011 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6601s/iter; left time: 2637.7159s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1696s/iter; left time: 660.8669s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1696s/iter; left time: 643.7075s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1696s/iter; left time: 626.7207s
Epoch: 2 cost time: 77.33792281150818
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6596s/iter; left time: 2335.7483s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 583.6084s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 566.5847s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 549.8439s
Epoch: 3 cost time: 77.32228422164917
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6601s/iter; left time: 2036.9237s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 506.4790s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 489.5978s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 472.6485s
Epoch: 4 cost time: 77.38149929046631
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6606s/iter; left time: 1738.0838s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 429.3849s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 412.4416s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 395.4056s
Epoch: 5 cost time: 77.37229919433594
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6605s/iter; left time: 1437.3062s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 352.2950s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 335.5678s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 318.1038s
Epoch: 6 cost time: 77.38765811920166
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6610s/iter; left time: 1137.5897s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 275.3419s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 258.1642s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 241.1649s
Epoch: 7 cost time: 77.4255793094635
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6603s/iter; left time: 835.9976s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 198.0618s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 181.0393s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 164.0130s
Epoch: 8 cost time: 77.42288708686829
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6616s/iter; left time: 536.5529s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 120.6620s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 103.6789s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 86.6862s
Epoch: 9 cost time: 77.40113067626953
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6608s/iter; left time: 235.2399s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 43.4245s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 26.4679s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1696s/iter; left time: 9.4994s
Epoch: 10 cost time: 77.37512183189392
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.0712246336197488e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9685, Precision : 0.8242, Recall : 0.8914, F-score : 0.8565, AUC : 0.9345
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=15, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000005
	speed: 0.1706s/iter; left time: 759.1898s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1697s/iter; left time: 738.3974s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1700s/iter; left time: 722.4700s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1697s/iter; left time: 704.3486s
Epoch: 1 cost time: 77.41315484046936
Epoch: 1, Steps: 455 | Train Loss: 0.0000011 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6616s/iter; left time: 2643.6938s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1697s/iter; left time: 660.9656s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1698s/iter; left time: 644.6175s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1698s/iter; left time: 627.6717s
Epoch: 2 cost time: 77.43621611595154
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6619s/iter; left time: 2343.9219s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 584.6306s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 567.8328s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 550.5640s
Epoch: 3 cost time: 77.4790997505188
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6632s/iter; left time: 2046.5930s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 507.2915s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 490.3646s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 473.3450s
Epoch: 4 cost time: 77.53143739700317
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6623s/iter; left time: 1742.5719s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1704s/iter; left time: 431.2409s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 413.1505s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 396.0369s
Epoch: 5 cost time: 77.54778361320496
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6622s/iter; left time: 1440.8877s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 352.4908s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 335.5373s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 318.7933s
Epoch: 6 cost time: 77.44714760780334
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6625s/iter; left time: 1140.2469s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 275.3159s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 258.3767s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 241.3547s
Epoch: 7 cost time: 77.46751117706299
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6623s/iter; left time: 838.5002s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 198.1439s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 181.1268s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 164.0778s
Epoch: 8 cost time: 77.51460385322571
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6630s/iter; left time: 537.7003s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 120.9494s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 103.7218s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 86.7547s
Epoch: 9 cost time: 77.43946599960327
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6624s/iter; left time: 235.8012s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 43.5132s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 26.5062s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 9.5120s
Epoch: 10 cost time: 77.48711657524109
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.0480146585223336e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9639, Precision : 0.8019, Recall : 0.8736, F-score : 0.8362, AUC : 0.9241
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=15, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 11565
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000004
	speed: 0.1708s/iter; left time: 760.2147s
	iters: 200, epoch: 1 | loss: 0.0000003
	speed: 0.1700s/iter; left time: 739.6416s
	iters: 300, epoch: 1 | loss: 0.0000003
	speed: 0.1702s/iter; left time: 723.7204s
	iters: 400, epoch: 1 | loss: 0.0000003
	speed: 0.1700s/iter; left time: 705.6857s
Epoch: 1 cost time: 77.53777003288269
Epoch: 1, Steps: 455 | Train Loss: 0.0000012 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000002
	speed: 0.6643s/iter; left time: 2654.5032s
	iters: 200, epoch: 2 | loss: 0.0000002
	speed: 0.1699s/iter; left time: 662.0942s
	iters: 300, epoch: 2 | loss: 0.0000002
	speed: 0.1699s/iter; left time: 644.9315s
	iters: 400, epoch: 2 | loss: 0.0000002
	speed: 0.1699s/iter; left time: 628.0779s
Epoch: 2 cost time: 77.51342129707336
Epoch: 2, Steps: 455 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000001
	speed: 0.6645s/iter; left time: 2352.8406s
	iters: 200, epoch: 3 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 584.8356s
	iters: 300, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 567.6688s
	iters: 400, epoch: 3 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 550.6642s
Epoch: 3 cost time: 77.49982595443726
Epoch: 3, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000001
	speed: 0.6645s/iter; left time: 2050.6758s
	iters: 200, epoch: 4 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 507.2077s
	iters: 300, epoch: 4 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 489.8845s
	iters: 400, epoch: 4 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 472.9810s
Epoch: 4 cost time: 77.46640968322754
Epoch: 4, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000001
	speed: 0.6641s/iter; left time: 1747.2798s
	iters: 200, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 430.2193s
	iters: 300, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 413.3042s
	iters: 400, epoch: 5 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 396.2067s
Epoch: 5 cost time: 77.52130699157715
Epoch: 5, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000001
	speed: 0.6638s/iter; left time: 1444.4859s
	iters: 200, epoch: 6 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 353.0709s
	iters: 300, epoch: 6 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 335.8924s
	iters: 400, epoch: 6 | loss: 0.0000001
	speed: 0.1702s/iter; left time: 319.3376s
Epoch: 6 cost time: 77.549973487854
Epoch: 6, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000001
	speed: 0.6642s/iter; left time: 1143.0184s
	iters: 200, epoch: 7 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 275.4461s
	iters: 300, epoch: 7 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 258.4623s
	iters: 400, epoch: 7 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 241.5002s
Epoch: 7 cost time: 77.50431180000305
Epoch: 7, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000001
	speed: 0.6652s/iter; left time: 842.1778s
	iters: 200, epoch: 8 | loss: 0.0000001
	speed: 0.1699s/iter; left time: 198.1100s
	iters: 300, epoch: 8 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 181.2238s
	iters: 400, epoch: 8 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 164.1844s
Epoch: 8 cost time: 77.5190703868866
Epoch: 8, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000001
	speed: 0.6643s/iter; left time: 538.7618s
	iters: 200, epoch: 9 | loss: 0.0000001
	speed: 0.1703s/iter; left time: 121.0629s
	iters: 300, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 103.8578s
	iters: 400, epoch: 9 | loss: 0.0000001
	speed: 0.1700s/iter; left time: 86.8638s
Epoch: 9 cost time: 77.54426169395447
Epoch: 9, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000001
	speed: 0.6645s/iter; left time: 236.5601s
	iters: 200, epoch: 10 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 43.5508s
	iters: 300, epoch: 10 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 26.5395s
	iters: 400, epoch: 10 | loss: 0.0000001
	speed: 0.1701s/iter; left time: 9.5276s
Epoch: 10 cost time: 77.5928602218628
Epoch: 10, Steps: 455 | Train Loss: 0.0000001 Vali Loss: 0.0000001 Test Loss: 0.0000001
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs15<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 73630
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 58218
Threshold : 1.07376643114776e-07
pred:    (7363000,)
gt:      (7363000,)
pred:  (7363000,)
gt:    (7363000,)
Accuracy : 0.9650, Precision : 0.8253, Recall : 0.8476, F-score : 0.8363, AUC : 0.9132
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 494901
test: (449919, 51)
train: (495000, 51)
val 98901
test: (449919, 51)
train: (495000, 51)
test 449820
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Batch: 455
Batch: 456
Batch: 457
Batch: 458
Batch: 459
Batch: 460
Batch: 461
Batch: 462
Batch: 463
Batch: 464
Batch: 465
Batch: 466
Batch: 467
Batch: 468
Batch: 469
Batch: 470
Batch: 471
Batch: 472
Batch: 473
Batch: 474
Batch: 475
Batch: 476
Batch: 477
Batch: 478
Batch: 479
Batch: 480
Batch: 481
Batch: 482
Batch: 483
Batch: 484
Batch: 485
Batch: 486
Batch: 487
Batch: 488
Batch: 489
Batch: 490
Batch: 491
Batch: 492
Batch: 493
Batch: 494
Batch: 495
Batch: 496
Batch: 497
Batch: 498
Batch: 499
Batch: 500
Batch: 501
Batch: 502
Batch: 503
Batch: 504
Batch: 505
Batch: 506
Batch: 507
Batch: 508
Batch: 509
Batch: 510
Batch: 511
Batch: 512
Batch: 513
Batch: 514
Batch: 515
Batch: 516
Batch: 517
Batch: 518
Batch: 519
Batch: 520
Batch: 521
Batch: 522
Batch: 523
Batch: 524
Batch: 525
Batch: 526
Batch: 527
Batch: 528
Batch: 529
Batch: 530
Batch: 531
Batch: 532
Batch: 533
Batch: 534
Batch: 535
Batch: 536
Batch: 537
Batch: 538
Batch: 539
Batch: 540
Batch: 541
Batch: 542
Batch: 543
Batch: 544
Batch: 545
Batch: 546
Batch: 547
Batch: 548
Batch: 549
Batch: 550
Batch: 551
Batch: 552
Batch: 553
Batch: 554
Batch: 555
Batch: 556
Batch: 557
Batch: 558
Batch: 559
Batch: 560
Batch: 561
Batch: 562
Batch: 563
Batch: 564
Batch: 565
Batch: 566
Batch: 567
Batch: 568
Batch: 569
Batch: 570
Batch: 571
Batch: 572
Batch: 573
Batch: 574
Batch: 575
Batch: 576
Batch: 577
Batch: 578
Batch: 579
Batch: 580
Batch: 581
Batch: 582
Batch: 583
Batch: 584
Batch: 585
Batch: 586
Batch: 587
Batch: 588
Batch: 589
Batch: 590
Batch: 591
Batch: 592
Batch: 593
Batch: 594
Batch: 595
Batch: 596
Batch: 597
Batch: 598
Batch: 599
Batch: 600
Batch: 601
Batch: 602
Batch: 603
Batch: 604
Batch: 605
Batch: 606
Batch: 607
Batch: 608
Batch: 609
Batch: 610
Batch: 611
Batch: 612
Batch: 613
Batch: 614
Batch: 615
Batch: 616
Batch: 617
Batch: 618
Batch: 619
Batch: 620
Batch: 621
Batch: 622
Batch: 623
Batch: 624
Batch: 625
Batch: 626
Batch: 627
Batch: 628
Batch: 629
Batch: 630
Batch: 631
Batch: 632
Batch: 633
Batch: 634
Batch: 635
Batch: 636
Batch: 637
Batch: 638
Batch: 639
Batch: 640
Batch: 641
Batch: 642
Batch: 643
Batch: 644
Batch: 645
Batch: 646
Batch: 647
Batch: 648
Batch: 649
Batch: 650
Batch: 651
Batch: 652
Batch: 653
Batch: 654
Batch: 655
Batch: 656
Batch: 657
Batch: 658
Batch: 659
Batch: 660
Batch: 661
Batch: 662
Batch: 663
Batch: 664
Batch: 665
Batch: 666
Batch: 667
Batch: 668
Batch: 669
Batch: 670
Batch: 671
Batch: 672
Batch: 673
Batch: 674
Batch: 675
Batch: 676
Batch: 677
Batch: 678
Batch: 679
Batch: 680
Batch: 681
Batch: 682
Batch: 683
Batch: 684
Batch: 685
Batch: 686
Batch: 687
Batch: 688
Batch: 689
Batch: 690
Batch: 691
Batch: 692
Batch: 693
Batch: 694
Batch: 695
Batch: 696
Batch: 697
Batch: 698
Batch: 699
Batch: 700
Batch: 701
Batch: 702
Batch: 703
Batch: 704
Batch: 705
Batch: 706
Batch: 707
Batch: 708
Batch: 709
Batch: 710
Batch: 711
Batch: 712
Batch: 713
Batch: 714
Batch: 715
Batch: 716
Batch: 717
Batch: 718
Batch: 719
Batch: 720
Batch: 721
Batch: 722
Batch: 723
Batch: 724
Batch: 725
Batch: 726
Batch: 727
Batch: 728
Batch: 729
Batch: 730
Batch: 731
Batch: 732
Batch: 733
Batch: 734
Batch: 735
Batch: 736
Batch: 737
Batch: 738
Batch: 739
Batch: 740
Batch: 741
Batch: 742
Batch: 743
Batch: 744
Batch: 745
Batch: 746
Batch: 747
Batch: 748
Batch: 749
Batch: 750
Batch: 751
Batch: 752
Batch: 753
Batch: 754
Batch: 755
Batch: 756
Batch: 757
Batch: 758
Batch: 759
Batch: 760
Batch: 761
Batch: 762
Batch: 763
Batch: 764
Batch: 765
Batch: 766
Batch: 767
Batch: 768
Batch: 769
Batch: 770
Batch: 771
Batch: 772
Batch: 773
Batch: 774
Batch: 775
Batch: 776
Batch: 777
Batch: 778
Batch: 779
Batch: 780
Batch: 781
Batch: 782
Batch: 783
Batch: 784
Batch: 785
Batch: 786
Batch: 787
Batch: 788
Batch: 789
Batch: 790
Batch: 791
Batch: 792
Batch: 793
Batch: 794
Batch: 795
Batch: 796
Batch: 797
Batch: 798
Batch: 799
Batch: 800
Batch: 801
Batch: 802
Batch: 803
Batch: 804
Batch: 805
Batch: 806
Batch: 807
Batch: 808
Batch: 809
Batch: 810
Batch: 811
Batch: 812
Batch: 813
Batch: 814
Batch: 815
Batch: 816
Batch: 817
Batch: 818
Batch: 819
Batch: 820
Batch: 821
Batch: 822
Batch: 823
Batch: 824
Batch: 825
Batch: 826
Batch: 827
Batch: 828
Batch: 829
Batch: 830
Batch: 831
Batch: 832
Batch: 833
Batch: 834
Batch: 835
Batch: 836
Batch: 837
Batch: 838
Batch: 839
Batch: 840
Batch: 841
Batch: 842
Batch: 843
Batch: 844
Batch: 845
Batch: 846
Batch: 847
Batch: 848
Batch: 849
Batch: 850
Batch: 851
Batch: 852
Batch: 853
Batch: 854
Batch: 855
Batch: 856
Batch: 857
Batch: 858
Batch: 859
Batch: 860
Batch: 861
Batch: 862
Batch: 863
Batch: 864
Batch: 865
Batch: 866
Batch: 867
Batch: 868
Batch: 869
Batch: 870
Batch: 871
Batch: 872
Batch: 873
Batch: 874
Batch: 875
Batch: 876
Batch: 877
Batch: 878
Batch: 879
Batch: 880
Batch: 881
Batch: 882
Batch: 883
Batch: 884
Batch: 885
Batch: 886
Batch: 887
Batch: 888
Batch: 889
Batch: 890
Batch: 891
Batch: 892
Batch: 893
Batch: 894
Batch: 895
Batch: 896
Batch: 897
Batch: 898
Batch: 899
Batch: 900
Batch: 901
Batch: 902
Batch: 903
Batch: 904
Batch: 905
Batch: 906
Batch: 907
Batch: 908
Batch: 909
Batch: 910
Batch: 911
Batch: 912
Batch: 913
Batch: 914
Batch: 915
Batch: 916
Batch: 917
Batch: 918
Batch: 919
Batch: 920
Batch: 921
Batch: 922
Batch: 923
Batch: 924
Batch: 925
Batch: 926
Batch: 927
Batch: 928
Batch: 929
Batch: 930
Batch: 931
Batch: 932
Batch: 933
Batch: 934
Batch: 935
Batch: 936
Batch: 937
Batch: 938
Batch: 939
Batch: 940
Batch: 941
Batch: 942
Batch: 943
Batch: 944
Batch: 945
Batch: 946
Batch: 947
Batch: 948
Batch: 949
Batch: 950
Batch: 951
Batch: 952
Batch: 953
Batch: 954
Batch: 955
Batch: 956
Batch: 957
Batch: 958
Batch: 959
Batch: 960
Batch: 961
Batch: 962
Batch: 963
Batch: 964
Batch: 965
Batch: 966
Batch: 967
Batch: 968
Batch: 969
Batch: 970
Batch: 971
Batch: 972
Batch: 973
Batch: 974
Batch: 975
Batch: 976
Batch: 977
Batch: 978
Batch: 979
Batch: 980
Batch: 981
Batch: 982
Batch: 983
Batch: 984
Batch: 985
Batch: 986
Batch: 987
Batch: 988
Batch: 989
Batch: 990
Batch: 991
Batch: 992
Batch: 993
Batch: 994
Batch: 995
Batch: 996
Batch: 997
Batch: 998
Batch: 999
Batch: 1000
Batch: 1001
Batch: 1002
Batch: 1003
Batch: 1004
Batch: 1005
Batch: 1006
Batch: 1007
Batch: 1008
Batch: 1009
Batch: 1010
Batch: 1011
Batch: 1012
Batch: 1013
Batch: 1014
Batch: 1015
Batch: 1016
Batch: 1017
Batch: 1018
Batch: 1019
Batch: 1020
Batch: 1021
Batch: 1022
Batch: 1023
Batch: 1024
Batch: 1025
Batch: 1026
Batch: 1027
Batch: 1028
Batch: 1029
Batch: 1030
Batch: 1031
Batch: 1032
Batch: 1033
Batch: 1034
Batch: 1035
Batch: 1036
Batch: 1037
Batch: 1038
Batch: 1039
Batch: 1040
Batch: 1041
Batch: 1042
Batch: 1043
Batch: 1044
Batch: 1045
Batch: 1046
Batch: 1047
Batch: 1048
Batch: 1049
Batch: 1050
Batch: 1051
Batch: 1052
Batch: 1053
Batch: 1054
Batch: 1055
Batch: 1056
Batch: 1057
Batch: 1058
Batch: 1059
Batch: 1060
Batch: 1061
Batch: 1062
Batch: 1063
Batch: 1064
Batch: 1065
Batch: 1066
Batch: 1067
Batch: 1068
Batch: 1069
Batch: 1070
Batch: 1071
Batch: 1072
Batch: 1073
Batch: 1074
Batch: 1075
Batch: 1076
Batch: 1077
Batch: 1078
Batch: 1079
Batch: 1080
Batch: 1081
Batch: 1082
Batch: 1083
Batch: 1084
Batch: 1085
Batch: 1086
Batch: 1087
Batch: 1088
Batch: 1089
Batch: 1090
Batch: 1091
Batch: 1092
Batch: 1093
Batch: 1094
Batch: 1095
Batch: 1096
Batch: 1097
Batch: 1098
Batch: 1099
Batch: 1100
Batch: 1101
Batch: 1102
Batch: 1103
Batch: 1104
Batch: 1105
Batch: 1106
Batch: 1107
Batch: 1108
Batch: 1109
Batch: 1110
Batch: 1111
Batch: 1112
Batch: 1113
Batch: 1114
Batch: 1115
Batch: 1116
Batch: 1117
Batch: 1118
Batch: 1119
Batch: 1120
Batch: 1121
Batch: 1122
Batch: 1123
Batch: 1124
Batch: 1125
Batch: 1126
Batch: 1127
Batch: 1128
Batch: 1129
Batch: 1130
Batch: 1131
Batch: 1132
Batch: 1133
Batch: 1134
Batch: 1135
Batch: 1136
Batch: 1137
Batch: 1138
Batch: 1139
Batch: 1140
Batch: 1141
Batch: 1142
Batch: 1143
Batch: 1144
Batch: 1145
Batch: 1146
Batch: 1147
Batch: 1148
Batch: 1149
Batch: 1150
Batch: 1151
Batch: 1152
Batch: 1153
Batch: 1154
Batch: 1155
Batch: 1156
Batch: 1157
Batch: 1158
Batch: 1159
Batch: 1160
Batch: 1161
Batch: 1162
Batch: 1163
Batch: 1164
Batch: 1165
Batch: 1166
Batch: 1167
Batch: 1168
Batch: 1169
Batch: 1170
Batch: 1171
Batch: 1172
Batch: 1173
Batch: 1174
Batch: 1175
Batch: 1176
Batch: 1177
Batch: 1178
Batch: 1179
Batch: 1180
Batch: 1181
Batch: 1182
Batch: 1183
Batch: 1184
Batch: 1185
Batch: 1186
Batch: 1187
Batch: 1188
Batch: 1189
Batch: 1190
Batch: 1191
Batch: 1192
Batch: 1193
Batch: 1194
Batch: 1195
Batch: 1196
Batch: 1197
Batch: 1198
Batch: 1199
Batch: 1200
Batch: 1201
Batch: 1202
Batch: 1203
Batch: 1204
Batch: 1205
Batch: 1206
Batch: 1207
Batch: 1208
Batch: 1209
Batch: 1210
Batch: 1211
Batch: 1212
Batch: 1213
Batch: 1214
Batch: 1215
Batch: 1216
Batch: 1217
Batch: 1218
Batch: 1219
Batch: 1220
Batch: 1221
Batch: 1222
Batch: 1223
Batch: 1224
Batch: 1225
Batch: 1226
Batch: 1227
Batch: 1228
Batch: 1229
Batch: 1230
Batch: 1231
Batch: 1232
Batch: 1233
Batch: 1234
Batch: 1235
Batch: 1236
Batch: 1237
Batch: 1238
Batch: 1239
Batch: 1240
Batch: 1241
Batch: 1242
Batch: 1243
Batch: 1244
Batch: 1245
Batch: 1246
Batch: 1247
Batch: 1248
Batch: 1249
Batch: 1250
Batch: 1251
Batch: 1252
Batch: 1253
Batch: 1254
Batch: 1255
Batch: 1256
Batch: 1257
Batch: 1258
Batch: 1259
Batch: 1260
Batch: 1261
Batch: 1262
Batch: 1263
Batch: 1264
Batch: 1265
Batch: 1266
Batch: 1267
Batch: 1268
Batch: 1269
Batch: 1270
Batch: 1271
Batch: 1272
Batch: 1273
Batch: 1274
Batch: 1275
Batch: 1276
Batch: 1277
Batch: 1278
Batch: 1279
Batch: 1280
Batch: 1281
Batch: 1282
Batch: 1283
Batch: 1284
Batch: 1285
Batch: 1286
Batch: 1287
Batch: 1288
Batch: 1289
Batch: 1290
Batch: 1291
Batch: 1292
Batch: 1293
Batch: 1294
Batch: 1295
Batch: 1296
Batch: 1297
Batch: 1298
Batch: 1299
Batch: 1300
Batch: 1301
Batch: 1302
Batch: 1303
Batch: 1304
Batch: 1305
Batch: 1306
Batch: 1307
Batch: 1308
Batch: 1309
Batch: 1310
Batch: 1311
Batch: 1312
Batch: 1313
Batch: 1314
Batch: 1315
Batch: 1316
Batch: 1317
Batch: 1318
Batch: 1319
Batch: 1320
Batch: 1321
Batch: 1322
Batch: 1323
Batch: 1324
Batch: 1325
Batch: 1326
Batch: 1327
Batch: 1328
Batch: 1329
Batch: 1330
Batch: 1331
Batch: 1332
Batch: 1333
Batch: 1334
Batch: 1335
Batch: 1336
Batch: 1337
Batch: 1338
Batch: 1339
Batch: 1340
Batch: 1341
Batch: 1342
Batch: 1343
Batch: 1344
Batch: 1345
Batch: 1346
Batch: 1347
Batch: 1348
Batch: 1349
Batch: 1350
Batch: 1351
Batch: 1352
Batch: 1353
Batch: 1354
Batch: 1355
Batch: 1356
Batch: 1357
Batch: 1358
Batch: 1359
Batch: 1360
Batch: 1361
Batch: 1362
Batch: 1363
Batch: 1364
Batch: 1365
Batch: 1366
Batch: 1367
Batch: 1368
Batch: 1369
Batch: 1370
Batch: 1371
Batch: 1372
Batch: 1373
Batch: 1374
Batch: 1375
Batch: 1376
Batch: 1377
Batch: 1378
Batch: 1379
Batch: 1380
Batch: 1381
Batch: 1382
Batch: 1383
Batch: 1384
Batch: 1385
Batch: 1386
Batch: 1387
Batch: 1388
Batch: 1389
Batch: 1390
Batch: 1391
Batch: 1392
Batch: 1393
Batch: 1394
Batch: 1395
Batch: 1396
Batch: 1397
Batch: 1398
Batch: 1399
Batch: 1400
Batch: 1401
Batch: 1402
Batch: 1403
Batch: 1404
Batch: 1405
Batch: 1406
Batch: 1407
Batch: 1408
Batch: 1409
Batch: 1410
Batch: 1411
Batch: 1412
Batch: 1413
Batch: 1414
Batch: 1415
Batch: 1416
Batch: 1417
Batch: 1418
Batch: 1419
Batch: 1420
Batch: 1421
Batch: 1422
Batch: 1423
Batch: 1424
Batch: 1425
Batch: 1426
Batch: 1427
Batch: 1428
Batch: 1429
Batch: 1430
Batch: 1431
Batch: 1432
Batch: 1433
Batch: 1434
Batch: 1435
Batch: 1436
Batch: 1437
Batch: 1438
Batch: 1439
Batch: 1440
Batch: 1441
Batch: 1442
Batch: 1443
Batch: 1444
Batch: 1445
Batch: 1446
Batch: 1447
Batch: 1448
Batch: 1449
Batch: 1450
Batch: 1451
Batch: 1452
Batch: 1453
Batch: 1454
Batch: 1455
Batch: 1456
Batch: 1457
Batch: 1458
Batch: 1459
Batch: 1460
Batch: 1461
Batch: 1462
Batch: 1463
Batch: 1464
Batch: 1465
Batch: 1466
Batch: 1467
Batch: 1468
Batch: 1469
Batch: 1470
Batch: 1471
Batch: 1472
Batch: 1473
Batch: 1474
Batch: 1475
Batch: 1476
Batch: 1477
Batch: 1478
Batch: 1479
Batch: 1480
Batch: 1481
Batch: 1482
Batch: 1483
Batch: 1484
Batch: 1485
Batch: 1486
Batch: 1487
Batch: 1488
Batch: 1489
Batch: 1490
Batch: 1491
Batch: 1492
Batch: 1493
Batch: 1494
Batch: 1495
Batch: 1496
Batch: 1497
Batch: 1498
Batch: 1499
Batch: 1500
Batch: 1501
Batch: 1502
Batch: 1503
Batch: 1504
Batch: 1505
Batch: 1506
Batch: 1507
Batch: 1508
Batch: 1509
Batch: 1510
Batch: 1511
Batch: 1512
Batch: 1513
Batch: 1514
Batch: 1515
Batch: 1516
Batch: 1517
Batch: 1518
Batch: 1519
Batch: 1520
Batch: 1521
Batch: 1522
Batch: 1523
Batch: 1524
Batch: 1525
Batch: 1526
Batch: 1527
Batch: 1528
Batch: 1529
Batch: 1530
Batch: 1531
Batch: 1532
Batch: 1533
Batch: 1534
Batch: 1535
Batch: 1536
Batch: 1537
Batch: 1538
Batch: 1539
Batch: 1540
Batch: 1541
Batch: 1542
Batch: 1543
Batch: 1544
Batch: 1545
Batch: 1546
Batch: 1547
Batch: 1548
Batch: 1549
Batch: 1550
Batch: 1551
Batch: 1552
Batch: 1553
Batch: 1554
Batch: 1555
Batch: 1556
Batch: 1557
Batch: 1558
Batch: 1559
Batch: 1560
Batch: 1561
Batch: 1562
Batch: 1563
Batch: 1564
Batch: 1565
Batch: 1566
Batch: 1567
Batch: 1568
Batch: 1569
Batch: 1570
Batch: 1571
Batch: 1572
Batch: 1573
Batch: 1574
Batch: 1575
Batch: 1576
Batch: 1577
Batch: 1578
Batch: 1579
Batch: 1580
Batch: 1581
Batch: 1582
Batch: 1583
Batch: 1584
Batch: 1585
Batch: 1586
Batch: 1587
Batch: 1588
Batch: 1589
Batch: 1590
Batch: 1591
Batch: 1592
Batch: 1593
Batch: 1594
Batch: 1595
Batch: 1596
Batch: 1597
Batch: 1598
Batch: 1599
Batch: 1600
Batch: 1601
Batch: 1602
Batch: 1603
Batch: 1604
Batch: 1605
Batch: 1606
Batch: 1607
Batch: 1608
Batch: 1609
Batch: 1610
Batch: 1611
Batch: 1612
Batch: 1613
Batch: 1614
Batch: 1615
Batch: 1616
Batch: 1617
Batch: 1618
Batch: 1619
Batch: 1620
Batch: 1621
Batch: 1622
Batch: 1623
Batch: 1624
Batch: 1625
Batch: 1626
Batch: 1627
Batch: 1628
Batch: 1629
Batch: 1630
Batch: 1631
Batch: 1632
Batch: 1633
Batch: 1634
Batch: 1635
Batch: 1636
Batch: 1637
Batch: 1638
Batch: 1639
Batch: 1640
Batch: 1641
Batch: 1642
Batch: 1643
Batch: 1644
Batch: 1645
Batch: 1646
Batch: 1647
Batch: 1648
Batch: 1649
Batch: 1650
Batch: 1651
Batch: 1652
Batch: 1653
Batch: 1654
Batch: 1655
Batch: 1656
Batch: 1657
Batch: 1658
Batch: 1659
Batch: 1660
Batch: 1661
Batch: 1662
Batch: 1663
Batch: 1664
Batch: 1665
Batch: 1666
Batch: 1667
Batch: 1668
Batch: 1669
Batch: 1670
Batch: 1671
Batch: 1672
Batch: 1673
Batch: 1674
Batch: 1675
Batch: 1676
Batch: 1677
Batch: 1678
Batch: 1679
Batch: 1680
Batch: 1681
Batch: 1682
Batch: 1683
Batch: 1684
Batch: 1685
Batch: 1686
Batch: 1687
Batch: 1688
Batch: 1689
Batch: 1690
Batch: 1691
Batch: 1692
Batch: 1693
Batch: 1694
Batch: 1695
Batch: 1696
Batch: 1697
Batch: 1698
Batch: 1699
Batch: 1700
Batch: 1701
Batch: 1702
Batch: 1703
Batch: 1704
Batch: 1705
Batch: 1706
Batch: 1707
Batch: 1708
Batch: 1709
Batch: 1710
Batch: 1711
Batch: 1712
Batch: 1713
Batch: 1714
Batch: 1715
Batch: 1716
Batch: 1717
Batch: 1718
Batch: 1719
Batch: 1720
Batch: 1721
Batch: 1722
Batch: 1723
Batch: 1724
Batch: 1725
Batch: 1726
Batch: 1727
Batch: 1728
Batch: 1729
Batch: 1730
Batch: 1731
Batch: 1732
Batch: 1733
Batch: 1734
Batch: 1735
Batch: 1736
Batch: 1737
Batch: 1738
Batch: 1739
Batch: 1740
Batch: 1741
Batch: 1742
Batch: 1743
Batch: 1744
Batch: 1745
Batch: 1746
Batch: 1747
Batch: 1748
Batch: 1749
Batch: 1750
Batch: 1751
Batch: 1752
Batch: 1753
Batch: 1754
Batch: 1755
Batch: 1756
Batch: 1757
Batch: 1758
Batch: 1759
Batch: 1760
Batch: 1761
Batch: 1762
Batch: 1763
Batch: 1764
Batch: 1765
Batch: 1766
Batch: 1767
Batch: 1768
Batch: 1769
Batch: 1770
Batch: 1771
Batch: 1772
Batch: 1773
Batch: 1774
Batch: 1775
Batch: 1776
Batch: 1777
Batch: 1778
Batch: 1779
Batch: 1780
Batch: 1781
Batch: 1782
Batch: 1783
Batch: 1784
Batch: 1785
Batch: 1786
Batch: 1787
Batch: 1788
Batch: 1789
Batch: 1790
Batch: 1791
Batch: 1792
Batch: 1793
Batch: 1794
Batch: 1795
Batch: 1796
Batch: 1797
Batch: 1798
Batch: 1799
Batch: 1800
Batch: 1801
Batch: 1802
Batch: 1803
Batch: 1804
Batch: 1805
Batch: 1806
Batch: 1807
Batch: 1808
Batch: 1809
Batch: 1810
Batch: 1811
Batch: 1812
Batch: 1813
Batch: 1814
Batch: 1815
Batch: 1816
Batch: 1817
Batch: 1818
Batch: 1819
Batch: 1820
Batch: 1821
Batch: 1822
Batch: 1823
Batch: 1824
Batch: 1825
Batch: 1826
Batch: 1827
Batch: 1828
Batch: 1829
Batch: 1830
Batch: 1831
Batch: 1832
Batch: 1833
Batch: 1834
Batch: 1835
Batch: 1836
Batch: 1837
Batch: 1838
Batch: 1839
Batch: 1840
Batch: 1841
Batch: 1842
Batch: 1843
Batch: 1844
Batch: 1845
Batch: 1846
Batch: 1847
Batch: 1848
Batch: 1849
Batch: 1850
Batch: 1851
Batch: 1852
Batch: 1853
Batch: 1854
Batch: 1855
Batch: 1856
Batch: 1857
Batch: 1858
Batch: 1859
Batch: 1860
Batch: 1861
Batch: 1862
Batch: 1863
Batch: 1864
Batch: 1865
Batch: 1866
Batch: 1867
Batch: 1868
Batch: 1869
Batch: 1870
Batch: 1871
Batch: 1872
Batch: 1873
Batch: 1874
Batch: 1875
Batch: 1876
Batch: 1877
Batch: 1878
Batch: 1879
Batch: 1880
Batch: 1881
Batch: 1882
Batch: 1883
Batch: 1884
Batch: 1885
Batch: 1886
Batch: 1887
Batch: 1888
Batch: 1889
Batch: 1890
Batch: 1891
Batch: 1892
Batch: 1893
Batch: 1894
Batch: 1895
Batch: 1896
Batch: 1897
Batch: 1898
Batch: 1899
Batch: 1900
Batch: 1901
Batch: 1902
Batch: 1903
Batch: 1904
Batch: 1905
Batch: 1906
Batch: 1907
Batch: 1908
Batch: 1909
Batch: 1910
Batch: 1911
Batch: 1912
Batch: 1913
Batch: 1914
Batch: 1915
Batch: 1916
Batch: 1917
Batch: 1918
Batch: 1919
Batch: 1920
Batch: 1921
Batch: 1922
Batch: 1923
Batch: 1924
Batch: 1925
Batch: 1926
Batch: 1927
Batch: 1928
Batch: 1929
Batch: 1930
Batch: 1931
Batch: 1932
Batch: 1933
Batch: 1934
Batch: 1935
Batch: 1936
Batch: 1937
Batch: 1938
Batch: 1939
Batch: 1940
Batch: 1941
Batch: 1942
Batch: 1943
Batch: 1944
Batch: 1945
Batch: 1946
Batch: 1947
Batch: 1948
Batch: 1949
Batch: 1950
Batch: 1951
Batch: 1952
Batch: 1953
Batch: 1954
Batch: 1955
Batch: 1956
Batch: 1957
Batch: 1958
Batch: 1959
Batch: 1960
Batch: 1961
Batch: 1962
Batch: 1963
Batch: 1964
Batch: 1965
Batch: 1966
Batch: 1967
Batch: 1968
Batch: 1969
Batch: 1970
Batch: 1971
Batch: 1972
Batch: 1973
Batch: 1974
Batch: 1975
Batch: 1976
Batch: 1977
Batch: 1978
Batch: 1979
Batch: 1980
Batch: 1981
Batch: 1982
Batch: 1983
Batch: 1984
Batch: 1985
Batch: 1986
Batch: 1987
Batch: 1988
Batch: 1989
Batch: 1990
Batch: 1991
Batch: 1992
Batch: 1993
Batch: 1994
Batch: 1995
Batch: 1996
Batch: 1997
Batch: 1998
Batch: 1999
Batch: 2000
Batch: 2001
Batch: 2002
Batch: 2003
Batch: 2004
Batch: 2005
Batch: 2006
Batch: 2007
Batch: 2008
Batch: 2009
Batch: 2010
Batch: 2011
Batch: 2012
Batch: 2013
Batch: 2014
Batch: 2015
Batch: 2016
Batch: 2017
Batch: 2018
Batch: 2019
Batch: 2020
Batch: 2021
Batch: 2022
Batch: 2023
Batch: 2024
Batch: 2025
Batch: 2026
Batch: 2027
Batch: 2028
Batch: 2029
Batch: 2030
Batch: 2031
Batch: 2032
Batch: 2033
Batch: 2034
Batch: 2035
Batch: 2036
Batch: 2037
Batch: 2038
Batch: 2039
Batch: 2040
Batch: 2041
Batch: 2042
Batch: 2043
Batch: 2044
Batch: 2045
Batch: 2046
Batch: 2047
Batch: 2048
Batch: 2049
Batch: 2050
Batch: 2051
Batch: 2052
Batch: 2053
Batch: 2054
Batch: 2055
Batch: 2056
Batch: 2057
Batch: 2058
Batch: 2059
Batch: 2060
Batch: 2061
Batch: 2062
Batch: 2063
Batch: 2064
Batch: 2065
Batch: 2066
Batch: 2067
Batch: 2068
Batch: 2069
Batch: 2070
Batch: 2071
Batch: 2072
Batch: 2073
Batch: 2074
Batch: 2075
Batch: 2076
Batch: 2077
Batch: 2078
Batch: 2079
Batch: 2080
Batch: 2081
Batch: 2082
Batch: 2083
Batch: 2084
Batch: 2085
Batch: 2086
Batch: 2087
Batch: 2088
Batch: 2089
Batch: 2090
Batch: 2091
Batch: 2092
Batch: 2093
Batch: 2094
Batch: 2095
Batch: 2096
Batch: 2097
Batch: 2098
Batch: 2099
Batch: 2100
Batch: 2101
Batch: 2102
Batch: 2103
Batch: 2104
Batch: 2105
Batch: 2106
Batch: 2107
Batch: 2108
Batch: 2109
Batch: 2110
Batch: 2111
Batch: 2112
Batch: 2113
Batch: 2114
Batch: 2115
Batch: 2116
Batch: 2117
Batch: 2118
Batch: 2119
Batch: 2120
Batch: 2121
Batch: 2122
Batch: 2123
Batch: 2124
Batch: 2125
Batch: 2126
Batch: 2127
Batch: 2128
Batch: 2129
Batch: 2130
Batch: 2131
Batch: 2132
Batch: 2133
Batch: 2134
Batch: 2135
Batch: 2136
Batch: 2137
Batch: 2138
Batch: 2139
Batch: 2140
Batch: 2141
Batch: 2142
Batch: 2143
Batch: 2144
Batch: 2145
Batch: 2146
Batch: 2147
Batch: 2148
Batch: 2149
Batch: 2150
Batch: 2151
Batch: 2152
Batch: 2153
Batch: 2154
Batch: 2155
Batch: 2156
Batch: 2157
Batch: 2158
Batch: 2159
Batch: 2160
Batch: 2161
Batch: 2162
Batch: 2163
Batch: 2164
Batch: 2165
Batch: 2166
Batch: 2167
Batch: 2168
Batch: 2169
Batch: 2170
Batch: 2171
Batch: 2172
Batch: 2173
Batch: 2174
Batch: 2175
Batch: 2176
Batch: 2177
Batch: 2178
Batch: 2179
Batch: 2180
Batch: 2181
Batch: 2182
Batch: 2183
Batch: 2184
Batch: 2185
Batch: 2186
Batch: 2187
Batch: 2188
Batch: 2189
Batch: 2190
Batch: 2191
Batch: 2192
Batch: 2193
Batch: 2194
Batch: 2195
Batch: 2196
Batch: 2197
Batch: 2198
Batch: 2199
Batch: 2200
Batch: 2201
Batch: 2202
Batch: 2203
Batch: 2204
Batch: 2205
Batch: 2206
Batch: 2207
Batch: 2208
Batch: 2209
Batch: 2210
Batch: 2211
Batch: 2212
Batch: 2213
Batch: 2214
Batch: 2215
Batch: 2216
Batch: 2217
Batch: 2218
Batch: 2219
Batch: 2220
Batch: 2221
Batch: 2222
Batch: 2223
Batch: 2224
Batch: 2225
Batch: 2226
Batch: 2227
Batch: 2228
Batch: 2229
Batch: 2230
Batch: 2231
Batch: 2232
Batch: 2233
Batch: 2234
Batch: 2235
Batch: 2236
Batch: 2237
Batch: 2238
Batch: 2239
Batch: 2240
Batch: 2241
Batch: 2242
Batch: 2243
Batch: 2244
Batch: 2245
Batch: 2246
Batch: 2247
Batch: 2248
Batch: 2249
Batch: 2250
Batch: 2251
Batch: 2252
Batch: 2253
Batch: 2254
Batch: 2255
Batch: 2256
Batch: 2257
Batch: 2258
Batch: 2259
Batch: 2260
Batch: 2261
Batch: 2262
Batch: 2263
Batch: 2264
Batch: 2265
Batch: 2266
Batch: 2267
Batch: 2268
Batch: 2269
Batch: 2270
Batch: 2271
Batch: 2272
Batch: 2273
Batch: 2274
Batch: 2275
Batch: 2276
Batch: 2277
Batch: 2278
Batch: 2279
Batch: 2280
Batch: 2281
Batch: 2282
Batch: 2283
Batch: 2284
Batch: 2285
Batch: 2286
Batch: 2287
Batch: 2288
Batch: 2289
Batch: 2290
Batch: 2291
Batch: 2292
Batch: 2293
Batch: 2294
Batch: 2295
Batch: 2296
Batch: 2297
Batch: 2298
Batch: 2299
Batch: 2300
Batch: 2301
Batch: 2302
Batch: 2303
Batch: 2304
Batch: 2305
Batch: 2306
Batch: 2307
Batch: 2308
Batch: 2309
Batch: 2310
Batch: 2311
Batch: 2312
Batch: 2313
Batch: 2314
Batch: 2315
Batch: 2316
Batch: 2317
Batch: 2318
Batch: 2319
Batch: 2320
Batch: 2321
Batch: 2322
Batch: 2323
Batch: 2324
Batch: 2325
Batch: 2326
Batch: 2327
Batch: 2328
Batch: 2329
Batch: 2330
Batch: 2331
Batch: 2332
Batch: 2333
Batch: 2334
Batch: 2335
Batch: 2336
Batch: 2337
Batch: 2338
Batch: 2339
Batch: 2340
Batch: 2341
Batch: 2342
Batch: 2343
Batch: 2344
Batch: 2345
Batch: 2346
Batch: 2347
Batch: 2348
Batch: 2349
Batch: 2350
Batch: 2351
Batch: 2352
Batch: 2353
Batch: 2354
Batch: 2355
Batch: 2356
Batch: 2357
Batch: 2358
Batch: 2359
Batch: 2360
Batch: 2361
Batch: 2362
Batch: 2363
Batch: 2364
Batch: 2365
Batch: 2366
Batch: 2367
Batch: 2368
Batch: 2369
Batch: 2370
Batch: 2371
Batch: 2372
Batch: 2373
Batch: 2374
Batch: 2375
Batch: 2376
Batch: 2377
Batch: 2378
Batch: 2379
Batch: 2380
Batch: 2381
Batch: 2382
Batch: 2383
Batch: 2384
Batch: 2385
Batch: 2386
Batch: 2387
Batch: 2388
Batch: 2389
Batch: 2390
Batch: 2391
Batch: 2392
Batch: 2393
Batch: 2394
Batch: 2395
Batch: 2396
Batch: 2397
Batch: 2398
Batch: 2399
Batch: 2400
Batch: 2401
Batch: 2402
Batch: 2403
Batch: 2404
Batch: 2405
Batch: 2406
Batch: 2407
Batch: 2408
Batch: 2409
Batch: 2410
Batch: 2411
Batch: 2412
Batch: 2413
Batch: 2414
Batch: 2415
Batch: 2416
Batch: 2417
Batch: 2418
Batch: 2419
Batch: 2420
Batch: 2421
Batch: 2422
Batch: 2423
Batch: 2424
Batch: 2425
Batch: 2426
Batch: 2427
Batch: 2428
Batch: 2429
Batch: 2430
Batch: 2431
Batch: 2432
Batch: 2433
Batch: 2434
Batch: 2435
Batch: 2436
Batch: 2437
Batch: 2438
Batch: 2439
Batch: 2440
Batch: 2441
Batch: 2442
Batch: 2443
Batch: 2444
Batch: 2445
Batch: 2446
Batch: 2447
Batch: 2448
Batch: 2449
Batch: 2450
Batch: 2451
Batch: 2452
Batch: 2453
Batch: 2454
Batch: 2455
Batch: 2456
Batch: 2457
Batch: 2458
Batch: 2459
Batch: 2460
Batch: 2461
Batch: 2462
Batch: 2463
Batch: 2464
Batch: 2465
Batch: 2466
Batch: 2467
Batch: 2468
Batch: 2469
Batch: 2470
Batch: 2471
Batch: 2472
Batch: 2473
Batch: 2474
Batch: 2475
Batch: 2476
Batch: 2477
Batch: 2478
Batch: 2479
Batch: 2480
Batch: 2481
Batch: 2482
Batch: 2483
Batch: 2484
Batch: 2485
Batch: 2486
Batch: 2487
Batch: 2488
Batch: 2489
Batch: 2490
Batch: 2491
Batch: 2492
Batch: 2493
Batch: 2494
Batch: 2495
Batch: 2496
Batch: 2497
Batch: 2498
Batch: 2499
Batch: 2500
Batch: 2501
Batch: 2502
Batch: 2503
Batch: 2504
Batch: 2505
Batch: 2506
Batch: 2507
Batch: 2508
Batch: 2509
Batch: 2510
Batch: 2511
Batch: 2512
Batch: 2513
Batch: 2514
Batch: 2515
Batch: 2516
Batch: 2517
Batch: 2518
Batch: 2519
Batch: 2520
Batch: 2521
Batch: 2522
Batch: 2523
Batch: 2524
Batch: 2525
Batch: 2526
Batch: 2527
Batch: 2528
Batch: 2529
Batch: 2530
Batch: 2531
Batch: 2532
Batch: 2533
Batch: 2534
Batch: 2535
Batch: 2536
Batch: 2537
Batch: 2538
Batch: 2539
Batch: 2540
Batch: 2541
Batch: 2542
Batch: 2543
Batch: 2544
Batch: 2545
Batch: 2546
Batch: 2547
Batch: 2548
Batch: 2549
Batch: 2550
Batch: 2551
Batch: 2552
Batch: 2553
Batch: 2554
Batch: 2555
Batch: 2556
Batch: 2557
Batch: 2558
Batch: 2559
Batch: 2560
Batch: 2561
Batch: 2562
Batch: 2563
Batch: 2564
Batch: 2565
Batch: 2566
Batch: 2567
Batch: 2568
Batch: 2569
Batch: 2570
Batch: 2571
Batch: 2572
Batch: 2573
Batch: 2574
Batch: 2575
Batch: 2576
Batch: 2577
Batch: 2578
Batch: 2579
Batch: 2580
Batch: 2581
Batch: 2582
Batch: 2583
Batch: 2584
Batch: 2585
Batch: 2586
Batch: 2587
Batch: 2588
Batch: 2589
Batch: 2590
Batch: 2591
Batch: 2592
Batch: 2593
Batch: 2594
Batch: 2595
Batch: 2596
Batch: 2597
Batch: 2598
Batch: 2599
Batch: 2600
Batch: 2601
Batch: 2602
Batch: 2603
Batch: 2604
Batch: 2605
Batch: 2606
Batch: 2607
Batch: 2608
Batch: 2609
Batch: 2610
Batch: 2611
Batch: 2612
Batch: 2613
Batch: 2614
Batch: 2615
Batch: 2616
Batch: 2617
Batch: 2618
Batch: 2619
Batch: 2620
Batch: 2621
Batch: 2622
Batch: 2623
Batch: 2624
Batch: 2625
Batch: 2626
Batch: 2627
Batch: 2628
Batch: 2629
Batch: 2630
Batch: 2631
Batch: 2632
Batch: 2633
Batch: 2634
Batch: 2635
Batch: 2636
Batch: 2637
Batch: 2638
Batch: 2639
Batch: 2640
Batch: 2641
Batch: 2642
Batch: 2643
Batch: 2644
Batch: 2645
Batch: 2646
Batch: 2647
Batch: 2648
Batch: 2649
Batch: 2650
Batch: 2651
Batch: 2652
Batch: 2653
Batch: 2654
Batch: 2655
Batch: 2656
Batch: 2657
Batch: 2658
Batch: 2659
Batch: 2660
Batch: 2661
Batch: 2662
Batch: 2663
Batch: 2664
Batch: 2665
Batch: 2666
Batch: 2667
Batch: 2668
Batch: 2669
Batch: 2670
Batch: 2671
Batch: 2672
Batch: 2673
Batch: 2674
Batch: 2675
Batch: 2676
Batch: 2677
Batch: 2678
Batch: 2679
Batch: 2680
Batch: 2681
Batch: 2682
Batch: 2683
Batch: 2684
Batch: 2685
Batch: 2686
Batch: 2687
Batch: 2688
Batch: 2689
Batch: 2690
Batch: 2691
Batch: 2692
Batch: 2693
Batch: 2694
Batch: 2695
Batch: 2696
Batch: 2697
Batch: 2698
Batch: 2699
Batch: 2700
Batch: 2701
Batch: 2702
Batch: 2703
Batch: 2704
Batch: 2705
Batch: 2706
Batch: 2707
Batch: 2708
Batch: 2709
Batch: 2710
Batch: 2711
Batch: 2712
Batch: 2713
Batch: 2714
Batch: 2715
Batch: 2716
Batch: 2717
Batch: 2718
Batch: 2719
Batch: 2720
Batch: 2721
Batch: 2722
Batch: 2723
Batch: 2724
Batch: 2725
Batch: 2726
Batch: 2727
Batch: 2728
Batch: 2729
Batch: 2730
Batch: 2731
Batch: 2732
Batch: 2733
Batch: 2734
Batch: 2735
Batch: 2736
Batch: 2737
Batch: 2738
Batch: 2739
Batch: 2740
Batch: 2741
Batch: 2742
Batch: 2743
Batch: 2744
Batch: 2745
Batch: 2746
Batch: 2747
Batch: 2748
Batch: 2749
Batch: 2750
Batch: 2751
Batch: 2752
Batch: 2753
Batch: 2754
Batch: 2755
Batch: 2756
Batch: 2757
Batch: 2758
Batch: 2759
Batch: 2760
Batch: 2761
Batch: 2762
Batch: 2763
Batch: 2764
Batch: 2765
Batch: 2766
Batch: 2767
Batch: 2768
Batch: 2769
Batch: 2770
Batch: 2771
Batch: 2772
Batch: 2773
Batch: 2774
Batch: 2775
Batch: 2776
Batch: 2777
Batch: 2778
Batch: 2779
Batch: 2780
Batch: 2781
Batch: 2782
Batch: 2783
Batch: 2784
Batch: 2785
Batch: 2786
Batch: 2787
Batch: 2788
Batch: 2789
Batch: 2790
Batch: 2791
Batch: 2792
Batch: 2793
Batch: 2794
Batch: 2795
Batch: 2796
Batch: 2797
Batch: 2798
Batch: 2799
Batch: 2800
Batch: 2801
Batch: 2802
Batch: 2803
Batch: 2804
Batch: 2805
Batch: 2806
Batch: 2807
Batch: 2808
Batch: 2809
Batch: 2810
Batch: 2811
Batch: 2812
Batch: 2813
Batch: 2814
Batch: 2815
Batch: 2816
Batch: 2817
Batch: 2818
Batch: 2819
Batch: 2820
Batch: 2821
Batch: 2822
Batch: 2823
Batch: 2824
Batch: 2825
Batch: 2826
Batch: 2827
Batch: 2828
Batch: 2829
Batch: 2830
Batch: 2831
Batch: 2832
Batch: 2833
Batch: 2834
Batch: 2835
Batch: 2836
Batch: 2837
Batch: 2838
Batch: 2839
Batch: 2840
Batch: 2841
Batch: 2842
Batch: 2843
Batch: 2844
Batch: 2845
Batch: 2846
Batch: 2847
Batch: 2848
Batch: 2849
Batch: 2850
Batch: 2851
Batch: 2852
Batch: 2853
Batch: 2854
Batch: 2855
Batch: 2856
Batch: 2857
Batch: 2858
Batch: 2859
Batch: 2860
Batch: 2861
Batch: 2862
Batch: 2863
Batch: 2864
Batch: 2865
Batch: 2866
Batch: 2867
Batch: 2868
Batch: 2869
Batch: 2870
Batch: 2871
Batch: 2872
Batch: 2873
Batch: 2874
Batch: 2875
Batch: 2876
Batch: 2877
Batch: 2878
Batch: 2879
Batch: 2880
Batch: 2881
Batch: 2882
Batch: 2883
Batch: 2884
Batch: 2885
Batch: 2886
Batch: 2887
Batch: 2888
Batch: 2889
Batch: 2890
Batch: 2891
Batch: 2892
Batch: 2893
Batch: 2894
Batch: 2895
Batch: 2896
Batch: 2897
Batch: 2898
Batch: 2899
Batch: 2900
Batch: 2901
Batch: 2902
Batch: 2903
Batch: 2904
Batch: 2905
Batch: 2906
Batch: 2907
Batch: 2908
Batch: 2909
Batch: 2910
Batch: 2911
Batch: 2912
Batch: 2913
Batch: 2914
Batch: 2915
Batch: 2916
Batch: 2917
Batch: 2918
Batch: 2919
Batch: 2920
Batch: 2921
Batch: 2922
Batch: 2923
Batch: 2924
Batch: 2925
Batch: 2926
Batch: 2927
Batch: 2928
Batch: 2929
Batch: 2930
Batch: 2931
Batch: 2932
Batch: 2933
Batch: 2934
Batch: 2935
Batch: 2936
Batch: 2937
Batch: 2938
Batch: 2939
Batch: 2940
Batch: 2941
Batch: 2942
Batch: 2943
Batch: 2944
Batch: 2945
Batch: 2946
Batch: 2947
Batch: 2948
Batch: 2949
Batch: 2950
Batch: 2951
Batch: 2952
Batch: 2953
Batch: 2954
Batch: 2955
Batch: 2956
Batch: 2957
Batch: 2958
Batch: 2959
Batch: 2960
Batch: 2961
Batch: 2962
Batch: 2963
Batch: 2964
Batch: 2965
Batch: 2966
Batch: 2967
Batch: 2968
Batch: 2969
Batch: 2970
Batch: 2971
Batch: 2972
Batch: 2973
Batch: 2974
Batch: 2975
Batch: 2976
Batch: 2977
Batch: 2978
Batch: 2979
Batch: 2980
Batch: 2981
Batch: 2982
Batch: 2983
Batch: 2984
Batch: 2985
Batch: 2986
Batch: 2987
Batch: 2988
Batch: 2989
Batch: 2990
Batch: 2991
Batch: 2992
Batch: 2993
Batch: 2994
Batch: 2995
Batch: 2996
Batch: 2997
Batch: 2998
Batch: 2999
Batch: 3000
Batch: 3001
Batch: 3002
Batch: 3003
Batch: 3004
Batch: 3005
Batch: 3006
Batch: 3007
Batch: 3008
Batch: 3009
Batch: 3010
Batch: 3011
Batch: 3012
Batch: 3013
Batch: 3014
Batch: 3015
Batch: 3016
Batch: 3017
Batch: 3018
Batch: 3019
Batch: 3020
Batch: 3021
Batch: 3022
Batch: 3023
Batch: 3024
Batch: 3025
Batch: 3026
Batch: 3027
Batch: 3028
Batch: 3029
Batch: 3030
Batch: 3031
Batch: 3032
Batch: 3033
Batch: 3034
Batch: 3035
Batch: 3036
Batch: 3037
Batch: 3038
Batch: 3039
Batch: 3040
Batch: 3041
Batch: 3042
Batch: 3043
Batch: 3044
Batch: 3045
Batch: 3046
Batch: 3047
Batch: 3048
Batch: 3049
Batch: 3050
Batch: 3051
Batch: 3052
Batch: 3053
Batch: 3054
Batch: 3055
Batch: 3056
Batch: 3057
Batch: 3058
Batch: 3059
Batch: 3060
Batch: 3061
Batch: 3062
Batch: 3063
Batch: 3064
Batch: 3065
Batch: 3066
Batch: 3067
Batch: 3068
Batch: 3069
Batch: 3070
Batch: 3071
Batch: 3072
Batch: 3073
Batch: 3074
Batch: 3075
Batch: 3076
Batch: 3077
Batch: 3078
Batch: 3079
Batch: 3080
Batch: 3081
Batch: 3082
Batch: 3083
Batch: 3084
Batch: 3085
Batch: 3086
Batch: 3087
Batch: 3088
Batch: 3089
Batch: 3090
Batch: 3091
Batch: 3092
Batch: 3093
Batch: 3094
Batch: 3095
Batch: 3096
Batch: 3097
Batch: 3098
Batch: 3099
Batch: 3100
Batch: 3101
Batch: 3102
Batch: 3103
Batch: 3104
Batch: 3105
Batch: 3106
Batch: 3107
Batch: 3108
Batch: 3109
Batch: 3110
Batch: 3111
Batch: 3112
Batch: 3113
Batch: 3114
Batch: 3115
Batch: 3116
Batch: 3117
Batch: 3118
Batch: 3119
Batch: 3120
Batch: 3121
Batch: 3122
Batch: 3123
Batch: 3124
Batch: 3125
Batch: 3126
Batch: 3127
Batch: 3128
Batch: 3129
Batch: 3130
Batch: 3131
Batch: 3132
Batch: 3133
Batch: 3134
Batch: 3135
Batch: 3136
Batch: 3137
Batch: 3138
Batch: 3139
Batch: 3140
Batch: 3141
Batch: 3142
Batch: 3143
Batch: 3144
Batch: 3145
Batch: 3146
Batch: 3147
Batch: 3148
Batch: 3149
Batch: 3150
Batch: 3151
Batch: 3152
Batch: 3153
Batch: 3154
Batch: 3155
Batch: 3156
Batch: 3157
Batch: 3158
Batch: 3159
Batch: 3160
Batch: 3161
Batch: 3162
Batch: 3163
Batch: 3164
Batch: 3165
Batch: 3166
Batch: 3167
Batch: 3168
Batch: 3169
Batch: 3170
Batch: 3171
Batch: 3172
Batch: 3173
Batch: 3174
Batch: 3175
Batch: 3176
Batch: 3177
Batch: 3178
Batch: 3179
Batch: 3180
Batch: 3181
Batch: 3182
Batch: 3183
Batch: 3184
Batch: 3185
Batch: 3186
Batch: 3187
Batch: 3188
Batch: 3189
Batch: 3190
Batch: 3191
Batch: 3192
Batch: 3193
Batch: 3194
Batch: 3195
Batch: 3196
Batch: 3197
Batch: 3198
Batch: 3199
Batch: 3200
Batch: 3201
Batch: 3202
Batch: 3203
Batch: 3204
Batch: 3205
Batch: 3206
Batch: 3207
Batch: 3208
Batch: 3209
Batch: 3210
Batch: 3211
Batch: 3212
Batch: 3213
Batch: 3214
Batch: 3215
Batch: 3216
Batch: 3217
Batch: 3218
Batch: 3219
Batch: 3220
Batch: 3221
Batch: 3222
Batch: 3223
Batch: 3224
Batch: 3225
Batch: 3226
Batch: 3227
Batch: 3228
Batch: 3229
Batch: 3230
Batch: 3231
Batch: 3232
Batch: 3233
Batch: 3234
Batch: 3235
Batch: 3236
Batch: 3237
Batch: 3238
Batch: 3239
Batch: 3240
Batch: 3241
Batch: 3242
Batch: 3243
Batch: 3244
Batch: 3245
Batch: 3246
Batch: 3247
Batch: 3248
Batch: 3249
Batch: 3250
Batch: 3251
Batch: 3252
Batch: 3253
Batch: 3254
Batch: 3255
Batch: 3256
Batch: 3257
Batch: 3258
Batch: 3259
Batch: 3260
Batch: 3261
Batch: 3262
Batch: 3263
Batch: 3264
Batch: 3265
Batch: 3266
Batch: 3267
Batch: 3268
Batch: 3269
Batch: 3270
Batch: 3271
Batch: 3272
Batch: 3273
Batch: 3274
Batch: 3275
Batch: 3276
Batch: 3277
Batch: 3278
Batch: 3279
Batch: 3280
Batch: 3281
Batch: 3282
Batch: 3283
Batch: 3284
Batch: 3285
Batch: 3286
Batch: 3287
Batch: 3288
Batch: 3289
Batch: 3290
Batch: 3291
Batch: 3292
Batch: 3293
Batch: 3294
Batch: 3295
Batch: 3296
Batch: 3297
Batch: 3298
Batch: 3299
Batch: 3300
Batch: 3301
Batch: 3302
Batch: 3303
Batch: 3304
Batch: 3305
Batch: 3306
Batch: 3307
Batch: 3308
Batch: 3309
Batch: 3310
Batch: 3311
Batch: 3312
Batch: 3313
Batch: 3314
Batch: 3315
Batch: 3316
Batch: 3317
Batch: 3318
Batch: 3319
Batch: 3320
Batch: 3321
Batch: 3322
Batch: 3323
Batch: 3324
Batch: 3325
Batch: 3326
Batch: 3327
Batch: 3328
Batch: 3329
Batch: 3330
Batch: 3331
Batch: 3332
Batch: 3333
Batch: 3334
Batch: 3335
Batch: 3336
Batch: 3337
Batch: 3338
Batch: 3339
Batch: 3340
Batch: 3341
Batch: 3342
Batch: 3343
Batch: 3344
Batch: 3345
Batch: 3346
Batch: 3347
Batch: 3348
Batch: 3349
Batch: 3350
Batch: 3351
Batch: 3352
Batch: 3353
Batch: 3354
Batch: 3355
Batch: 3356
Batch: 3357
Batch: 3358
Batch: 3359
Batch: 3360
Batch: 3361
Batch: 3362
Batch: 3363
Batch: 3364
Batch: 3365
Batch: 3366
Batch: 3367
Batch: 3368
Batch: 3369
Batch: 3370
Batch: 3371
Batch: 3372
Batch: 3373
Batch: 3374
Batch: 3375
Batch: 3376
Batch: 3377
Batch: 3378
Batch: 3379
Batch: 3380
Batch: 3381
Batch: 3382
Batch: 3383
Batch: 3384
Batch: 3385
Batch: 3386
Batch: 3387
Batch: 3388
Batch: 3389
Batch: 3390
Batch: 3391
Batch: 3392
Batch: 3393
Batch: 3394
Batch: 3395
Batch: 3396
Batch: 3397
Batch: 3398
Batch: 3399
Batch: 3400
Batch: 3401
Batch: 3402
Batch: 3403
Batch: 3404
Batch: 3405
Batch: 3406
Batch: 3407
Batch: 3408
Batch: 3409
Batch: 3410
Batch: 3411
Batch: 3412
Batch: 3413
Batch: 3414
Batch: 3415
Batch: 3416
Batch: 3417
Batch: 3418
Batch: 3419
Batch: 3420
Batch: 3421
Batch: 3422
Batch: 3423
Batch: 3424
Batch: 3425
Batch: 3426
Batch: 3427
Batch: 3428
Batch: 3429
Batch: 3430
Batch: 3431
Batch: 3432
Batch: 3433
Batch: 3434
Batch: 3435
Batch: 3436
Batch: 3437
Batch: 3438
Batch: 3439
Batch: 3440
Batch: 3441
Batch: 3442
Batch: 3443
Batch: 3444
Batch: 3445
Batch: 3446
Batch: 3447
Batch: 3448
Batch: 3449
Batch: 3450
Batch: 3451
Batch: 3452
Batch: 3453
Batch: 3454
Batch: 3455
Batch: 3456
Batch: 3457
Batch: 3458
Batch: 3459
Batch: 3460
Batch: 3461
Batch: 3462
Batch: 3463
Batch: 3464
Batch: 3465
Batch: 3466
Batch: 3467
Batch: 3468
Batch: 3469
Batch: 3470
Batch: 3471
Batch: 3472
Batch: 3473
Batch: 3474
Batch: 3475
Batch: 3476
Batch: 3477
Batch: 3478
Batch: 3479
Batch: 3480
Batch: 3481
Batch: 3482
Batch: 3483
Batch: 3484
Batch: 3485
Batch: 3486
Batch: 3487
Batch: 3488
Batch: 3489
Batch: 3490
Batch: 3491
Batch: 3492
Batch: 3493
Batch: 3494
Batch: 3495
Batch: 3496
Batch: 3497
Batch: 3498
Batch: 3499
Batch: 3500
Batch: 3501
Batch: 3502
Batch: 3503
Batch: 3504
Batch: 3505
Batch: 3506
Batch: 3507
Batch: 3508
Batch: 3509
Batch: 3510
Batch: 3511
Batch: 3512
Batch: 3513
Batch: 3514
Batch: 3515
Batch: 3516
Batch: 3517
Batch: 3518
Batch: 3519
Batch: 3520
Batch: 3521
Batch: 3522
Batch: 3523
Batch: 3524
Batch: 3525
Batch: 3526
Batch: 3527
Batch: 3528
Batch: 3529
Batch: 3530
Batch: 3531
Batch: 3532
Batch: 3533
Batch: 3534
Batch: 3535
Batch: 3536
Batch: 3537
Batch: 3538
Batch: 3539
Batch: 3540
Batch: 3541
Batch: 3542
Batch: 3543
Batch: 3544
Batch: 3545
Batch: 3546
Batch: 3547
Batch: 3548
Batch: 3549
Batch: 3550
Batch: 3551
Batch: 3552
Batch: 3553
Batch: 3554
Batch: 3555
Batch: 3556
Batch: 3557
Batch: 3558
Batch: 3559
Batch: 3560
Batch: 3561
Batch: 3562
Batch: 3563
Batch: 3564
Batch: 3565
Batch: 3566
Batch: 3567
Batch: 3568
Batch: 3569
Batch: 3570
Batch: 3571
Batch: 3572
Batch: 3573
Batch: 3574
Batch: 3575
Batch: 3576
Batch: 3577
Batch: 3578
Batch: 3579
Batch: 3580
Batch: 3581
Batch: 3582
Batch: 3583
Batch: 3584
Batch: 3585
Batch: 3586
Batch: 3587
Batch: 3588
Batch: 3589
Batch: 3590
Batch: 3591
Batch: 3592
Batch: 3593
Batch: 3594
Batch: 3595
Batch: 3596
Batch: 3597
Batch: 3598
Batch: 3599
Batch: 3600
Batch: 3601
Batch: 3602
Batch: 3603
Batch: 3604
Batch: 3605
Batch: 3606
Batch: 3607
Batch: 3608
Batch: 3609
Batch: 3610
Batch: 3611
Batch: 3612
Batch: 3613
Batch: 3614
Batch: 3615
Batch: 3616
Batch: 3617
Batch: 3618
Batch: 3619
Batch: 3620
Batch: 3621
Batch: 3622
Batch: 3623
Batch: 3624
Batch: 3625
Batch: 3626
Batch: 3627
Batch: 3628
Batch: 3629
Batch: 3630
Batch: 3631
Batch: 3632
Batch: 3633
Batch: 3634
Batch: 3635
Batch: 3636
Batch: 3637
Batch: 3638
Batch: 3639
Batch: 3640
Batch: 3641
Batch: 3642
Batch: 3643
Batch: 3644
Batch: 3645
Batch: 3646
Batch: 3647
Batch: 3648
Batch: 3649
Batch: 3650
Batch: 3651
Batch: 3652
Batch: 3653
Batch: 3654
Batch: 3655
Batch: 3656
Batch: 3657
Batch: 3658
Batch: 3659
Batch: 3660
Batch: 3661
Batch: 3662
Batch: 3663
Batch: 3664
Batch: 3665
Batch: 3666
Batch: 3667
Batch: 3668
Batch: 3669
Batch: 3670
Batch: 3671
Batch: 3672
Batch: 3673
Batch: 3674
Batch: 3675
Batch: 3676
Batch: 3677
Batch: 3678
Batch: 3679
Batch: 3680
Batch: 3681
Batch: 3682
Batch: 3683
Batch: 3684
Batch: 3685
Batch: 3686
Batch: 3687
Batch: 3688
Batch: 3689
Batch: 3690
Batch: 3691
Batch: 3692
Batch: 3693
Batch: 3694
Batch: 3695
Batch: 3696
Batch: 3697
Batch: 3698
Batch: 3699
Batch: 3700
Batch: 3701
Batch: 3702
Batch: 3703
Batch: 3704
Batch: 3705
Batch: 3706
Batch: 3707
Batch: 3708
Batch: 3709
Batch: 3710
Batch: 3711
Batch: 3712
Batch: 3713
Batch: 3714
Batch: 3715
Batch: 3716
Batch: 3717
Batch: 3718
Batch: 3719
Batch: 3720
Batch: 3721
Batch: 3722
Batch: 3723
Batch: 3724
Batch: 3725
Batch: 3726
Batch: 3727
Batch: 3728
Batch: 3729
Batch: 3730
Batch: 3731
Batch: 3732
Batch: 3733
Batch: 3734
Batch: 3735
Batch: 3736
Batch: 3737
Batch: 3738
Batch: 3739
Batch: 3740
Batch: 3741
Batch: 3742
Batch: 3743
Batch: 3744
Batch: 3745
Batch: 3746
Batch: 3747
Batch: 3748
Batch: 3749
Batch: 3750
Batch: 3751
Batch: 3752
Batch: 3753
Batch: 3754
Batch: 3755
Batch: 3756
Batch: 3757
Batch: 3758
Batch: 3759
Batch: 3760
Batch: 3761
Batch: 3762
Batch: 3763
Batch: 3764
Batch: 3765
Batch: 3766
Batch: 3767
Batch: 3768
Batch: 3769
Batch: 3770
Batch: 3771
Batch: 3772
Batch: 3773
Batch: 3774
Batch: 3775
Batch: 3776
Batch: 3777
Batch: 3778
Batch: 3779
Batch: 3780
Batch: 3781
Batch: 3782
Batch: 3783
Batch: 3784
Batch: 3785
Batch: 3786
Batch: 3787
Batch: 3788
Batch: 3789
Batch: 3790
Batch: 3791
Batch: 3792
Batch: 3793
Batch: 3794
Batch: 3795
Batch: 3796
Batch: 3797
Batch: 3798
Batch: 3799
Batch: 3800
Batch: 3801
Batch: 3802
Batch: 3803
Batch: 3804
Batch: 3805
Batch: 3806
Batch: 3807
Batch: 3808
Batch: 3809
Batch: 3810
Batch: 3811
Batch: 3812
Batch: 3813
Batch: 3814
Batch: 3815
Batch: 3816
Batch: 3817
Batch: 3818
Batch: 3819
Batch: 3820
Batch: 3821
Batch: 3822
Batch: 3823
Batch: 3824
Batch: 3825
Batch: 3826
Batch: 3827
Batch: 3828
Batch: 3829
Batch: 3830
Batch: 3831
Batch: 3832
Batch: 3833
Batch: 3834
Batch: 3835
Batch: 3836
Batch: 3837
Batch: 3838
Batch: 3839
Batch: 3840
Batch: 3841
Batch: 3842
Batch: 3843
Batch: 3844
Batch: 3845
Batch: 3846
Batch: 3847
Batch: 3848
Batch: 3849
Batch: 3850
Batch: 3851
Batch: 3852
Batch: 3853
Batch: 3854
Batch: 3855
Batch: 3856
Batch: 3857
Batch: 3858
Batch: 3859
Batch: 3860
Batch: 3861
Batch: 3862
Batch: 3863
Batch: 3864
Batch: 3865
Batch: 3866
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000002
	speed: 0.1711s/iter; left time: 6597.8316s
	iters: 200, epoch: 1 | loss: 0.0000001
	speed: 0.1697s/iter; left time: 6528.7202s
	iters: 300, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6520.2093s
	iters: 400, epoch: 1 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 6492.6343s
	iters: 500, epoch: 1 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 6475.6259s
	iters: 600, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6460.9295s
	iters: 700, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6445.4867s
	iters: 800, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6426.1515s
	iters: 900, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6410.0785s
	iters: 1000, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6394.2790s
	iters: 1100, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6376.9032s
	iters: 1200, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6357.2890s
	iters: 1300, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6340.6216s
	iters: 1400, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6328.3744s
	iters: 1500, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6320.2002s
	iters: 1600, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6294.3305s
	iters: 1700, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6275.1376s
	iters: 1800, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6258.6005s
	iters: 1900, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6241.6695s
	iters: 2000, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6223.1063s
	iters: 2100, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6204.7457s
	iters: 2200, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6192.9164s
	iters: 2300, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6175.0309s
	iters: 2400, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6156.6669s
	iters: 2500, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6140.7744s
	iters: 2600, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6123.0832s
	iters: 2700, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6102.5033s
	iters: 2800, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6092.7537s
	iters: 2900, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6069.1556s
	iters: 3000, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6052.7735s
	iters: 3100, epoch: 1 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 6034.2914s
	iters: 3200, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6019.1877s
	iters: 3300, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 6002.8293s
	iters: 3400, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5985.2147s
	iters: 3500, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5968.4036s
	iters: 3600, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5952.1309s
	iters: 3700, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5933.9578s
	iters: 3800, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5917.8932s
Epoch: 1 cost time: 656.5037069320679
Epoch: 1, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000000
	speed: 2.8266s/iter; left time: 98095.7853s
	iters: 200, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5870.7420s
	iters: 300, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5857.1579s
	iters: 400, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 5836.5857s
	iters: 500, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 5819.6167s
	iters: 600, epoch: 2 | loss: 0.0000000
	speed: 0.1672s/iter; left time: 5717.5525s
	iters: 700, epoch: 2 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 5772.6702s
	iters: 800, epoch: 2 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 5758.8163s
	iters: 900, epoch: 2 | loss: 0.0000000
	speed: 0.1695s/iter; left time: 5747.0816s
	iters: 1000, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 5732.9189s
	iters: 1100, epoch: 2 | loss: 0.0000000
	speed: 0.1708s/iter; left time: 5755.6991s
	iters: 1200, epoch: 2 | loss: 0.0000000
	speed: 0.1711s/iter; left time: 5750.2973s
	iters: 1300, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5699.0660s
	iters: 1400, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5672.8264s
	iters: 1500, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5653.7342s
	iters: 1600, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5636.0931s
	iters: 1700, epoch: 2 | loss: 0.0000000
	speed: 0.1681s/iter; left time: 5565.3022s
	iters: 1800, epoch: 2 | loss: 0.0000000
	speed: 0.1689s/iter; left time: 5574.7603s
	iters: 1900, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 5580.1990s
	iters: 2000, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5568.2501s
	iters: 2100, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5551.4134s
	iters: 2200, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5531.3246s
	iters: 2300, epoch: 2 | loss: 0.0000000
	speed: 0.1713s/iter; left time: 5567.0711s
	iters: 2400, epoch: 2 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5512.5812s
	iters: 2500, epoch: 2 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5488.6770s
	iters: 2600, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5466.8377s
	iters: 2700, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5452.1072s
	iters: 2800, epoch: 2 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5436.3669s
	iters: 2900, epoch: 2 | loss: 0.0000000
	speed: 0.1684s/iter; left time: 5372.4020s
	iters: 3000, epoch: 2 | loss: 0.0000000
	speed: 0.1695s/iter; left time: 5389.8963s
	iters: 3100, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5379.3520s
	iters: 3200, epoch: 2 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5361.9033s
	iters: 3300, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 5344.5096s
	iters: 3400, epoch: 2 | loss: 0.0000000
	speed: 0.1708s/iter; left time: 5363.1332s
	iters: 3500, epoch: 2 | loss: 0.0000000
	speed: 0.1714s/iter; left time: 5365.6975s
	iters: 3600, epoch: 2 | loss: 0.0000000
	speed: 0.1685s/iter; left time: 5258.4401s
	iters: 3700, epoch: 2 | loss: 0.0000000
	speed: 0.1685s/iter; left time: 5241.6982s
	iters: 3800, epoch: 2 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 5257.7978s
Epoch: 2 cost time: 656.2046387195587
Epoch: 2, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000000
	speed: 2.8297s/iter; left time: 87259.1625s
	iters: 200, epoch: 3 | loss: 0.0000000
	speed: 0.1678s/iter; left time: 5157.0339s
	iters: 300, epoch: 3 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 5187.6594s
	iters: 400, epoch: 3 | loss: 0.0000000
	speed: 0.1708s/iter; left time: 5216.1269s
	iters: 500, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 5176.5272s
	iters: 600, epoch: 3 | loss: 0.0000000
	speed: 0.1710s/iter; left time: 5187.3884s
	iters: 700, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5137.0883s
	iters: 800, epoch: 3 | loss: 0.0000000
	speed: 0.1683s/iter; left time: 5070.7594s
	iters: 900, epoch: 3 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 5089.2549s
	iters: 1000, epoch: 3 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 5065.5771s
	iters: 1100, epoch: 3 | loss: 0.0000000
	speed: 0.1703s/iter; left time: 5082.5460s
	iters: 1200, epoch: 3 | loss: 0.0000000
	speed: 0.1712s/iter; left time: 5091.4694s
	iters: 1300, epoch: 3 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 5008.3227s
	iters: 1400, epoch: 3 | loss: 0.0000000
	speed: 0.1681s/iter; left time: 4965.9080s
	iters: 1500, epoch: 3 | loss: 0.0000000
	speed: 0.1695s/iter; left time: 4989.3696s
	iters: 1600, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4978.1385s
	iters: 1700, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4962.2737s
	iters: 1800, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4944.8220s
	iters: 1900, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4928.1597s
	iters: 2000, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4911.5602s
	iters: 2100, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4894.4786s
	iters: 2200, epoch: 3 | loss: 0.0000000
	speed: 0.1710s/iter; left time: 4914.9337s
	iters: 2300, epoch: 3 | loss: 0.0000000
	speed: 0.1704s/iter; left time: 4880.1340s
	iters: 2400, epoch: 3 | loss: 0.0000000
	speed: 0.1713s/iter; left time: 4889.4690s
	iters: 2500, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4830.6926s
	iters: 2600, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 4805.8108s
	iters: 2700, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4792.7350s
	iters: 2800, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4776.6441s
	iters: 2900, epoch: 3 | loss: 0.0000000
	speed: 0.1686s/iter; left time: 4726.6080s
	iters: 3000, epoch: 3 | loss: 0.0000000
	speed: 0.1689s/iter; left time: 4717.6611s
	iters: 3100, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4727.4335s
	iters: 3200, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4713.7491s
	iters: 3300, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4693.4048s
	iters: 3400, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4675.8767s
	iters: 3500, epoch: 3 | loss: 0.0000000
	speed: 0.1714s/iter; left time: 4703.7331s
	iters: 3600, epoch: 3 | loss: 0.0000000
	speed: 0.1705s/iter; left time: 4660.0512s
	iters: 3700, epoch: 3 | loss: 0.0000000
	speed: 0.1683s/iter; left time: 4584.7915s
	iters: 3800, epoch: 3 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 4594.6518s
Epoch: 3 cost time: 656.5379889011383
Epoch: 3, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000000
	speed: 2.8268s/iter; left time: 76238.2296s
	iters: 200, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4561.6023s
	iters: 300, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4550.6508s
	iters: 400, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4531.2825s
	iters: 500, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4512.4850s
	iters: 600, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4494.3492s
	iters: 700, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4483.7068s
	iters: 800, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4459.9451s
	iters: 900, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4442.4234s
	iters: 1000, epoch: 4 | loss: 0.0000000
	speed: 0.1707s/iter; left time: 4449.4986s
	iters: 1100, epoch: 4 | loss: 0.0000000
	speed: 0.1706s/iter; left time: 4429.2149s
	iters: 1200, epoch: 4 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4399.9012s
	iters: 1300, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4379.6295s
	iters: 1400, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4363.5732s
	iters: 1500, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4344.1203s
	iters: 1600, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4327.6609s
	iters: 1700, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4308.6349s
	iters: 1800, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4292.3800s
	iters: 1900, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4277.9587s
	iters: 2000, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4256.5098s
	iters: 2100, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4239.4113s
	iters: 2200, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4222.3117s
	iters: 2300, epoch: 4 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4204.6346s
	iters: 2400, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4187.7988s
	iters: 2500, epoch: 4 | loss: 0.0000000
	speed: 0.1688s/iter; left time: 4148.1175s
	iters: 2600, epoch: 4 | loss: 0.0000000
	speed: 0.1688s/iter; left time: 4130.0493s
	iters: 2700, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4141.9522s
	iters: 2800, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4126.7478s
	iters: 2900, epoch: 4 | loss: 0.0000000
	speed: 0.1695s/iter; left time: 4096.4868s
	iters: 3000, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 4081.8177s
	iters: 3100, epoch: 4 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 4050.5167s
	iters: 3200, epoch: 4 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 4047.8687s
	iters: 3300, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4038.5467s
	iters: 3400, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4023.1677s
	iters: 3500, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4003.1432s
	iters: 3600, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3986.5395s
	iters: 3700, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3968.6369s
	iters: 3800, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3951.8801s
Epoch: 4 cost time: 656.800136089325
Epoch: 4, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000000
	speed: 2.8292s/iter; left time: 65362.7961s
	iters: 200, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3906.7290s
	iters: 300, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3888.5612s
	iters: 400, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3877.2101s
	iters: 500, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3854.0591s
	iters: 600, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3836.7565s
	iters: 700, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3820.0935s
	iters: 800, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3804.0430s
	iters: 900, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3787.6025s
	iters: 1000, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3770.5094s
	iters: 1100, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3753.1099s
	iters: 1200, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3736.7016s
	iters: 1300, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3718.5958s
	iters: 1400, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3702.2462s
	iters: 1500, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3685.8642s
	iters: 1600, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3672.6366s
	iters: 1700, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3652.2899s
	iters: 1800, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3636.9237s
	iters: 1900, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3618.2975s
	iters: 2000, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3602.0404s
	iters: 2100, epoch: 5 | loss: 0.0000000
	speed: 0.1695s/iter; left time: 3577.2756s
	iters: 2200, epoch: 5 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 3552.4657s
	iters: 2300, epoch: 5 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 3541.8920s
	iters: 2400, epoch: 5 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 3529.0020s
	iters: 2500, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3512.2818s
	iters: 2600, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3497.1661s
	iters: 2700, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3481.2230s
	iters: 2800, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3466.9570s
	iters: 2900, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3446.2222s
	iters: 3000, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3429.1526s
	iters: 3100, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3411.6613s
	iters: 3200, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3394.1285s
	iters: 3300, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3378.0544s
	iters: 3400, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3360.7993s
	iters: 3500, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3342.7336s
	iters: 3600, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3325.9559s
	iters: 3700, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3311.8575s
	iters: 3800, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3294.2214s
Epoch: 5 cost time: 656.623862028122
Epoch: 5, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000000
	speed: 2.8320s/iter; left time: 54475.8026s
	iters: 200, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3248.8005s
	iters: 300, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3232.4376s
	iters: 400, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3214.2430s
	iters: 500, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3197.8266s
	iters: 600, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3179.6541s
	iters: 700, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3163.2940s
	iters: 800, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3147.3363s
	iters: 900, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3129.5969s
	iters: 1000, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3111.3773s
	iters: 1100, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3095.1052s
	iters: 1200, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3080.7049s
	iters: 1300, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3066.1695s
	iters: 1400, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3046.0102s
	iters: 1500, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3029.5421s
	iters: 1600, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3012.3674s
	iters: 1700, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2995.6727s
	iters: 1800, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2978.3534s
	iters: 1900, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2961.1160s
	iters: 2000, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2943.7182s
	iters: 2100, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2925.8083s
	iters: 2200, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2908.7154s
	iters: 2300, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2892.7294s
	iters: 2400, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2876.4234s
	iters: 2500, epoch: 6 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 2863.0960s
	iters: 2600, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2843.2804s
	iters: 2700, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2826.2586s
	iters: 2800, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2808.7215s
	iters: 2900, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2790.4139s
	iters: 3000, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2772.8209s
	iters: 3100, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2755.5892s
	iters: 3200, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2739.0538s
	iters: 3300, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2722.2992s
	iters: 3400, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2705.1047s
	iters: 3500, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2688.6660s
	iters: 3600, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2672.6863s
	iters: 3700, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2658.7883s
	iters: 3800, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2638.9798s
Epoch: 6 cost time: 656.8554005622864
Epoch: 6, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000000
	speed: 2.8275s/iter; left time: 43456.1746s
	iters: 200, epoch: 7 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 2589.9968s
	iters: 300, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2574.4835s
	iters: 400, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2556.5255s
	iters: 500, epoch: 7 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 2539.2533s
	iters: 600, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2523.3257s
	iters: 700, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2506.4577s
	iters: 800, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2489.3352s
	iters: 900, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2471.6373s
	iters: 1000, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2455.0839s
	iters: 1100, epoch: 7 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 2437.4593s
	iters: 1200, epoch: 7 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2424.4495s
	iters: 1300, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2403.8098s
	iters: 1400, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2387.2366s
	iters: 1500, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2370.3152s
	iters: 1600, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2353.3297s
	iters: 1700, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2336.0195s
	iters: 1800, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2320.0948s
	iters: 1900, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2303.7403s
	iters: 2000, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2286.8424s
	iters: 2100, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2269.5737s
	iters: 2200, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2252.9368s
	iters: 2300, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2234.8456s
	iters: 2400, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2221.7014s
	iters: 2500, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2202.7238s
	iters: 2600, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2184.5338s
	iters: 2700, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2167.2465s
	iters: 2800, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2150.8758s
	iters: 2900, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2133.3942s
	iters: 3000, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2117.5399s
	iters: 3100, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2100.6964s
	iters: 3200, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2082.6906s
	iters: 3300, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2065.4791s
	iters: 3400, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2048.2595s
	iters: 3500, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2031.4759s
	iters: 3600, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2018.0397s
	iters: 3700, epoch: 7 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1999.1444s
	iters: 3800, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1981.9018s
Epoch: 7 cost time: 656.5688116550446
Epoch: 7, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000000
	speed: 2.8280s/iter; left time: 32527.6284s
	iters: 200, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1935.8407s
	iters: 300, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1919.0265s
	iters: 400, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1902.0558s
	iters: 500, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1885.0303s
	iters: 600, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1868.2809s
	iters: 700, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1851.4600s
	iters: 800, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1834.2714s
	iters: 900, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1817.1564s
	iters: 1000, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1802.8207s
	iters: 1100, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1782.7984s
	iters: 1200, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1766.0150s
	iters: 1300, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1749.5937s
	iters: 1400, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1732.1581s
	iters: 1500, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1715.6584s
	iters: 1600, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1698.4205s
	iters: 1700, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1681.9017s
	iters: 1800, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1664.3707s
	iters: 1900, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1647.4873s
	iters: 2000, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1630.6315s
	iters: 2100, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1614.0324s
	iters: 2200, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1598.0590s
	iters: 2300, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1579.3874s
	iters: 2400, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1562.5549s
	iters: 2500, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1545.3241s
	iters: 2600, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1528.5231s
	iters: 2700, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1511.7084s
	iters: 2800, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1494.6081s
	iters: 2900, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1477.8785s
	iters: 3000, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1460.6137s
	iters: 3100, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1443.3910s
	iters: 3200, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1426.5532s
	iters: 3300, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1409.7922s
	iters: 3400, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1394.2343s
	iters: 3500, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1376.0311s
	iters: 3600, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1358.8512s
	iters: 3700, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1341.7881s
	iters: 3800, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1324.4219s
Epoch: 8 cost time: 656.8322386741638
Epoch: 8, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000000
	speed: 2.8276s/iter; left time: 21588.4971s
	iters: 200, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1278.9698s
	iters: 300, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1262.0896s
	iters: 400, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1244.9501s
	iters: 500, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1227.8653s
	iters: 600, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1211.1555s
	iters: 700, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1195.4188s
	iters: 800, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1176.7561s
	iters: 900, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1159.7408s
	iters: 1000, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1143.0582s
	iters: 1100, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1125.8396s
	iters: 1200, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1109.0801s
	iters: 1300, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1092.0600s
	iters: 1400, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1075.0144s
	iters: 1500, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1058.3251s
	iters: 1600, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1041.4355s
	iters: 1700, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1024.6188s
	iters: 1800, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1007.5094s
	iters: 1900, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 991.5075s
	iters: 2000, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 973.4799s
	iters: 2100, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 956.4270s
	iters: 2200, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 939.3600s
	iters: 2300, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 922.2528s
	iters: 2400, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 905.4599s
	iters: 2500, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 888.4375s
	iters: 2600, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 871.5652s
	iters: 2700, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 854.5960s
	iters: 2800, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 837.3557s
	iters: 2900, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 820.7373s
	iters: 3000, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 803.8511s
	iters: 3100, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 786.7757s
	iters: 3200, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 770.6180s
	iters: 3300, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 752.8191s
	iters: 3400, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 735.8085s
	iters: 3500, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 718.8526s
	iters: 3600, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 701.8326s
	iters: 3700, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 684.7593s
	iters: 3800, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 667.9862s
Epoch: 9 cost time: 656.5394909381866
Epoch: 9, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000000
	speed: 2.8286s/iter; left time: 10658.2657s
	iters: 200, epoch: 10 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 622.6056s
	iters: 300, epoch: 10 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 605.6361s
	iters: 400, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 589.6953s
	iters: 500, epoch: 10 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 571.6696s
	iters: 600, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 554.7740s
	iters: 700, epoch: 10 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 537.7405s
	iters: 800, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 521.0627s
	iters: 900, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 504.0528s
	iters: 1000, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 487.0239s
	iters: 1100, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 469.9648s
	iters: 1200, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 453.0376s
	iters: 1300, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 436.0447s
	iters: 1400, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 419.0170s
	iters: 1500, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 402.0624s
	iters: 1600, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 385.5504s
	iters: 1700, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 368.1222s
	iters: 1800, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 351.1076s
	iters: 1900, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 334.1772s
	iters: 2000, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 317.2327s
	iters: 2100, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 300.2412s
	iters: 2200, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 283.4075s
	iters: 2300, epoch: 10 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 266.1607s
	iters: 2400, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 249.2046s
	iters: 2500, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 232.2242s
	iters: 2600, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 215.2959s
	iters: 2700, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 198.2862s
	iters: 2800, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 181.5911s
	iters: 2900, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 164.3386s
	iters: 3000, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 147.4432s
	iters: 3100, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 130.4790s
	iters: 3200, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 113.5047s
	iters: 3300, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 96.5186s
	iters: 3400, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 79.4941s
	iters: 3500, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 62.4810s
	iters: 3600, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 45.5044s
	iters: 3700, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 28.5249s
	iters: 3800, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 11.5508s
Epoch: 10 cost time: 656.8474378585815
Epoch: 10, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 449820
test: (449919, 51)
train: (495000, 51)
train 494901
Threshold : 2.8991769021047295e-08
pred:    (44982000,)
gt:      (44982000,)
pred:  (44982000,)
gt:    (44982000,)
Accuracy : 0.9663, Precision : 0.9054, Recall : 0.8071, F-score : 0.8534, AUC : 0.8977
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 494901
test: (449919, 51)
train: (495000, 51)
val 449820
test: (449919, 51)
train: (495000, 51)
test 449820
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Batch: 455
Batch: 456
Batch: 457
Batch: 458
Batch: 459
Batch: 460
Batch: 461
Batch: 462
Batch: 463
Batch: 464
Batch: 465
Batch: 466
Batch: 467
Batch: 468
Batch: 469
Batch: 470
Batch: 471
Batch: 472
Batch: 473
Batch: 474
Batch: 475
Batch: 476
Batch: 477
Batch: 478
Batch: 479
Batch: 480
Batch: 481
Batch: 482
Batch: 483
Batch: 484
Batch: 485
Batch: 486
Batch: 487
Batch: 488
Batch: 489
Batch: 490
Batch: 491
Batch: 492
Batch: 493
Batch: 494
Batch: 495
Batch: 496
Batch: 497
Batch: 498
Batch: 499
Batch: 500
Batch: 501
Batch: 502
Batch: 503
Batch: 504
Batch: 505
Batch: 506
Batch: 507
Batch: 508
Batch: 509
Batch: 510
Batch: 511
Batch: 512
Batch: 513
Batch: 514
Batch: 515
Batch: 516
Batch: 517
Batch: 518
Batch: 519
Batch: 520
Batch: 521
Batch: 522
Batch: 523
Batch: 524
Batch: 525
Batch: 526
Batch: 527
Batch: 528
Batch: 529
Batch: 530
Batch: 531
Batch: 532
Batch: 533
Batch: 534
Batch: 535
Batch: 536
Batch: 537
Batch: 538
Batch: 539
Batch: 540
Batch: 541
Batch: 542
Batch: 543
Batch: 544
Batch: 545
Batch: 546
Batch: 547
Batch: 548
Batch: 549
Batch: 550
Batch: 551
Batch: 552
Batch: 553
Batch: 554
Batch: 555
Batch: 556
Batch: 557
Batch: 558
Batch: 559
Batch: 560
Batch: 561
Batch: 562
Batch: 563
Batch: 564
Batch: 565
Batch: 566
Batch: 567
Batch: 568
Batch: 569
Batch: 570
Batch: 571
Batch: 572
Batch: 573
Batch: 574
Batch: 575
Batch: 576
Batch: 577
Batch: 578
Batch: 579
Batch: 580
Batch: 581
Batch: 582
Batch: 583
Batch: 584
Batch: 585
Batch: 586
Batch: 587
Batch: 588
Batch: 589
Batch: 590
Batch: 591
Batch: 592
Batch: 593
Batch: 594
Batch: 595
Batch: 596
Batch: 597
Batch: 598
Batch: 599
Batch: 600
Batch: 601
Batch: 602
Batch: 603
Batch: 604
Batch: 605
Batch: 606
Batch: 607
Batch: 608
Batch: 609
Batch: 610
Batch: 611
Batch: 612
Batch: 613
Batch: 614
Batch: 615
Batch: 616
Batch: 617
Batch: 618
Batch: 619
Batch: 620
Batch: 621
Batch: 622
Batch: 623
Batch: 624
Batch: 625
Batch: 626
Batch: 627
Batch: 628
Batch: 629
Batch: 630
Batch: 631
Batch: 632
Batch: 633
Batch: 634
Batch: 635
Batch: 636
Batch: 637
Batch: 638
Batch: 639
Batch: 640
Batch: 641
Batch: 642
Batch: 643
Batch: 644
Batch: 645
Batch: 646
Batch: 647
Batch: 648
Batch: 649
Batch: 650
Batch: 651
Batch: 652
Batch: 653
Batch: 654
Batch: 655
Batch: 656
Batch: 657
Batch: 658
Batch: 659
Batch: 660
Batch: 661
Batch: 662
Batch: 663
Batch: 664
Batch: 665
Batch: 666
Batch: 667
Batch: 668
Batch: 669
Batch: 670
Batch: 671
Batch: 672
Batch: 673
Batch: 674
Batch: 675
Batch: 676
Batch: 677
Batch: 678
Batch: 679
Batch: 680
Batch: 681
Batch: 682
Batch: 683
Batch: 684
Batch: 685
Batch: 686
Batch: 687
Batch: 688
Batch: 689
Batch: 690
Batch: 691
Batch: 692
Batch: 693
Batch: 694
Batch: 695
Batch: 696
Batch: 697
Batch: 698
Batch: 699
Batch: 700
Batch: 701
Batch: 702
Batch: 703
Batch: 704
Batch: 705
Batch: 706
Batch: 707
Batch: 708
Batch: 709
Batch: 710
Batch: 711
Batch: 712
Batch: 713
Batch: 714
Batch: 715
Batch: 716
Batch: 717
Batch: 718
Batch: 719
Batch: 720
Batch: 721
Batch: 722
Batch: 723
Batch: 724
Batch: 725
Batch: 726
Batch: 727
Batch: 728
Batch: 729
Batch: 730
Batch: 731
Batch: 732
Batch: 733
Batch: 734
Batch: 735
Batch: 736
Batch: 737
Batch: 738
Batch: 739
Batch: 740
Batch: 741
Batch: 742
Batch: 743
Batch: 744
Batch: 745
Batch: 746
Batch: 747
Batch: 748
Batch: 749
Batch: 750
Batch: 751
Batch: 752
Batch: 753
Batch: 754
Batch: 755
Batch: 756
Batch: 757
Batch: 758
Batch: 759
Batch: 760
Batch: 761
Batch: 762
Batch: 763
Batch: 764
Batch: 765
Batch: 766
Batch: 767
Batch: 768
Batch: 769
Batch: 770
Batch: 771
Batch: 772
Batch: 773
Batch: 774
Batch: 775
Batch: 776
Batch: 777
Batch: 778
Batch: 779
Batch: 780
Batch: 781
Batch: 782
Batch: 783
Batch: 784
Batch: 785
Batch: 786
Batch: 787
Batch: 788
Batch: 789
Batch: 790
Batch: 791
Batch: 792
Batch: 793
Batch: 794
Batch: 795
Batch: 796
Batch: 797
Batch: 798
Batch: 799
Batch: 800
Batch: 801
Batch: 802
Batch: 803
Batch: 804
Batch: 805
Batch: 806
Batch: 807
Batch: 808
Batch: 809
Batch: 810
Batch: 811
Batch: 812
Batch: 813
Batch: 814
Batch: 815
Batch: 816
Batch: 817
Batch: 818
Batch: 819
Batch: 820
Batch: 821
Batch: 822
Batch: 823
Batch: 824
Batch: 825
Batch: 826
Batch: 827
Batch: 828
Batch: 829
Batch: 830
Batch: 831
Batch: 832
Batch: 833
Batch: 834
Batch: 835
Batch: 836
Batch: 837
Batch: 838
Batch: 839
Batch: 840
Batch: 841
Batch: 842
Batch: 843
Batch: 844
Batch: 845
Batch: 846
Batch: 847
Batch: 848
Batch: 849
Batch: 850
Batch: 851
Batch: 852
Batch: 853
Batch: 854
Batch: 855
Batch: 856
Batch: 857
Batch: 858
Batch: 859
Batch: 860
Batch: 861
Batch: 862
Batch: 863
Batch: 864
Batch: 865
Batch: 866
Batch: 867
Batch: 868
Batch: 869
Batch: 870
Batch: 871
Batch: 872
Batch: 873
Batch: 874
Batch: 875
Batch: 876
Batch: 877
Batch: 878
Batch: 879
Batch: 880
Batch: 881
Batch: 882
Batch: 883
Batch: 884
Batch: 885
Batch: 886
Batch: 887
Batch: 888
Batch: 889
Batch: 890
Batch: 891
Batch: 892
Batch: 893
Batch: 894
Batch: 895
Batch: 896
Batch: 897
Batch: 898
Batch: 899
Batch: 900
Batch: 901
Batch: 902
Batch: 903
Batch: 904
Batch: 905
Batch: 906
Batch: 907
Batch: 908
Batch: 909
Batch: 910
Batch: 911
Batch: 912
Batch: 913
Batch: 914
Batch: 915
Batch: 916
Batch: 917
Batch: 918
Batch: 919
Batch: 920
Batch: 921
Batch: 922
Batch: 923
Batch: 924
Batch: 925
Batch: 926
Batch: 927
Batch: 928
Batch: 929
Batch: 930
Batch: 931
Batch: 932
Batch: 933
Batch: 934
Batch: 935
Batch: 936
Batch: 937
Batch: 938
Batch: 939
Batch: 940
Batch: 941
Batch: 942
Batch: 943
Batch: 944
Batch: 945
Batch: 946
Batch: 947
Batch: 948
Batch: 949
Batch: 950
Batch: 951
Batch: 952
Batch: 953
Batch: 954
Batch: 955
Batch: 956
Batch: 957
Batch: 958
Batch: 959
Batch: 960
Batch: 961
Batch: 962
Batch: 963
Batch: 964
Batch: 965
Batch: 966
Batch: 967
Batch: 968
Batch: 969
Batch: 970
Batch: 971
Batch: 972
Batch: 973
Batch: 974
Batch: 975
Batch: 976
Batch: 977
Batch: 978
Batch: 979
Batch: 980
Batch: 981
Batch: 982
Batch: 983
Batch: 984
Batch: 985
Batch: 986
Batch: 987
Batch: 988
Batch: 989
Batch: 990
Batch: 991
Batch: 992
Batch: 993
Batch: 994
Batch: 995
Batch: 996
Batch: 997
Batch: 998
Batch: 999
Batch: 1000
Batch: 1001
Batch: 1002
Batch: 1003
Batch: 1004
Batch: 1005
Batch: 1006
Batch: 1007
Batch: 1008
Batch: 1009
Batch: 1010
Batch: 1011
Batch: 1012
Batch: 1013
Batch: 1014
Batch: 1015
Batch: 1016
Batch: 1017
Batch: 1018
Batch: 1019
Batch: 1020
Batch: 1021
Batch: 1022
Batch: 1023
Batch: 1024
Batch: 1025
Batch: 1026
Batch: 1027
Batch: 1028
Batch: 1029
Batch: 1030
Batch: 1031
Batch: 1032
Batch: 1033
Batch: 1034
Batch: 1035
Batch: 1036
Batch: 1037
Batch: 1038
Batch: 1039
Batch: 1040
Batch: 1041
Batch: 1042
Batch: 1043
Batch: 1044
Batch: 1045
Batch: 1046
Batch: 1047
Batch: 1048
Batch: 1049
Batch: 1050
Batch: 1051
Batch: 1052
Batch: 1053
Batch: 1054
Batch: 1055
Batch: 1056
Batch: 1057
Batch: 1058
Batch: 1059
Batch: 1060
Batch: 1061
Batch: 1062
Batch: 1063
Batch: 1064
Batch: 1065
Batch: 1066
Batch: 1067
Batch: 1068
Batch: 1069
Batch: 1070
Batch: 1071
Batch: 1072
Batch: 1073
Batch: 1074
Batch: 1075
Batch: 1076
Batch: 1077
Batch: 1078
Batch: 1079
Batch: 1080
Batch: 1081
Batch: 1082
Batch: 1083
Batch: 1084
Batch: 1085
Batch: 1086
Batch: 1087
Batch: 1088
Batch: 1089
Batch: 1090
Batch: 1091
Batch: 1092
Batch: 1093
Batch: 1094
Batch: 1095
Batch: 1096
Batch: 1097
Batch: 1098
Batch: 1099
Batch: 1100
Batch: 1101
Batch: 1102
Batch: 1103
Batch: 1104
Batch: 1105
Batch: 1106
Batch: 1107
Batch: 1108
Batch: 1109
Batch: 1110
Batch: 1111
Batch: 1112
Batch: 1113
Batch: 1114
Batch: 1115
Batch: 1116
Batch: 1117
Batch: 1118
Batch: 1119
Batch: 1120
Batch: 1121
Batch: 1122
Batch: 1123
Batch: 1124
Batch: 1125
Batch: 1126
Batch: 1127
Batch: 1128
Batch: 1129
Batch: 1130
Batch: 1131
Batch: 1132
Batch: 1133
Batch: 1134
Batch: 1135
Batch: 1136
Batch: 1137
Batch: 1138
Batch: 1139
Batch: 1140
Batch: 1141
Batch: 1142
Batch: 1143
Batch: 1144
Batch: 1145
Batch: 1146
Batch: 1147
Batch: 1148
Batch: 1149
Batch: 1150
Batch: 1151
Batch: 1152
Batch: 1153
Batch: 1154
Batch: 1155
Batch: 1156
Batch: 1157
Batch: 1158
Batch: 1159
Batch: 1160
Batch: 1161
Batch: 1162
Batch: 1163
Batch: 1164
Batch: 1165
Batch: 1166
Batch: 1167
Batch: 1168
Batch: 1169
Batch: 1170
Batch: 1171
Batch: 1172
Batch: 1173
Batch: 1174
Batch: 1175
Batch: 1176
Batch: 1177
Batch: 1178
Batch: 1179
Batch: 1180
Batch: 1181
Batch: 1182
Batch: 1183
Batch: 1184
Batch: 1185
Batch: 1186
Batch: 1187
Batch: 1188
Batch: 1189
Batch: 1190
Batch: 1191
Batch: 1192
Batch: 1193
Batch: 1194
Batch: 1195
Batch: 1196
Batch: 1197
Batch: 1198
Batch: 1199
Batch: 1200
Batch: 1201
Batch: 1202
Batch: 1203
Batch: 1204
Batch: 1205
Batch: 1206
Batch: 1207
Batch: 1208
Batch: 1209
Batch: 1210
Batch: 1211
Batch: 1212
Batch: 1213
Batch: 1214
Batch: 1215
Batch: 1216
Batch: 1217
Batch: 1218
Batch: 1219
Batch: 1220
Batch: 1221
Batch: 1222
Batch: 1223
Batch: 1224
Batch: 1225
Batch: 1226
Batch: 1227
Batch: 1228
Batch: 1229
Batch: 1230
Batch: 1231
Batch: 1232
Batch: 1233
Batch: 1234
Batch: 1235
Batch: 1236
Batch: 1237
Batch: 1238
Batch: 1239
Batch: 1240
Batch: 1241
Batch: 1242
Batch: 1243
Batch: 1244
Batch: 1245
Batch: 1246
Batch: 1247
Batch: 1248
Batch: 1249
Batch: 1250
Batch: 1251
Batch: 1252
Batch: 1253
Batch: 1254
Batch: 1255
Batch: 1256
Batch: 1257
Batch: 1258
Batch: 1259
Batch: 1260
Batch: 1261
Batch: 1262
Batch: 1263
Batch: 1264
Batch: 1265
Batch: 1266
Batch: 1267
Batch: 1268
Batch: 1269
Batch: 1270
Batch: 1271
Batch: 1272
Batch: 1273
Batch: 1274
Batch: 1275
Batch: 1276
Batch: 1277
Batch: 1278
Batch: 1279
Batch: 1280
Batch: 1281
Batch: 1282
Batch: 1283
Batch: 1284
Batch: 1285
Batch: 1286
Batch: 1287
Batch: 1288
Batch: 1289
Batch: 1290
Batch: 1291
Batch: 1292
Batch: 1293
Batch: 1294
Batch: 1295
Batch: 1296
Batch: 1297
Batch: 1298
Batch: 1299
Batch: 1300
Batch: 1301
Batch: 1302
Batch: 1303
Batch: 1304
Batch: 1305
Batch: 1306
Batch: 1307
Batch: 1308
Batch: 1309
Batch: 1310
Batch: 1311
Batch: 1312
Batch: 1313
Batch: 1314
Batch: 1315
Batch: 1316
Batch: 1317
Batch: 1318
Batch: 1319
Batch: 1320
Batch: 1321
Batch: 1322
Batch: 1323
Batch: 1324
Batch: 1325
Batch: 1326
Batch: 1327
Batch: 1328
Batch: 1329
Batch: 1330
Batch: 1331
Batch: 1332
Batch: 1333
Batch: 1334
Batch: 1335
Batch: 1336
Batch: 1337
Batch: 1338
Batch: 1339
Batch: 1340
Batch: 1341
Batch: 1342
Batch: 1343
Batch: 1344
Batch: 1345
Batch: 1346
Batch: 1347
Batch: 1348
Batch: 1349
Batch: 1350
Batch: 1351
Batch: 1352
Batch: 1353
Batch: 1354
Batch: 1355
Batch: 1356
Batch: 1357
Batch: 1358
Batch: 1359
Batch: 1360
Batch: 1361
Batch: 1362
Batch: 1363
Batch: 1364
Batch: 1365
Batch: 1366
Batch: 1367
Batch: 1368
Batch: 1369
Batch: 1370
Batch: 1371
Batch: 1372
Batch: 1373
Batch: 1374
Batch: 1375
Batch: 1376
Batch: 1377
Batch: 1378
Batch: 1379
Batch: 1380
Batch: 1381
Batch: 1382
Batch: 1383
Batch: 1384
Batch: 1385
Batch: 1386
Batch: 1387
Batch: 1388
Batch: 1389
Batch: 1390
Batch: 1391
Batch: 1392
Batch: 1393
Batch: 1394
Batch: 1395
Batch: 1396
Batch: 1397
Batch: 1398
Batch: 1399
Batch: 1400
Batch: 1401
Batch: 1402
Batch: 1403
Batch: 1404
Batch: 1405
Batch: 1406
Batch: 1407
Batch: 1408
Batch: 1409
Batch: 1410
Batch: 1411
Batch: 1412
Batch: 1413
Batch: 1414
Batch: 1415
Batch: 1416
Batch: 1417
Batch: 1418
Batch: 1419
Batch: 1420
Batch: 1421
Batch: 1422
Batch: 1423
Batch: 1424
Batch: 1425
Batch: 1426
Batch: 1427
Batch: 1428
Batch: 1429
Batch: 1430
Batch: 1431
Batch: 1432
Batch: 1433
Batch: 1434
Batch: 1435
Batch: 1436
Batch: 1437
Batch: 1438
Batch: 1439
Batch: 1440
Batch: 1441
Batch: 1442
Batch: 1443
Batch: 1444
Batch: 1445
Batch: 1446
Batch: 1447
Batch: 1448
Batch: 1449
Batch: 1450
Batch: 1451
Batch: 1452
Batch: 1453
Batch: 1454
Batch: 1455
Batch: 1456
Batch: 1457
Batch: 1458
Batch: 1459
Batch: 1460
Batch: 1461
Batch: 1462
Batch: 1463
Batch: 1464
Batch: 1465
Batch: 1466
Batch: 1467
Batch: 1468
Batch: 1469
Batch: 1470
Batch: 1471
Batch: 1472
Batch: 1473
Batch: 1474
Batch: 1475
Batch: 1476
Batch: 1477
Batch: 1478
Batch: 1479
Batch: 1480
Batch: 1481
Batch: 1482
Batch: 1483
Batch: 1484
Batch: 1485
Batch: 1486
Batch: 1487
Batch: 1488
Batch: 1489
Batch: 1490
Batch: 1491
Batch: 1492
Batch: 1493
Batch: 1494
Batch: 1495
Batch: 1496
Batch: 1497
Batch: 1498
Batch: 1499
Batch: 1500
Batch: 1501
Batch: 1502
Batch: 1503
Batch: 1504
Batch: 1505
Batch: 1506
Batch: 1507
Batch: 1508
Batch: 1509
Batch: 1510
Batch: 1511
Batch: 1512
Batch: 1513
Batch: 1514
Batch: 1515
Batch: 1516
Batch: 1517
Batch: 1518
Batch: 1519
Batch: 1520
Batch: 1521
Batch: 1522
Batch: 1523
Batch: 1524
Batch: 1525
Batch: 1526
Batch: 1527
Batch: 1528
Batch: 1529
Batch: 1530
Batch: 1531
Batch: 1532
Batch: 1533
Batch: 1534
Batch: 1535
Batch: 1536
Batch: 1537
Batch: 1538
Batch: 1539
Batch: 1540
Batch: 1541
Batch: 1542
Batch: 1543
Batch: 1544
Batch: 1545
Batch: 1546
Batch: 1547
Batch: 1548
Batch: 1549
Batch: 1550
Batch: 1551
Batch: 1552
Batch: 1553
Batch: 1554
Batch: 1555
Batch: 1556
Batch: 1557
Batch: 1558
Batch: 1559
Batch: 1560
Batch: 1561
Batch: 1562
Batch: 1563
Batch: 1564
Batch: 1565
Batch: 1566
Batch: 1567
Batch: 1568
Batch: 1569
Batch: 1570
Batch: 1571
Batch: 1572
Batch: 1573
Batch: 1574
Batch: 1575
Batch: 1576
Batch: 1577
Batch: 1578
Batch: 1579
Batch: 1580
Batch: 1581
Batch: 1582
Batch: 1583
Batch: 1584
Batch: 1585
Batch: 1586
Batch: 1587
Batch: 1588
Batch: 1589
Batch: 1590
Batch: 1591
Batch: 1592
Batch: 1593
Batch: 1594
Batch: 1595
Batch: 1596
Batch: 1597
Batch: 1598
Batch: 1599
Batch: 1600
Batch: 1601
Batch: 1602
Batch: 1603
Batch: 1604
Batch: 1605
Batch: 1606
Batch: 1607
Batch: 1608
Batch: 1609
Batch: 1610
Batch: 1611
Batch: 1612
Batch: 1613
Batch: 1614
Batch: 1615
Batch: 1616
Batch: 1617
Batch: 1618
Batch: 1619
Batch: 1620
Batch: 1621
Batch: 1622
Batch: 1623
Batch: 1624
Batch: 1625
Batch: 1626
Batch: 1627
Batch: 1628
Batch: 1629
Batch: 1630
Batch: 1631
Batch: 1632
Batch: 1633
Batch: 1634
Batch: 1635
Batch: 1636
Batch: 1637
Batch: 1638
Batch: 1639
Batch: 1640
Batch: 1641
Batch: 1642
Batch: 1643
Batch: 1644
Batch: 1645
Batch: 1646
Batch: 1647
Batch: 1648
Batch: 1649
Batch: 1650
Batch: 1651
Batch: 1652
Batch: 1653
Batch: 1654
Batch: 1655
Batch: 1656
Batch: 1657
Batch: 1658
Batch: 1659
Batch: 1660
Batch: 1661
Batch: 1662
Batch: 1663
Batch: 1664
Batch: 1665
Batch: 1666
Batch: 1667
Batch: 1668
Batch: 1669
Batch: 1670
Batch: 1671
Batch: 1672
Batch: 1673
Batch: 1674
Batch: 1675
Batch: 1676
Batch: 1677
Batch: 1678
Batch: 1679
Batch: 1680
Batch: 1681
Batch: 1682
Batch: 1683
Batch: 1684
Batch: 1685
Batch: 1686
Batch: 1687
Batch: 1688
Batch: 1689
Batch: 1690
Batch: 1691
Batch: 1692
Batch: 1693
Batch: 1694
Batch: 1695
Batch: 1696
Batch: 1697
Batch: 1698
Batch: 1699
Batch: 1700
Batch: 1701
Batch: 1702
Batch: 1703
Batch: 1704
Batch: 1705
Batch: 1706
Batch: 1707
Batch: 1708
Batch: 1709
Batch: 1710
Batch: 1711
Batch: 1712
Batch: 1713
Batch: 1714
Batch: 1715
Batch: 1716
Batch: 1717
Batch: 1718
Batch: 1719
Batch: 1720
Batch: 1721
Batch: 1722
Batch: 1723
Batch: 1724
Batch: 1725
Batch: 1726
Batch: 1727
Batch: 1728
Batch: 1729
Batch: 1730
Batch: 1731
Batch: 1732
Batch: 1733
Batch: 1734
Batch: 1735
Batch: 1736
Batch: 1737
Batch: 1738
Batch: 1739
Batch: 1740
Batch: 1741
Batch: 1742
Batch: 1743
Batch: 1744
Batch: 1745
Batch: 1746
Batch: 1747
Batch: 1748
Batch: 1749
Batch: 1750
Batch: 1751
Batch: 1752
Batch: 1753
Batch: 1754
Batch: 1755
Batch: 1756
Batch: 1757
Batch: 1758
Batch: 1759
Batch: 1760
Batch: 1761
Batch: 1762
Batch: 1763
Batch: 1764
Batch: 1765
Batch: 1766
Batch: 1767
Batch: 1768
Batch: 1769
Batch: 1770
Batch: 1771
Batch: 1772
Batch: 1773
Batch: 1774
Batch: 1775
Batch: 1776
Batch: 1777
Batch: 1778
Batch: 1779
Batch: 1780
Batch: 1781
Batch: 1782
Batch: 1783
Batch: 1784
Batch: 1785
Batch: 1786
Batch: 1787
Batch: 1788
Batch: 1789
Batch: 1790
Batch: 1791
Batch: 1792
Batch: 1793
Batch: 1794
Batch: 1795
Batch: 1796
Batch: 1797
Batch: 1798
Batch: 1799
Batch: 1800
Batch: 1801
Batch: 1802
Batch: 1803
Batch: 1804
Batch: 1805
Batch: 1806
Batch: 1807
Batch: 1808
Batch: 1809
Batch: 1810
Batch: 1811
Batch: 1812
Batch: 1813
Batch: 1814
Batch: 1815
Batch: 1816
Batch: 1817
Batch: 1818
Batch: 1819
Batch: 1820
Batch: 1821
Batch: 1822
Batch: 1823
Batch: 1824
Batch: 1825
Batch: 1826
Batch: 1827
Batch: 1828
Batch: 1829
Batch: 1830
Batch: 1831
Batch: 1832
Batch: 1833
Batch: 1834
Batch: 1835
Batch: 1836
Batch: 1837
Batch: 1838
Batch: 1839
Batch: 1840
Batch: 1841
Batch: 1842
Batch: 1843
Batch: 1844
Batch: 1845
Batch: 1846
Batch: 1847
Batch: 1848
Batch: 1849
Batch: 1850
Batch: 1851
Batch: 1852
Batch: 1853
Batch: 1854
Batch: 1855
Batch: 1856
Batch: 1857
Batch: 1858
Batch: 1859
Batch: 1860
Batch: 1861
Batch: 1862
Batch: 1863
Batch: 1864
Batch: 1865
Batch: 1866
Batch: 1867
Batch: 1868
Batch: 1869
Batch: 1870
Batch: 1871
Batch: 1872
Batch: 1873
Batch: 1874
Batch: 1875
Batch: 1876
Batch: 1877
Batch: 1878
Batch: 1879
Batch: 1880
Batch: 1881
Batch: 1882
Batch: 1883
Batch: 1884
Batch: 1885
Batch: 1886
Batch: 1887
Batch: 1888
Batch: 1889
Batch: 1890
Batch: 1891
Batch: 1892
Batch: 1893
Batch: 1894
Batch: 1895
Batch: 1896
Batch: 1897
Batch: 1898
Batch: 1899
Batch: 1900
Batch: 1901
Batch: 1902
Batch: 1903
Batch: 1904
Batch: 1905
Batch: 1906
Batch: 1907
Batch: 1908
Batch: 1909
Batch: 1910
Batch: 1911
Batch: 1912
Batch: 1913
Batch: 1914
Batch: 1915
Batch: 1916
Batch: 1917
Batch: 1918
Batch: 1919
Batch: 1920
Batch: 1921
Batch: 1922
Batch: 1923
Batch: 1924
Batch: 1925
Batch: 1926
Batch: 1927
Batch: 1928
Batch: 1929
Batch: 1930
Batch: 1931
Batch: 1932
Batch: 1933
Batch: 1934
Batch: 1935
Batch: 1936
Batch: 1937
Batch: 1938
Batch: 1939
Batch: 1940
Batch: 1941
Batch: 1942
Batch: 1943
Batch: 1944
Batch: 1945
Batch: 1946
Batch: 1947
Batch: 1948
Batch: 1949
Batch: 1950
Batch: 1951
Batch: 1952
Batch: 1953
Batch: 1954
Batch: 1955
Batch: 1956
Batch: 1957
Batch: 1958
Batch: 1959
Batch: 1960
Batch: 1961
Batch: 1962
Batch: 1963
Batch: 1964
Batch: 1965
Batch: 1966
Batch: 1967
Batch: 1968
Batch: 1969
Batch: 1970
Batch: 1971
Batch: 1972
Batch: 1973
Batch: 1974
Batch: 1975
Batch: 1976
Batch: 1977
Batch: 1978
Batch: 1979
Batch: 1980
Batch: 1981
Batch: 1982
Batch: 1983
Batch: 1984
Batch: 1985
Batch: 1986
Batch: 1987
Batch: 1988
Batch: 1989
Batch: 1990
Batch: 1991
Batch: 1992
Batch: 1993
Batch: 1994
Batch: 1995
Batch: 1996
Batch: 1997
Batch: 1998
Batch: 1999
Batch: 2000
Batch: 2001
Batch: 2002
Batch: 2003
Batch: 2004
Batch: 2005
Batch: 2006
Batch: 2007
Batch: 2008
Batch: 2009
Batch: 2010
Batch: 2011
Batch: 2012
Batch: 2013
Batch: 2014
Batch: 2015
Batch: 2016
Batch: 2017
Batch: 2018
Batch: 2019
Batch: 2020
Batch: 2021
Batch: 2022
Batch: 2023
Batch: 2024
Batch: 2025
Batch: 2026
Batch: 2027
Batch: 2028
Batch: 2029
Batch: 2030
Batch: 2031
Batch: 2032
Batch: 2033
Batch: 2034
Batch: 2035
Batch: 2036
Batch: 2037
Batch: 2038
Batch: 2039
Batch: 2040
Batch: 2041
Batch: 2042
Batch: 2043
Batch: 2044
Batch: 2045
Batch: 2046
Batch: 2047
Batch: 2048
Batch: 2049
Batch: 2050
Batch: 2051
Batch: 2052
Batch: 2053
Batch: 2054
Batch: 2055
Batch: 2056
Batch: 2057
Batch: 2058
Batch: 2059
Batch: 2060
Batch: 2061
Batch: 2062
Batch: 2063
Batch: 2064
Batch: 2065
Batch: 2066
Batch: 2067
Batch: 2068
Batch: 2069
Batch: 2070
Batch: 2071
Batch: 2072
Batch: 2073
Batch: 2074
Batch: 2075
Batch: 2076
Batch: 2077
Batch: 2078
Batch: 2079
Batch: 2080
Batch: 2081
Batch: 2082
Batch: 2083
Batch: 2084
Batch: 2085
Batch: 2086
Batch: 2087
Batch: 2088
Batch: 2089
Batch: 2090
Batch: 2091
Batch: 2092
Batch: 2093
Batch: 2094
Batch: 2095
Batch: 2096
Batch: 2097
Batch: 2098
Batch: 2099
Batch: 2100
Batch: 2101
Batch: 2102
Batch: 2103
Batch: 2104
Batch: 2105
Batch: 2106
Batch: 2107
Batch: 2108
Batch: 2109
Batch: 2110
Batch: 2111
Batch: 2112
Batch: 2113
Batch: 2114
Batch: 2115
Batch: 2116
Batch: 2117
Batch: 2118
Batch: 2119
Batch: 2120
Batch: 2121
Batch: 2122
Batch: 2123
Batch: 2124
Batch: 2125
Batch: 2126
Batch: 2127
Batch: 2128
Batch: 2129
Batch: 2130
Batch: 2131
Batch: 2132
Batch: 2133
Batch: 2134
Batch: 2135
Batch: 2136
Batch: 2137
Batch: 2138
Batch: 2139
Batch: 2140
Batch: 2141
Batch: 2142
Batch: 2143
Batch: 2144
Batch: 2145
Batch: 2146
Batch: 2147
Batch: 2148
Batch: 2149
Batch: 2150
Batch: 2151
Batch: 2152
Batch: 2153
Batch: 2154
Batch: 2155
Batch: 2156
Batch: 2157
Batch: 2158
Batch: 2159
Batch: 2160
Batch: 2161
Batch: 2162
Batch: 2163
Batch: 2164
Batch: 2165
Batch: 2166
Batch: 2167
Batch: 2168
Batch: 2169
Batch: 2170
Batch: 2171
Batch: 2172
Batch: 2173
Batch: 2174
Batch: 2175
Batch: 2176
Batch: 2177
Batch: 2178
Batch: 2179
Batch: 2180
Batch: 2181
Batch: 2182
Batch: 2183
Batch: 2184
Batch: 2185
Batch: 2186
Batch: 2187
Batch: 2188
Batch: 2189
Batch: 2190
Batch: 2191
Batch: 2192
Batch: 2193
Batch: 2194
Batch: 2195
Batch: 2196
Batch: 2197
Batch: 2198
Batch: 2199
Batch: 2200
Batch: 2201
Batch: 2202
Batch: 2203
Batch: 2204
Batch: 2205
Batch: 2206
Batch: 2207
Batch: 2208
Batch: 2209
Batch: 2210
Batch: 2211
Batch: 2212
Batch: 2213
Batch: 2214
Batch: 2215
Batch: 2216
Batch: 2217
Batch: 2218
Batch: 2219
Batch: 2220
Batch: 2221
Batch: 2222
Batch: 2223
Batch: 2224
Batch: 2225
Batch: 2226
Batch: 2227
Batch: 2228
Batch: 2229
Batch: 2230
Batch: 2231
Batch: 2232
Batch: 2233
Batch: 2234
Batch: 2235
Batch: 2236
Batch: 2237
Batch: 2238
Batch: 2239
Batch: 2240
Batch: 2241
Batch: 2242
Batch: 2243
Batch: 2244
Batch: 2245
Batch: 2246
Batch: 2247
Batch: 2248
Batch: 2249
Batch: 2250
Batch: 2251
Batch: 2252
Batch: 2253
Batch: 2254
Batch: 2255
Batch: 2256
Batch: 2257
Batch: 2258
Batch: 2259
Batch: 2260
Batch: 2261
Batch: 2262
Batch: 2263
Batch: 2264
Batch: 2265
Batch: 2266
Batch: 2267
Batch: 2268
Batch: 2269
Batch: 2270
Batch: 2271
Batch: 2272
Batch: 2273
Batch: 2274
Batch: 2275
Batch: 2276
Batch: 2277
Batch: 2278
Batch: 2279
Batch: 2280
Batch: 2281
Batch: 2282
Batch: 2283
Batch: 2284
Batch: 2285
Batch: 2286
Batch: 2287
Batch: 2288
Batch: 2289
Batch: 2290
Batch: 2291
Batch: 2292
Batch: 2293
Batch: 2294
Batch: 2295
Batch: 2296
Batch: 2297
Batch: 2298
Batch: 2299
Batch: 2300
Batch: 2301
Batch: 2302
Batch: 2303
Batch: 2304
Batch: 2305
Batch: 2306
Batch: 2307
Batch: 2308
Batch: 2309
Batch: 2310
Batch: 2311
Batch: 2312
Batch: 2313
Batch: 2314
Batch: 2315
Batch: 2316
Batch: 2317
Batch: 2318
Batch: 2319
Batch: 2320
Batch: 2321
Batch: 2322
Batch: 2323
Batch: 2324
Batch: 2325
Batch: 2326
Batch: 2327
Batch: 2328
Batch: 2329
Batch: 2330
Batch: 2331
Batch: 2332
Batch: 2333
Batch: 2334
Batch: 2335
Batch: 2336
Batch: 2337
Batch: 2338
Batch: 2339
Batch: 2340
Batch: 2341
Batch: 2342
Batch: 2343
Batch: 2344
Batch: 2345
Batch: 2346
Batch: 2347
Batch: 2348
Batch: 2349
Batch: 2350
Batch: 2351
Batch: 2352
Batch: 2353
Batch: 2354
Batch: 2355
Batch: 2356
Batch: 2357
Batch: 2358
Batch: 2359
Batch: 2360
Batch: 2361
Batch: 2362
Batch: 2363
Batch: 2364
Batch: 2365
Batch: 2366
Batch: 2367
Batch: 2368
Batch: 2369
Batch: 2370
Batch: 2371
Batch: 2372
Batch: 2373
Batch: 2374
Batch: 2375
Batch: 2376
Batch: 2377
Batch: 2378
Batch: 2379
Batch: 2380
Batch: 2381
Batch: 2382
Batch: 2383
Batch: 2384
Batch: 2385
Batch: 2386
Batch: 2387
Batch: 2388
Batch: 2389
Batch: 2390
Batch: 2391
Batch: 2392
Batch: 2393
Batch: 2394
Batch: 2395
Batch: 2396
Batch: 2397
Batch: 2398
Batch: 2399
Batch: 2400
Batch: 2401
Batch: 2402
Batch: 2403
Batch: 2404
Batch: 2405
Batch: 2406
Batch: 2407
Batch: 2408
Batch: 2409
Batch: 2410
Batch: 2411
Batch: 2412
Batch: 2413
Batch: 2414
Batch: 2415
Batch: 2416
Batch: 2417
Batch: 2418
Batch: 2419
Batch: 2420
Batch: 2421
Batch: 2422
Batch: 2423
Batch: 2424
Batch: 2425
Batch: 2426
Batch: 2427
Batch: 2428
Batch: 2429
Batch: 2430
Batch: 2431
Batch: 2432
Batch: 2433
Batch: 2434
Batch: 2435
Batch: 2436
Batch: 2437
Batch: 2438
Batch: 2439
Batch: 2440
Batch: 2441
Batch: 2442
Batch: 2443
Batch: 2444
Batch: 2445
Batch: 2446
Batch: 2447
Batch: 2448
Batch: 2449
Batch: 2450
Batch: 2451
Batch: 2452
Batch: 2453
Batch: 2454
Batch: 2455
Batch: 2456
Batch: 2457
Batch: 2458
Batch: 2459
Batch: 2460
Batch: 2461
Batch: 2462
Batch: 2463
Batch: 2464
Batch: 2465
Batch: 2466
Batch: 2467
Batch: 2468
Batch: 2469
Batch: 2470
Batch: 2471
Batch: 2472
Batch: 2473
Batch: 2474
Batch: 2475
Batch: 2476
Batch: 2477
Batch: 2478
Batch: 2479
Batch: 2480
Batch: 2481
Batch: 2482
Batch: 2483
Batch: 2484
Batch: 2485
Batch: 2486
Batch: 2487
Batch: 2488
Batch: 2489
Batch: 2490
Batch: 2491
Batch: 2492
Batch: 2493
Batch: 2494
Batch: 2495
Batch: 2496
Batch: 2497
Batch: 2498
Batch: 2499
Batch: 2500
Batch: 2501
Batch: 2502
Batch: 2503
Batch: 2504
Batch: 2505
Batch: 2506
Batch: 2507
Batch: 2508
Batch: 2509
Batch: 2510
Batch: 2511
Batch: 2512
Batch: 2513
Batch: 2514
Batch: 2515
Batch: 2516
Batch: 2517
Batch: 2518
Batch: 2519
Batch: 2520
Batch: 2521
Batch: 2522
Batch: 2523
Batch: 2524
Batch: 2525
Batch: 2526
Batch: 2527
Batch: 2528
Batch: 2529
Batch: 2530
Batch: 2531
Batch: 2532
Batch: 2533
Batch: 2534
Batch: 2535
Batch: 2536
Batch: 2537
Batch: 2538
Batch: 2539
Batch: 2540
Batch: 2541
Batch: 2542
Batch: 2543
Batch: 2544
Batch: 2545
Batch: 2546
Batch: 2547
Batch: 2548
Batch: 2549
Batch: 2550
Batch: 2551
Batch: 2552
Batch: 2553
Batch: 2554
Batch: 2555
Batch: 2556
Batch: 2557
Batch: 2558
Batch: 2559
Batch: 2560
Batch: 2561
Batch: 2562
Batch: 2563
Batch: 2564
Batch: 2565
Batch: 2566
Batch: 2567
Batch: 2568
Batch: 2569
Batch: 2570
Batch: 2571
Batch: 2572
Batch: 2573
Batch: 2574
Batch: 2575
Batch: 2576
Batch: 2577
Batch: 2578
Batch: 2579
Batch: 2580
Batch: 2581
Batch: 2582
Batch: 2583
Batch: 2584
Batch: 2585
Batch: 2586
Batch: 2587
Batch: 2588
Batch: 2589
Batch: 2590
Batch: 2591
Batch: 2592
Batch: 2593
Batch: 2594
Batch: 2595
Batch: 2596
Batch: 2597
Batch: 2598
Batch: 2599
Batch: 2600
Batch: 2601
Batch: 2602
Batch: 2603
Batch: 2604
Batch: 2605
Batch: 2606
Batch: 2607
Batch: 2608
Batch: 2609
Batch: 2610
Batch: 2611
Batch: 2612
Batch: 2613
Batch: 2614
Batch: 2615
Batch: 2616
Batch: 2617
Batch: 2618
Batch: 2619
Batch: 2620
Batch: 2621
Batch: 2622
Batch: 2623
Batch: 2624
Batch: 2625
Batch: 2626
Batch: 2627
Batch: 2628
Batch: 2629
Batch: 2630
Batch: 2631
Batch: 2632
Batch: 2633
Batch: 2634
Batch: 2635
Batch: 2636
Batch: 2637
Batch: 2638
Batch: 2639
Batch: 2640
Batch: 2641
Batch: 2642
Batch: 2643
Batch: 2644
Batch: 2645
Batch: 2646
Batch: 2647
Batch: 2648
Batch: 2649
Batch: 2650
Batch: 2651
Batch: 2652
Batch: 2653
Batch: 2654
Batch: 2655
Batch: 2656
Batch: 2657
Batch: 2658
Batch: 2659
Batch: 2660
Batch: 2661
Batch: 2662
Batch: 2663
Batch: 2664
Batch: 2665
Batch: 2666
Batch: 2667
Batch: 2668
Batch: 2669
Batch: 2670
Batch: 2671
Batch: 2672
Batch: 2673
Batch: 2674
Batch: 2675
Batch: 2676
Batch: 2677
Batch: 2678
Batch: 2679
Batch: 2680
Batch: 2681
Batch: 2682
Batch: 2683
Batch: 2684
Batch: 2685
Batch: 2686
Batch: 2687
Batch: 2688
Batch: 2689
Batch: 2690
Batch: 2691
Batch: 2692
Batch: 2693
Batch: 2694
Batch: 2695
Batch: 2696
Batch: 2697
Batch: 2698
Batch: 2699
Batch: 2700
Batch: 2701
Batch: 2702
Batch: 2703
Batch: 2704
Batch: 2705
Batch: 2706
Batch: 2707
Batch: 2708
Batch: 2709
Batch: 2710
Batch: 2711
Batch: 2712
Batch: 2713
Batch: 2714
Batch: 2715
Batch: 2716
Batch: 2717
Batch: 2718
Batch: 2719
Batch: 2720
Batch: 2721
Batch: 2722
Batch: 2723
Batch: 2724
Batch: 2725
Batch: 2726
Batch: 2727
Batch: 2728
Batch: 2729
Batch: 2730
Batch: 2731
Batch: 2732
Batch: 2733
Batch: 2734
Batch: 2735
Batch: 2736
Batch: 2737
Batch: 2738
Batch: 2739
Batch: 2740
Batch: 2741
Batch: 2742
Batch: 2743
Batch: 2744
Batch: 2745
Batch: 2746
Batch: 2747
Batch: 2748
Batch: 2749
Batch: 2750
Batch: 2751
Batch: 2752
Batch: 2753
Batch: 2754
Batch: 2755
Batch: 2756
Batch: 2757
Batch: 2758
Batch: 2759
Batch: 2760
Batch: 2761
Batch: 2762
Batch: 2763
Batch: 2764
Batch: 2765
Batch: 2766
Batch: 2767
Batch: 2768
Batch: 2769
Batch: 2770
Batch: 2771
Batch: 2772
Batch: 2773
Batch: 2774
Batch: 2775
Batch: 2776
Batch: 2777
Batch: 2778
Batch: 2779
Batch: 2780
Batch: 2781
Batch: 2782
Batch: 2783
Batch: 2784
Batch: 2785
Batch: 2786
Batch: 2787
Batch: 2788
Batch: 2789
Batch: 2790
Batch: 2791
Batch: 2792
Batch: 2793
Batch: 2794
Batch: 2795
Batch: 2796
Batch: 2797
Batch: 2798
Batch: 2799
Batch: 2800
Batch: 2801
Batch: 2802
Batch: 2803
Batch: 2804
Batch: 2805
Batch: 2806
Batch: 2807
Batch: 2808
Batch: 2809
Batch: 2810
Batch: 2811
Batch: 2812
Batch: 2813
Batch: 2814
Batch: 2815
Batch: 2816
Batch: 2817
Batch: 2818
Batch: 2819
Batch: 2820
Batch: 2821
Batch: 2822
Batch: 2823
Batch: 2824
Batch: 2825
Batch: 2826
Batch: 2827
Batch: 2828
Batch: 2829
Batch: 2830
Batch: 2831
Batch: 2832
Batch: 2833
Batch: 2834
Batch: 2835
Batch: 2836
Batch: 2837
Batch: 2838
Batch: 2839
Batch: 2840
Batch: 2841
Batch: 2842
Batch: 2843
Batch: 2844
Batch: 2845
Batch: 2846
Batch: 2847
Batch: 2848
Batch: 2849
Batch: 2850
Batch: 2851
Batch: 2852
Batch: 2853
Batch: 2854
Batch: 2855
Batch: 2856
Batch: 2857
Batch: 2858
Batch: 2859
Batch: 2860
Batch: 2861
Batch: 2862
Batch: 2863
Batch: 2864
Batch: 2865
Batch: 2866
Batch: 2867
Batch: 2868
Batch: 2869
Batch: 2870
Batch: 2871
Batch: 2872
Batch: 2873
Batch: 2874
Batch: 2875
Batch: 2876
Batch: 2877
Batch: 2878
Batch: 2879
Batch: 2880
Batch: 2881
Batch: 2882
Batch: 2883
Batch: 2884
Batch: 2885
Batch: 2886
Batch: 2887
Batch: 2888
Batch: 2889
Batch: 2890
Batch: 2891
Batch: 2892
Batch: 2893
Batch: 2894
Batch: 2895
Batch: 2896
Batch: 2897
Batch: 2898
Batch: 2899
Batch: 2900
Batch: 2901
Batch: 2902
Batch: 2903
Batch: 2904
Batch: 2905
Batch: 2906
Batch: 2907
Batch: 2908
Batch: 2909
Batch: 2910
Batch: 2911
Batch: 2912
Batch: 2913
Batch: 2914
Batch: 2915
Batch: 2916
Batch: 2917
Batch: 2918
Batch: 2919
Batch: 2920
Batch: 2921
Batch: 2922
Batch: 2923
Batch: 2924
Batch: 2925
Batch: 2926
Batch: 2927
Batch: 2928
Batch: 2929
Batch: 2930
Batch: 2931
Batch: 2932
Batch: 2933
Batch: 2934
Batch: 2935
Batch: 2936
Batch: 2937
Batch: 2938
Batch: 2939
Batch: 2940
Batch: 2941
Batch: 2942
Batch: 2943
Batch: 2944
Batch: 2945
Batch: 2946
Batch: 2947
Batch: 2948
Batch: 2949
Batch: 2950
Batch: 2951
Batch: 2952
Batch: 2953
Batch: 2954
Batch: 2955
Batch: 2956
Batch: 2957
Batch: 2958
Batch: 2959
Batch: 2960
Batch: 2961
Batch: 2962
Batch: 2963
Batch: 2964
Batch: 2965
Batch: 2966
Batch: 2967
Batch: 2968
Batch: 2969
Batch: 2970
Batch: 2971
Batch: 2972
Batch: 2973
Batch: 2974
Batch: 2975
Batch: 2976
Batch: 2977
Batch: 2978
Batch: 2979
Batch: 2980
Batch: 2981
Batch: 2982
Batch: 2983
Batch: 2984
Batch: 2985
Batch: 2986
Batch: 2987
Batch: 2988
Batch: 2989
Batch: 2990
Batch: 2991
Batch: 2992
Batch: 2993
Batch: 2994
Batch: 2995
Batch: 2996
Batch: 2997
Batch: 2998
Batch: 2999
Batch: 3000
Batch: 3001
Batch: 3002
Batch: 3003
Batch: 3004
Batch: 3005
Batch: 3006
Batch: 3007
Batch: 3008
Batch: 3009
Batch: 3010
Batch: 3011
Batch: 3012
Batch: 3013
Batch: 3014
Batch: 3015
Batch: 3016
Batch: 3017
Batch: 3018
Batch: 3019
Batch: 3020
Batch: 3021
Batch: 3022
Batch: 3023
Batch: 3024
Batch: 3025
Batch: 3026
Batch: 3027
Batch: 3028
Batch: 3029
Batch: 3030
Batch: 3031
Batch: 3032
Batch: 3033
Batch: 3034
Batch: 3035
Batch: 3036
Batch: 3037
Batch: 3038
Batch: 3039
Batch: 3040
Batch: 3041
Batch: 3042
Batch: 3043
Batch: 3044
Batch: 3045
Batch: 3046
Batch: 3047
Batch: 3048
Batch: 3049
Batch: 3050
Batch: 3051
Batch: 3052
Batch: 3053
Batch: 3054
Batch: 3055
Batch: 3056
Batch: 3057
Batch: 3058
Batch: 3059
Batch: 3060
Batch: 3061
Batch: 3062
Batch: 3063
Batch: 3064
Batch: 3065
Batch: 3066
Batch: 3067
Batch: 3068
Batch: 3069
Batch: 3070
Batch: 3071
Batch: 3072
Batch: 3073
Batch: 3074
Batch: 3075
Batch: 3076
Batch: 3077
Batch: 3078
Batch: 3079
Batch: 3080
Batch: 3081
Batch: 3082
Batch: 3083
Batch: 3084
Batch: 3085
Batch: 3086
Batch: 3087
Batch: 3088
Batch: 3089
Batch: 3090
Batch: 3091
Batch: 3092
Batch: 3093
Batch: 3094
Batch: 3095
Batch: 3096
Batch: 3097
Batch: 3098
Batch: 3099
Batch: 3100
Batch: 3101
Batch: 3102
Batch: 3103
Batch: 3104
Batch: 3105
Batch: 3106
Batch: 3107
Batch: 3108
Batch: 3109
Batch: 3110
Batch: 3111
Batch: 3112
Batch: 3113
Batch: 3114
Batch: 3115
Batch: 3116
Batch: 3117
Batch: 3118
Batch: 3119
Batch: 3120
Batch: 3121
Batch: 3122
Batch: 3123
Batch: 3124
Batch: 3125
Batch: 3126
Batch: 3127
Batch: 3128
Batch: 3129
Batch: 3130
Batch: 3131
Batch: 3132
Batch: 3133
Batch: 3134
Batch: 3135
Batch: 3136
Batch: 3137
Batch: 3138
Batch: 3139
Batch: 3140
Batch: 3141
Batch: 3142
Batch: 3143
Batch: 3144
Batch: 3145
Batch: 3146
Batch: 3147
Batch: 3148
Batch: 3149
Batch: 3150
Batch: 3151
Batch: 3152
Batch: 3153
Batch: 3154
Batch: 3155
Batch: 3156
Batch: 3157
Batch: 3158
Batch: 3159
Batch: 3160
Batch: 3161
Batch: 3162
Batch: 3163
Batch: 3164
Batch: 3165
Batch: 3166
Batch: 3167
Batch: 3168
Batch: 3169
Batch: 3170
Batch: 3171
Batch: 3172
Batch: 3173
Batch: 3174
Batch: 3175
Batch: 3176
Batch: 3177
Batch: 3178
Batch: 3179
Batch: 3180
Batch: 3181
Batch: 3182
Batch: 3183
Batch: 3184
Batch: 3185
Batch: 3186
Batch: 3187
Batch: 3188
Batch: 3189
Batch: 3190
Batch: 3191
Batch: 3192
Batch: 3193
Batch: 3194
Batch: 3195
Batch: 3196
Batch: 3197
Batch: 3198
Batch: 3199
Batch: 3200
Batch: 3201
Batch: 3202
Batch: 3203
Batch: 3204
Batch: 3205
Batch: 3206
Batch: 3207
Batch: 3208
Batch: 3209
Batch: 3210
Batch: 3211
Batch: 3212
Batch: 3213
Batch: 3214
Batch: 3215
Batch: 3216
Batch: 3217
Batch: 3218
Batch: 3219
Batch: 3220
Batch: 3221
Batch: 3222
Batch: 3223
Batch: 3224
Batch: 3225
Batch: 3226
Batch: 3227
Batch: 3228
Batch: 3229
Batch: 3230
Batch: 3231
Batch: 3232
Batch: 3233
Batch: 3234
Batch: 3235
Batch: 3236
Batch: 3237
Batch: 3238
Batch: 3239
Batch: 3240
Batch: 3241
Batch: 3242
Batch: 3243
Batch: 3244
Batch: 3245
Batch: 3246
Batch: 3247
Batch: 3248
Batch: 3249
Batch: 3250
Batch: 3251
Batch: 3252
Batch: 3253
Batch: 3254
Batch: 3255
Batch: 3256
Batch: 3257
Batch: 3258
Batch: 3259
Batch: 3260
Batch: 3261
Batch: 3262
Batch: 3263
Batch: 3264
Batch: 3265
Batch: 3266
Batch: 3267
Batch: 3268
Batch: 3269
Batch: 3270
Batch: 3271
Batch: 3272
Batch: 3273
Batch: 3274
Batch: 3275
Batch: 3276
Batch: 3277
Batch: 3278
Batch: 3279
Batch: 3280
Batch: 3281
Batch: 3282
Batch: 3283
Batch: 3284
Batch: 3285
Batch: 3286
Batch: 3287
Batch: 3288
Batch: 3289
Batch: 3290
Batch: 3291
Batch: 3292
Batch: 3293
Batch: 3294
Batch: 3295
Batch: 3296
Batch: 3297
Batch: 3298
Batch: 3299
Batch: 3300
Batch: 3301
Batch: 3302
Batch: 3303
Batch: 3304
Batch: 3305
Batch: 3306
Batch: 3307
Batch: 3308
Batch: 3309
Batch: 3310
Batch: 3311
Batch: 3312
Batch: 3313
Batch: 3314
Batch: 3315
Batch: 3316
Batch: 3317
Batch: 3318
Batch: 3319
Batch: 3320
Batch: 3321
Batch: 3322
Batch: 3323
Batch: 3324
Batch: 3325
Batch: 3326
Batch: 3327
Batch: 3328
Batch: 3329
Batch: 3330
Batch: 3331
Batch: 3332
Batch: 3333
Batch: 3334
Batch: 3335
Batch: 3336
Batch: 3337
Batch: 3338
Batch: 3339
Batch: 3340
Batch: 3341
Batch: 3342
Batch: 3343
Batch: 3344
Batch: 3345
Batch: 3346
Batch: 3347
Batch: 3348
Batch: 3349
Batch: 3350
Batch: 3351
Batch: 3352
Batch: 3353
Batch: 3354
Batch: 3355
Batch: 3356
Batch: 3357
Batch: 3358
Batch: 3359
Batch: 3360
Batch: 3361
Batch: 3362
Batch: 3363
Batch: 3364
Batch: 3365
Batch: 3366
Batch: 3367
Batch: 3368
Batch: 3369
Batch: 3370
Batch: 3371
Batch: 3372
Batch: 3373
Batch: 3374
Batch: 3375
Batch: 3376
Batch: 3377
Batch: 3378
Batch: 3379
Batch: 3380
Batch: 3381
Batch: 3382
Batch: 3383
Batch: 3384
Batch: 3385
Batch: 3386
Batch: 3387
Batch: 3388
Batch: 3389
Batch: 3390
Batch: 3391
Batch: 3392
Batch: 3393
Batch: 3394
Batch: 3395
Batch: 3396
Batch: 3397
Batch: 3398
Batch: 3399
Batch: 3400
Batch: 3401
Batch: 3402
Batch: 3403
Batch: 3404
Batch: 3405
Batch: 3406
Batch: 3407
Batch: 3408
Batch: 3409
Batch: 3410
Batch: 3411
Batch: 3412
Batch: 3413
Batch: 3414
Batch: 3415
Batch: 3416
Batch: 3417
Batch: 3418
Batch: 3419
Batch: 3420
Batch: 3421
Batch: 3422
Batch: 3423
Batch: 3424
Batch: 3425
Batch: 3426
Batch: 3427
Batch: 3428
Batch: 3429
Batch: 3430
Batch: 3431
Batch: 3432
Batch: 3433
Batch: 3434
Batch: 3435
Batch: 3436
Batch: 3437
Batch: 3438
Batch: 3439
Batch: 3440
Batch: 3441
Batch: 3442
Batch: 3443
Batch: 3444
Batch: 3445
Batch: 3446
Batch: 3447
Batch: 3448
Batch: 3449
Batch: 3450
Batch: 3451
Batch: 3452
Batch: 3453
Batch: 3454
Batch: 3455
Batch: 3456
Batch: 3457
Batch: 3458
Batch: 3459
Batch: 3460
Batch: 3461
Batch: 3462
Batch: 3463
Batch: 3464
Batch: 3465
Batch: 3466
Batch: 3467
Batch: 3468
Batch: 3469
Batch: 3470
Batch: 3471
Batch: 3472
Batch: 3473
Batch: 3474
Batch: 3475
Batch: 3476
Batch: 3477
Batch: 3478
Batch: 3479
Batch: 3480
Batch: 3481
Batch: 3482
Batch: 3483
Batch: 3484
Batch: 3485
Batch: 3486
Batch: 3487
Batch: 3488
Batch: 3489
Batch: 3490
Batch: 3491
Batch: 3492
Batch: 3493
Batch: 3494
Batch: 3495
Batch: 3496
Batch: 3497
Batch: 3498
Batch: 3499
Batch: 3500
Batch: 3501
Batch: 3502
Batch: 3503
Batch: 3504
Batch: 3505
Batch: 3506
Batch: 3507
Batch: 3508
Batch: 3509
Batch: 3510
Batch: 3511
Batch: 3512
Batch: 3513
Batch: 3514
Batch: 3515
Batch: 3516
Batch: 3517
Batch: 3518
Batch: 3519
Batch: 3520
Batch: 3521
Batch: 3522
Batch: 3523
Batch: 3524
Batch: 3525
Batch: 3526
Batch: 3527
Batch: 3528
Batch: 3529
Batch: 3530
Batch: 3531
Batch: 3532
Batch: 3533
Batch: 3534
Batch: 3535
Batch: 3536
Batch: 3537
Batch: 3538
Batch: 3539
Batch: 3540
Batch: 3541
Batch: 3542
Batch: 3543
Batch: 3544
Batch: 3545
Batch: 3546
Batch: 3547
Batch: 3548
Batch: 3549
Batch: 3550
Batch: 3551
Batch: 3552
Batch: 3553
Batch: 3554
Batch: 3555
Batch: 3556
Batch: 3557
Batch: 3558
Batch: 3559
Batch: 3560
Batch: 3561
Batch: 3562
Batch: 3563
Batch: 3564
Batch: 3565
Batch: 3566
Batch: 3567
Batch: 3568
Batch: 3569
Batch: 3570
Batch: 3571
Batch: 3572
Batch: 3573
Batch: 3574
Batch: 3575
Batch: 3576
Batch: 3577
Batch: 3578
Batch: 3579
Batch: 3580
Batch: 3581
Batch: 3582
Batch: 3583
Batch: 3584
Batch: 3585
Batch: 3586
Batch: 3587
Batch: 3588
Batch: 3589
Batch: 3590
Batch: 3591
Batch: 3592
Batch: 3593
Batch: 3594
Batch: 3595
Batch: 3596
Batch: 3597
Batch: 3598
Batch: 3599
Batch: 3600
Batch: 3601
Batch: 3602
Batch: 3603
Batch: 3604
Batch: 3605
Batch: 3606
Batch: 3607
Batch: 3608
Batch: 3609
Batch: 3610
Batch: 3611
Batch: 3612
Batch: 3613
Batch: 3614
Batch: 3615
Batch: 3616
Batch: 3617
Batch: 3618
Batch: 3619
Batch: 3620
Batch: 3621
Batch: 3622
Batch: 3623
Batch: 3624
Batch: 3625
Batch: 3626
Batch: 3627
Batch: 3628
Batch: 3629
Batch: 3630
Batch: 3631
Batch: 3632
Batch: 3633
Batch: 3634
Batch: 3635
Batch: 3636
Batch: 3637
Batch: 3638
Batch: 3639
Batch: 3640
Batch: 3641
Batch: 3642
Batch: 3643
Batch: 3644
Batch: 3645
Batch: 3646
Batch: 3647
Batch: 3648
Batch: 3649
Batch: 3650
Batch: 3651
Batch: 3652
Batch: 3653
Batch: 3654
Batch: 3655
Batch: 3656
Batch: 3657
Batch: 3658
Batch: 3659
Batch: 3660
Batch: 3661
Batch: 3662
Batch: 3663
Batch: 3664
Batch: 3665
Batch: 3666
Batch: 3667
Batch: 3668
Batch: 3669
Batch: 3670
Batch: 3671
Batch: 3672
Batch: 3673
Batch: 3674
Batch: 3675
Batch: 3676
Batch: 3677
Batch: 3678
Batch: 3679
Batch: 3680
Batch: 3681
Batch: 3682
Batch: 3683
Batch: 3684
Batch: 3685
Batch: 3686
Batch: 3687
Batch: 3688
Batch: 3689
Batch: 3690
Batch: 3691
Batch: 3692
Batch: 3693
Batch: 3694
Batch: 3695
Batch: 3696
Batch: 3697
Batch: 3698
Batch: 3699
Batch: 3700
Batch: 3701
Batch: 3702
Batch: 3703
Batch: 3704
Batch: 3705
Batch: 3706
Batch: 3707
Batch: 3708
Batch: 3709
Batch: 3710
Batch: 3711
Batch: 3712
Batch: 3713
Batch: 3714
Batch: 3715
Batch: 3716
Batch: 3717
Batch: 3718
Batch: 3719
Batch: 3720
Batch: 3721
Batch: 3722
Batch: 3723
Batch: 3724
Batch: 3725
Batch: 3726
Batch: 3727
Batch: 3728
Batch: 3729
Batch: 3730
Batch: 3731
Batch: 3732
Batch: 3733
Batch: 3734
Batch: 3735
Batch: 3736
Batch: 3737
Batch: 3738
Batch: 3739
Batch: 3740
Batch: 3741
Batch: 3742
Batch: 3743
Batch: 3744
Batch: 3745
Batch: 3746
Batch: 3747
Batch: 3748
Batch: 3749
Batch: 3750
Batch: 3751
Batch: 3752
Batch: 3753
Batch: 3754
Batch: 3755
Batch: 3756
Batch: 3757
Batch: 3758
Batch: 3759
Batch: 3760
Batch: 3761
Batch: 3762
Batch: 3763
Batch: 3764
Batch: 3765
Batch: 3766
Batch: 3767
Batch: 3768
Batch: 3769
Batch: 3770
Batch: 3771
Batch: 3772
Batch: 3773
Batch: 3774
Batch: 3775
Batch: 3776
Batch: 3777
Batch: 3778
Batch: 3779
Batch: 3780
Batch: 3781
Batch: 3782
Batch: 3783
Batch: 3784
Batch: 3785
Batch: 3786
Batch: 3787
Batch: 3788
Batch: 3789
Batch: 3790
Batch: 3791
Batch: 3792
Batch: 3793
Batch: 3794
Batch: 3795
Batch: 3796
Batch: 3797
Batch: 3798
Batch: 3799
Batch: 3800
Batch: 3801
Batch: 3802
Batch: 3803
Batch: 3804
Batch: 3805
Batch: 3806
Batch: 3807
Batch: 3808
Batch: 3809
Batch: 3810
Batch: 3811
Batch: 3812
Batch: 3813
Batch: 3814
Batch: 3815
Batch: 3816
Batch: 3817
Batch: 3818
Batch: 3819
Batch: 3820
Batch: 3821
Batch: 3822
Batch: 3823
Batch: 3824
Batch: 3825
Batch: 3826
Batch: 3827
Batch: 3828
Batch: 3829
Batch: 3830
Batch: 3831
Batch: 3832
Batch: 3833
Batch: 3834
Batch: 3835
Batch: 3836
Batch: 3837
Batch: 3838
Batch: 3839
Batch: 3840
Batch: 3841
Batch: 3842
Batch: 3843
Batch: 3844
Batch: 3845
Batch: 3846
Batch: 3847
Batch: 3848
Batch: 3849
Batch: 3850
Batch: 3851
Batch: 3852
Batch: 3853
Batch: 3854
Batch: 3855
Batch: 3856
Batch: 3857
Batch: 3858
Batch: 3859
Batch: 3860
Batch: 3861
Batch: 3862
Batch: 3863
Batch: 3864
Batch: 3865
Batch: 3866
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000002
	speed: 0.1710s/iter; left time: 6597.0598s
	iters: 200, epoch: 1 | loss: 0.0000001
	speed: 0.1698s/iter; left time: 6531.8351s
	iters: 300, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 6526.7420s
	iters: 400, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6502.7172s
	iters: 500, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6482.3647s
	iters: 600, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6466.5727s
	iters: 700, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6451.0108s
	iters: 800, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6432.1347s
	iters: 900, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6415.8710s
	iters: 1000, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6396.7005s
	iters: 1100, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6379.0245s
	iters: 1200, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6362.6675s
	iters: 1300, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6345.7650s
	iters: 1400, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6329.9625s
	iters: 1500, epoch: 1 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 6320.1364s
	iters: 1600, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6296.6028s
	iters: 1700, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6279.2905s
	iters: 1800, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6259.9518s
	iters: 1900, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6242.2111s
	iters: 2000, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6227.9196s
	iters: 2100, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6209.1455s
	iters: 2200, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6194.3603s
	iters: 2300, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6177.9177s
	iters: 2400, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6160.2524s
	iters: 2500, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6143.9660s
	iters: 2600, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6128.2033s
	iters: 2700, epoch: 1 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 6117.7448s
	iters: 2800, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6094.6740s
	iters: 2900, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6077.1753s
	iters: 3000, epoch: 1 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 6060.1930s
	iters: 3100, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6038.4763s
	iters: 3200, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6022.3086s
	iters: 3300, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 6005.3014s
	iters: 3400, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5987.7562s
	iters: 3500, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5969.8326s
	iters: 3600, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5953.1157s
	iters: 3700, epoch: 1 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5935.5892s
	iters: 3800, epoch: 1 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5920.2992s
Epoch: 1 cost time: 656.9317133426666
Epoch: 1, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000000
	speed: 4.4517s/iter; left time: 154492.4806s
	iters: 200, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5875.3414s
	iters: 300, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5858.7869s
	iters: 400, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5841.2833s
	iters: 500, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5824.2982s
	iters: 600, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5807.2762s
	iters: 700, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5790.7250s
	iters: 800, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5773.8344s
	iters: 900, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5755.9181s
	iters: 1000, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5739.7706s
	iters: 1100, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5722.4670s
	iters: 1200, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5705.0464s
	iters: 1300, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5696.7000s
	iters: 1400, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5672.9089s
	iters: 1500, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5654.1710s
	iters: 1600, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5637.6157s
	iters: 1700, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5619.6905s
	iters: 1800, epoch: 2 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5605.9215s
	iters: 1900, epoch: 2 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5589.2821s
	iters: 2000, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5569.6313s
	iters: 2100, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5551.5767s
	iters: 2200, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5535.7601s
	iters: 2300, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5520.6048s
	iters: 2400, epoch: 2 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5504.2603s
	iters: 2500, epoch: 2 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5491.7619s
	iters: 2600, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5467.6572s
	iters: 2700, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5449.9200s
	iters: 2800, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5433.8263s
	iters: 2900, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5418.4009s
	iters: 3000, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5401.8012s
	iters: 3100, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5382.2177s
	iters: 3200, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5366.0816s
	iters: 3300, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5348.2752s
	iters: 3400, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5331.8927s
	iters: 3500, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5314.9874s
	iters: 3600, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5297.6409s
	iters: 3700, epoch: 2 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 5285.4172s
	iters: 3800, epoch: 2 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 5263.0275s
Epoch: 2 cost time: 656.8127388954163
Epoch: 2, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000000
	speed: 4.4510s/iter; left time: 137254.8240s
	iters: 200, epoch: 3 | loss: 0.0000000
	speed: 0.1696s/iter; left time: 5212.9992s
	iters: 300, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5199.0455s
	iters: 400, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5183.0774s
	iters: 500, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5165.2445s
	iters: 600, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5149.1549s
	iters: 700, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5131.4766s
	iters: 800, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5114.3251s
	iters: 900, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5097.5879s
	iters: 1000, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 5088.9235s
	iters: 1100, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5062.6574s
	iters: 1200, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5046.1846s
	iters: 1300, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5029.7261s
	iters: 1400, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 5012.2934s
	iters: 1500, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4995.6506s
	iters: 1600, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4978.8499s
	iters: 1700, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4963.3090s
	iters: 1800, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4945.2169s
	iters: 1900, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4928.2356s
	iters: 2000, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4914.8304s
	iters: 2100, epoch: 3 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4903.0767s
	iters: 2200, epoch: 3 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 4887.3225s
	iters: 2300, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4864.4486s
	iters: 2400, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4848.3804s
	iters: 2500, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4830.8946s
	iters: 2600, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4813.2082s
	iters: 2700, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4794.7777s
	iters: 2800, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4779.9076s
	iters: 2900, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4761.3944s
	iters: 3000, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4742.4838s
	iters: 3100, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4724.4821s
	iters: 3200, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4707.6551s
	iters: 3300, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4692.8998s
	iters: 3400, epoch: 3 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4678.6582s
	iters: 3500, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4656.9272s
	iters: 3600, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4641.4058s
	iters: 3700, epoch: 3 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4624.7551s
	iters: 3800, epoch: 3 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 4606.4233s
Epoch: 3 cost time: 656.6689405441284
Epoch: 3, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000000
	speed: 4.4501s/iter; left time: 120018.4786s
	iters: 200, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4563.5585s
	iters: 300, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4545.6649s
	iters: 400, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4527.8201s
	iters: 500, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4511.2278s
	iters: 600, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4494.0594s
	iters: 700, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4477.0667s
	iters: 800, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4466.2374s
	iters: 900, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4443.8219s
	iters: 1000, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4426.0637s
	iters: 1100, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4408.7123s
	iters: 1200, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4393.5804s
	iters: 1300, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4377.1399s
	iters: 1400, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4361.5175s
	iters: 1500, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4344.0414s
	iters: 1600, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4327.2520s
	iters: 1700, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4309.9465s
	iters: 1800, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4293.0659s
	iters: 1900, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4274.8001s
	iters: 2000, epoch: 4 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 4261.3630s
	iters: 2100, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4239.1589s
	iters: 2200, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4222.8719s
	iters: 2300, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4205.1809s
	iters: 2400, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4188.4884s
	iters: 2500, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4171.5457s
	iters: 2600, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4154.3438s
	iters: 2700, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4137.6966s
	iters: 2800, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4120.8567s
	iters: 2900, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4104.4360s
	iters: 3000, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4087.0390s
	iters: 3100, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4070.0361s
	iters: 3200, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 4056.6328s
	iters: 3300, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4035.9153s
	iters: 3400, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4019.6160s
	iters: 3500, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 4002.4782s
	iters: 3600, epoch: 4 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3985.0704s
	iters: 3700, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3970.3154s
	iters: 3800, epoch: 4 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3953.7853s
Epoch: 4 cost time: 656.8781278133392
Epoch: 4, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000000
	speed: 4.4510s/iter; left time: 102830.4792s
	iters: 200, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3906.5929s
	iters: 300, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3888.0564s
	iters: 400, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3876.3643s
	iters: 500, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3854.6216s
	iters: 600, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3837.1604s
	iters: 700, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3821.7744s
	iters: 800, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3804.7027s
	iters: 900, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3785.6794s
	iters: 1000, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3768.6591s
	iters: 1100, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3752.1296s
	iters: 1200, epoch: 5 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3734.4674s
	iters: 1300, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3718.0615s
	iters: 1400, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3701.2320s
	iters: 1500, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3684.7263s
	iters: 1600, epoch: 5 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 3673.1882s
	iters: 1700, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3651.4683s
	iters: 1800, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3634.5597s
	iters: 1900, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3617.4158s
	iters: 2000, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3600.4023s
	iters: 2100, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3582.9929s
	iters: 2200, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3566.0403s
	iters: 2300, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3550.0133s
	iters: 2400, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3532.8821s
	iters: 2500, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3517.1630s
	iters: 2600, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3500.7554s
	iters: 2700, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3483.3305s
	iters: 2800, epoch: 5 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 3469.6423s
	iters: 2900, epoch: 5 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3447.1015s
	iters: 3000, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3432.7292s
	iters: 3100, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3415.2446s
	iters: 3200, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3397.6728s
	iters: 3300, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3381.6062s
	iters: 3400, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3364.5047s
	iters: 3500, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3347.3395s
	iters: 3600, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3331.3682s
	iters: 3700, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3314.3138s
	iters: 3800, epoch: 5 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3297.2969s
Epoch: 5 cost time: 656.9485967159271
Epoch: 5, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000000
	speed: 4.4503s/iter; left time: 85606.7704s
	iters: 200, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3248.9356s
	iters: 300, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3231.1114s
	iters: 400, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3214.0084s
	iters: 500, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3197.1979s
	iters: 600, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3180.4991s
	iters: 700, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3163.5273s
	iters: 800, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3146.9214s
	iters: 900, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 3129.6955s
	iters: 1000, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3112.1851s
	iters: 1100, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3098.6290s
	iters: 1200, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3078.5532s
	iters: 1300, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 3065.2050s
	iters: 1400, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3044.4229s
	iters: 1500, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3027.1358s
	iters: 1600, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 3010.5310s
	iters: 1700, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2993.6828s
	iters: 1800, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2977.1167s
	iters: 1900, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2958.8163s
	iters: 2000, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2942.7491s
	iters: 2100, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2925.5156s
	iters: 2200, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2908.5868s
	iters: 2300, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2891.3161s
	iters: 2400, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2875.3385s
	iters: 2500, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2860.9279s
	iters: 2600, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2842.9087s
	iters: 2700, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2825.8060s
	iters: 2800, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2807.4520s
	iters: 2900, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2791.3832s
	iters: 3000, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2775.1284s
	iters: 3100, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2758.9018s
	iters: 3200, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2740.6581s
	iters: 3300, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2723.8367s
	iters: 3400, epoch: 6 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2706.9609s
	iters: 3500, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2687.6413s
	iters: 3600, epoch: 6 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2671.0324s
	iters: 3700, epoch: 6 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2657.5704s
	iters: 3800, epoch: 6 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2638.0212s
Epoch: 6 cost time: 656.750333070755
Epoch: 6, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000000
	speed: 4.4470s/iter; left time: 68346.0798s
	iters: 200, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2592.7980s
	iters: 300, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2574.9665s
	iters: 400, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2557.8670s
	iters: 500, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2540.5345s
	iters: 600, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2524.2550s
	iters: 700, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2507.0009s
	iters: 800, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2489.7321s
	iters: 900, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2472.5503s
	iters: 1000, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2455.3695s
	iters: 1100, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2442.6411s
	iters: 1200, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2422.1089s
	iters: 1300, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2405.3486s
	iters: 1400, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2387.7308s
	iters: 1500, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2371.5696s
	iters: 1600, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2354.8493s
	iters: 1700, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2337.3961s
	iters: 1800, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2320.1698s
	iters: 1900, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2303.3461s
	iters: 2000, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2286.6431s
	iters: 2100, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2269.4077s
	iters: 2200, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2252.5014s
	iters: 2300, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2238.5166s
	iters: 2400, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2218.8642s
	iters: 2500, epoch: 7 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 2203.4737s
	iters: 2600, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2185.1798s
	iters: 2700, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2167.7071s
	iters: 2800, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2151.1663s
	iters: 2900, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2134.1087s
	iters: 3000, epoch: 7 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 2116.3035s
	iters: 3100, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2099.6956s
	iters: 3200, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2083.2171s
	iters: 3300, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2066.0344s
	iters: 3400, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2049.0588s
	iters: 3500, epoch: 7 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 2034.3206s
	iters: 3600, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 2015.3942s
	iters: 3700, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1998.1218s
	iters: 3800, epoch: 7 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1981.3862s
Epoch: 7 cost time: 656.6995980739594
Epoch: 7, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000000
	speed: 4.4503s/iter; left time: 51187.4949s
	iters: 200, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1936.4262s
	iters: 300, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1919.0209s
	iters: 400, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1901.9959s
	iters: 500, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1884.8223s
	iters: 600, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1868.1429s
	iters: 700, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1853.8209s
	iters: 800, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1834.0624s
	iters: 900, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1817.3555s
	iters: 1000, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1800.1242s
	iters: 1100, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1783.2078s
	iters: 1200, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1766.2882s
	iters: 1300, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1748.9869s
	iters: 1400, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1731.9517s
	iters: 1500, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1714.8953s
	iters: 1600, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1698.1379s
	iters: 1700, epoch: 8 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1680.8093s
	iters: 1800, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1664.2947s
	iters: 1900, epoch: 8 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1649.4005s
	iters: 2000, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1631.3047s
	iters: 2100, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1614.2698s
	iters: 2200, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1597.4634s
	iters: 2300, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1579.6488s
	iters: 2400, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1562.9170s
	iters: 2500, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1545.4642s
	iters: 2600, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1528.2725s
	iters: 2700, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1511.3685s
	iters: 2800, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1494.7160s
	iters: 2900, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1477.7217s
	iters: 3000, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1460.9420s
	iters: 3100, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1444.5788s
	iters: 3200, epoch: 8 | loss: 0.0000000
	speed: 0.1701s/iter; left time: 1429.3154s
	iters: 3300, epoch: 8 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1410.0682s
	iters: 3400, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1393.3691s
	iters: 3500, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1376.5154s
	iters: 3600, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1359.4133s
	iters: 3700, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1342.3975s
	iters: 3800, epoch: 8 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 1325.2980s
Epoch: 8 cost time: 656.9243953227997
Epoch: 8, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000000
	speed: 4.4549s/iter; left time: 34013.3133s
	iters: 200, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1279.2013s
	iters: 300, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1262.1648s
	iters: 400, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1247.0867s
	iters: 500, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1228.5038s
	iters: 600, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1211.3678s
	iters: 700, epoch: 9 | loss: 0.0000000
	speed: 0.1697s/iter; left time: 1194.1604s
	iters: 800, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1177.2815s
	iters: 900, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1160.2945s
	iters: 1000, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1143.4577s
	iters: 1100, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1126.4414s
	iters: 1200, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1109.3765s
	iters: 1300, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1092.5381s
	iters: 1400, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1075.6667s
	iters: 1500, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1058.4968s
	iters: 1600, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 1042.7242s
	iters: 1700, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1024.7165s
	iters: 1800, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 1007.7306s
	iters: 1900, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 990.6392s
	iters: 2000, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 973.7318s
	iters: 2100, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 956.7879s
	iters: 2200, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 939.9501s
	iters: 2300, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 923.0116s
	iters: 2400, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 906.1236s
	iters: 2500, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 889.1474s
	iters: 2600, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 872.1547s
	iters: 2700, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 855.0946s
	iters: 2800, epoch: 9 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 839.0504s
	iters: 2900, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 821.3194s
	iters: 3000, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 804.0647s
	iters: 3100, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 786.8415s
	iters: 3200, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 769.8584s
	iters: 3300, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 753.0029s
	iters: 3400, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 735.9602s
	iters: 3500, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 718.9824s
	iters: 3600, epoch: 9 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 702.1280s
	iters: 3700, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 685.4184s
	iters: 3800, epoch: 9 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 668.3667s
Epoch: 9 cost time: 656.801365852356
Epoch: 9, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000000
	speed: 4.4489s/iter; left time: 16763.6178s
	iters: 200, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 622.9593s
	iters: 300, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 605.9630s
	iters: 400, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 588.9319s
	iters: 500, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 571.9378s
	iters: 600, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 554.9201s
	iters: 700, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 537.9633s
	iters: 800, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 520.9584s
	iters: 900, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 503.9682s
	iters: 1000, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 486.9642s
	iters: 1100, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 469.9683s
	iters: 1200, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 452.9919s
	iters: 1300, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 436.0837s
	iters: 1400, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 419.6428s
	iters: 1500, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 402.0572s
	iters: 1600, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 385.0930s
	iters: 1700, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 368.2612s
	iters: 1800, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 351.3889s
	iters: 1900, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 334.2733s
	iters: 2000, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 317.3034s
	iters: 2100, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 300.4269s
	iters: 2200, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 283.2385s
	iters: 2300, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 266.2426s
	iters: 2400, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 249.4145s
	iters: 2500, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 232.2748s
	iters: 2600, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 215.5369s
	iters: 2700, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 198.3056s
	iters: 2800, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 181.3041s
	iters: 2900, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 164.4336s
	iters: 3000, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 147.4569s
	iters: 3100, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 130.4210s
	iters: 3200, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 113.4375s
	iters: 3300, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 96.4465s
	iters: 3400, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 79.4647s
	iters: 3500, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 62.5070s
	iters: 3600, epoch: 10 | loss: 0.0000000
	speed: 0.1699s/iter; left time: 45.5247s
	iters: 3700, epoch: 10 | loss: 0.0000000
	speed: 0.1698s/iter; left time: 28.5296s
	iters: 3800, epoch: 10 | loss: 0.0000000
	speed: 0.1700s/iter; left time: 11.5591s
Epoch: 10 cost time: 656.9120786190033
Epoch: 10, Steps: 3867 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 449820
test: (449919, 51)
train: (495000, 51)
train 494901
Threshold : 2.0306485417620498e-08
pred:    (44982000,)
gt:      (44982000,)
pred:  (44982000,)
gt:    (44982000,)
Accuracy : 0.9642, Precision : 0.8540, Recall : 0.8510, F-score : 0.8525, AUC : 0.9154
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=25, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='PSM', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=25, factor=1, feature_epochs=1, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='PSM', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, root_path='./all_datasets/PSM', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : PSM_GPT4TS_PSM_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (87841, 25)
train: (132481, 25)
train 132382
test: (87841, 25)
train: (132481, 25)
val 87742
test: (87841, 25)
train: (132481, 25)
test 87742
Start train feature encoder...
Epoch: 0
Batch: 0
Batch: 1
Batch: 2
Batch: 3
Batch: 4
Batch: 5
Batch: 6
Batch: 7
Batch: 8
Batch: 9
Batch: 10
Batch: 11
Batch: 12
Batch: 13
Batch: 14
Batch: 15
Batch: 16
Batch: 17
Batch: 18
Batch: 19
Batch: 20
Batch: 21
Batch: 22
Batch: 23
Batch: 24
Batch: 25
Batch: 26
Batch: 27
Batch: 28
Batch: 29
Batch: 30
Batch: 31
Batch: 32
Batch: 33
Batch: 34
Batch: 35
Batch: 36
Batch: 37
Batch: 38
Batch: 39
Batch: 40
Batch: 41
Batch: 42
Batch: 43
Batch: 44
Batch: 45
Batch: 46
Batch: 47
Batch: 48
Batch: 49
Batch: 50
Batch: 51
Batch: 52
Batch: 53
Batch: 54
Batch: 55
Batch: 56
Batch: 57
Batch: 58
Batch: 59
Batch: 60
Batch: 61
Batch: 62
Batch: 63
Batch: 64
Batch: 65
Batch: 66
Batch: 67
Batch: 68
Batch: 69
Batch: 70
Batch: 71
Batch: 72
Batch: 73
Batch: 74
Batch: 75
Batch: 76
Batch: 77
Batch: 78
Batch: 79
Batch: 80
Batch: 81
Batch: 82
Batch: 83
Batch: 84
Batch: 85
Batch: 86
Batch: 87
Batch: 88
Batch: 89
Batch: 90
Batch: 91
Batch: 92
Batch: 93
Batch: 94
Batch: 95
Batch: 96
Batch: 97
Batch: 98
Batch: 99
Batch: 100
Batch: 101
Batch: 102
Batch: 103
Batch: 104
Batch: 105
Batch: 106
Batch: 107
Batch: 108
Batch: 109
Batch: 110
Batch: 111
Batch: 112
Batch: 113
Batch: 114
Batch: 115
Batch: 116
Batch: 117
Batch: 118
Batch: 119
Batch: 120
Batch: 121
Batch: 122
Batch: 123
Batch: 124
Batch: 125
Batch: 126
Batch: 127
Batch: 128
Batch: 129
Batch: 130
Batch: 131
Batch: 132
Batch: 133
Batch: 134
Batch: 135
Batch: 136
Batch: 137
Batch: 138
Batch: 139
Batch: 140
Batch: 141
Batch: 142
Batch: 143
Batch: 144
Batch: 145
Batch: 146
Batch: 147
Batch: 148
Batch: 149
Batch: 150
Batch: 151
Batch: 152
Batch: 153
Batch: 154
Batch: 155
Batch: 156
Batch: 157
Batch: 158
Batch: 159
Batch: 160
Batch: 161
Batch: 162
Batch: 163
Batch: 164
Batch: 165
Batch: 166
Batch: 167
Batch: 168
Batch: 169
Batch: 170
Batch: 171
Batch: 172
Batch: 173
Batch: 174
Batch: 175
Batch: 176
Batch: 177
Batch: 178
Batch: 179
Batch: 180
Batch: 181
Batch: 182
Batch: 183
Batch: 184
Batch: 185
Batch: 186
Batch: 187
Batch: 188
Batch: 189
Batch: 190
Batch: 191
Batch: 192
Batch: 193
Batch: 194
Batch: 195
Batch: 196
Batch: 197
Batch: 198
Batch: 199
Batch: 200
Batch: 201
Batch: 202
Batch: 203
Batch: 204
Batch: 205
Batch: 206
Batch: 207
Batch: 208
Batch: 209
Batch: 210
Batch: 211
Batch: 212
Batch: 213
Batch: 214
Batch: 215
Batch: 216
Batch: 217
Batch: 218
Batch: 219
Batch: 220
Batch: 221
Batch: 222
Batch: 223
Batch: 224
Batch: 225
Batch: 226
Batch: 227
Batch: 228
Batch: 229
Batch: 230
Batch: 231
Batch: 232
Batch: 233
Batch: 234
Batch: 235
Batch: 236
Batch: 237
Batch: 238
Batch: 239
Batch: 240
Batch: 241
Batch: 242
Batch: 243
Batch: 244
Batch: 245
Batch: 246
Batch: 247
Batch: 248
Batch: 249
Batch: 250
Batch: 251
Batch: 252
Batch: 253
Batch: 254
Batch: 255
Batch: 256
Batch: 257
Batch: 258
Batch: 259
Batch: 260
Batch: 261
Batch: 262
Batch: 263
Batch: 264
Batch: 265
Batch: 266
Batch: 267
Batch: 268
Batch: 269
Batch: 270
Batch: 271
Batch: 272
Batch: 273
Batch: 274
Batch: 275
Batch: 276
Batch: 277
Batch: 278
Batch: 279
Batch: 280
Batch: 281
Batch: 282
Batch: 283
Batch: 284
Batch: 285
Batch: 286
Batch: 287
Batch: 288
Batch: 289
Batch: 290
Batch: 291
Batch: 292
Batch: 293
Batch: 294
Batch: 295
Batch: 296
Batch: 297
Batch: 298
Batch: 299
Batch: 300
Batch: 301
Batch: 302
Batch: 303
Batch: 304
Batch: 305
Batch: 306
Batch: 307
Batch: 308
Batch: 309
Batch: 310
Batch: 311
Batch: 312
Batch: 313
Batch: 314
Batch: 315
Batch: 316
Batch: 317
Batch: 318
Batch: 319
Batch: 320
Batch: 321
Batch: 322
Batch: 323
Batch: 324
Batch: 325
Batch: 326
Batch: 327
Batch: 328
Batch: 329
Batch: 330
Batch: 331
Batch: 332
Batch: 333
Batch: 334
Batch: 335
Batch: 336
Batch: 337
Batch: 338
Batch: 339
Batch: 340
Batch: 341
Batch: 342
Batch: 343
Batch: 344
Batch: 345
Batch: 346
Batch: 347
Batch: 348
Batch: 349
Batch: 350
Batch: 351
Batch: 352
Batch: 353
Batch: 354
Batch: 355
Batch: 356
Batch: 357
Batch: 358
Batch: 359
Batch: 360
Batch: 361
Batch: 362
Batch: 363
Batch: 364
Batch: 365
Batch: 366
Batch: 367
Batch: 368
Batch: 369
Batch: 370
Batch: 371
Batch: 372
Batch: 373
Batch: 374
Batch: 375
Batch: 376
Batch: 377
Batch: 378
Batch: 379
Batch: 380
Batch: 381
Batch: 382
Batch: 383
Batch: 384
Batch: 385
Batch: 386
Batch: 387
Batch: 388
Batch: 389
Batch: 390
Batch: 391
Batch: 392
Batch: 393
Batch: 394
Batch: 395
Batch: 396
Batch: 397
Batch: 398
Batch: 399
Batch: 400
Batch: 401
Batch: 402
Batch: 403
Batch: 404
Batch: 405
Batch: 406
Batch: 407
Batch: 408
Batch: 409
Batch: 410
Batch: 411
Batch: 412
Batch: 413
Batch: 414
Batch: 415
Batch: 416
Batch: 417
Batch: 418
Batch: 419
Batch: 420
Batch: 421
Batch: 422
Batch: 423
Batch: 424
Batch: 425
Batch: 426
Batch: 427
Batch: 428
Batch: 429
Batch: 430
Batch: 431
Batch: 432
Batch: 433
Batch: 434
Batch: 435
Batch: 436
Batch: 437
Batch: 438
Batch: 439
Batch: 440
Batch: 441
Batch: 442
Batch: 443
Batch: 444
Batch: 445
Batch: 446
Batch: 447
Batch: 448
Batch: 449
Batch: 450
Batch: 451
Batch: 452
Batch: 453
Batch: 454
Batch: 455
Batch: 456
Batch: 457
Batch: 458
Batch: 459
Batch: 460
Batch: 461
Batch: 462
Batch: 463
Batch: 464
Batch: 465
Batch: 466
Batch: 467
Batch: 468
Batch: 469
Batch: 470
Batch: 471
Batch: 472
Batch: 473
Batch: 474
Batch: 475
Batch: 476
Batch: 477
Batch: 478
Batch: 479
Batch: 480
Batch: 481
Batch: 482
Batch: 483
Batch: 484
Batch: 485
Batch: 486
Batch: 487
Batch: 488
Batch: 489
Batch: 490
Batch: 491
Batch: 492
Batch: 493
Batch: 494
Batch: 495
Batch: 496
Batch: 497
Batch: 498
Batch: 499
Batch: 500
Batch: 501
Batch: 502
Batch: 503
Batch: 504
Batch: 505
Batch: 506
Batch: 507
Batch: 508
Batch: 509
Batch: 510
Batch: 511
Batch: 512
Batch: 513
Batch: 514
Batch: 515
Batch: 516
Batch: 517
Batch: 518
Batch: 519
Batch: 520
Batch: 521
Batch: 522
Batch: 523
Batch: 524
Batch: 525
Batch: 526
Batch: 527
Batch: 528
Batch: 529
Batch: 530
Batch: 531
Batch: 532
Batch: 533
Batch: 534
Batch: 535
Batch: 536
Batch: 537
Batch: 538
Batch: 539
Batch: 540
Batch: 541
Batch: 542
Batch: 543
Batch: 544
Batch: 545
Batch: 546
Batch: 547
Batch: 548
Batch: 549
Batch: 550
Batch: 551
Batch: 552
Batch: 553
Batch: 554
Batch: 555
Batch: 556
Batch: 557
Batch: 558
Batch: 559
Batch: 560
Batch: 561
Batch: 562
Batch: 563
Batch: 564
Batch: 565
Batch: 566
Batch: 567
Batch: 568
Batch: 569
Batch: 570
Batch: 571
Batch: 572
Batch: 573
Batch: 574
Batch: 575
Batch: 576
Batch: 577
Batch: 578
Batch: 579
Batch: 580
Batch: 581
Batch: 582
Batch: 583
Batch: 584
Batch: 585
Batch: 586
Batch: 587
Batch: 588
Batch: 589
Batch: 590
Batch: 591
Batch: 592
Batch: 593
Batch: 594
Batch: 595
Batch: 596
Batch: 597
Batch: 598
Batch: 599
Batch: 600
Batch: 601
Batch: 602
Batch: 603
Batch: 604
Batch: 605
Batch: 606
Batch: 607
Batch: 608
Batch: 609
Batch: 610
Batch: 611
Batch: 612
Batch: 613
Batch: 614
Batch: 615
Batch: 616
Batch: 617
Batch: 618
Batch: 619
Batch: 620
Batch: 621
Batch: 622
Batch: 623
Batch: 624
Batch: 625
Batch: 626
Batch: 627
Batch: 628
Batch: 629
Batch: 630
Batch: 631
Batch: 632
Batch: 633
Batch: 634
Batch: 635
Batch: 636
Batch: 637
Batch: 638
Batch: 639
Batch: 640
Batch: 641
Batch: 642
Batch: 643
Batch: 644
Batch: 645
Batch: 646
Batch: 647
Batch: 648
Batch: 649
Batch: 650
Batch: 651
Batch: 652
Batch: 653
Batch: 654
Batch: 655
Batch: 656
Batch: 657
Batch: 658
Batch: 659
Batch: 660
Batch: 661
Batch: 662
Batch: 663
Batch: 664
Batch: 665
Batch: 666
Batch: 667
Batch: 668
Batch: 669
Batch: 670
Batch: 671
Batch: 672
Batch: 673
Batch: 674
Batch: 675
Batch: 676
Batch: 677
Batch: 678
Batch: 679
Batch: 680
Batch: 681
Batch: 682
Batch: 683
Batch: 684
Batch: 685
Batch: 686
Batch: 687
Batch: 688
Batch: 689
Batch: 690
Batch: 691
Batch: 692
Batch: 693
Batch: 694
Batch: 695
Batch: 696
Batch: 697
Batch: 698
Batch: 699
Batch: 700
Batch: 701
Batch: 702
Batch: 703
Batch: 704
Batch: 705
Batch: 706
Batch: 707
Batch: 708
Batch: 709
Batch: 710
Batch: 711
Batch: 712
Batch: 713
Batch: 714
Batch: 715
Batch: 716
Batch: 717
Batch: 718
Batch: 719
Batch: 720
Batch: 721
Batch: 722
Batch: 723
Batch: 724
Batch: 725
Batch: 726
Batch: 727
Batch: 728
Batch: 729
Batch: 730
Batch: 731
Batch: 732
Batch: 733
Batch: 734
Batch: 735
Batch: 736
Batch: 737
Batch: 738
Batch: 739
Batch: 740
Batch: 741
Batch: 742
Batch: 743
Batch: 744
Batch: 745
Batch: 746
Batch: 747
Batch: 748
Batch: 749
Batch: 750
Batch: 751
Batch: 752
Batch: 753
Batch: 754
Batch: 755
Batch: 756
Batch: 757
Batch: 758
Batch: 759
Batch: 760
Batch: 761
Batch: 762
Batch: 763
Batch: 764
Batch: 765
Batch: 766
Batch: 767
Batch: 768
Batch: 769
Batch: 770
Batch: 771
Batch: 772
Batch: 773
Batch: 774
Batch: 775
Batch: 776
Batch: 777
Batch: 778
Batch: 779
Batch: 780
Batch: 781
Batch: 782
Batch: 783
Batch: 784
Batch: 785
Batch: 786
Batch: 787
Batch: 788
Batch: 789
Batch: 790
Batch: 791
Batch: 792
Batch: 793
Batch: 794
Batch: 795
Batch: 796
Batch: 797
Batch: 798
Batch: 799
Batch: 800
Batch: 801
Batch: 802
Batch: 803
Batch: 804
Batch: 805
Batch: 806
Batch: 807
Batch: 808
Batch: 809
Batch: 810
Batch: 811
Batch: 812
Batch: 813
Batch: 814
Batch: 815
Batch: 816
Batch: 817
Batch: 818
Batch: 819
Batch: 820
Batch: 821
Batch: 822
Batch: 823
Batch: 824
Batch: 825
Batch: 826
Batch: 827
Batch: 828
Batch: 829
Batch: 830
Batch: 831
Batch: 832
Batch: 833
Batch: 834
Batch: 835
Batch: 836
Batch: 837
Batch: 838
Batch: 839
Batch: 840
Batch: 841
Batch: 842
Batch: 843
Batch: 844
Batch: 845
Batch: 846
Batch: 847
Batch: 848
Batch: 849
Batch: 850
Batch: 851
Batch: 852
Batch: 853
Batch: 854
Batch: 855
Batch: 856
Batch: 857
Batch: 858
Batch: 859
Batch: 860
Batch: 861
Batch: 862
Batch: 863
Batch: 864
Batch: 865
Batch: 866
Batch: 867
Batch: 868
Batch: 869
Batch: 870
Batch: 871
Batch: 872
Batch: 873
Batch: 874
Batch: 875
Batch: 876
Batch: 877
Batch: 878
Batch: 879
Batch: 880
Batch: 881
Batch: 882
Batch: 883
Batch: 884
Batch: 885
Batch: 886
Batch: 887
Batch: 888
Batch: 889
Batch: 890
Batch: 891
Batch: 892
Batch: 893
Batch: 894
Batch: 895
Batch: 896
Batch: 897
Batch: 898
Batch: 899
Batch: 900
Batch: 901
Batch: 902
Batch: 903
Batch: 904
Batch: 905
Batch: 906
Batch: 907
Batch: 908
Batch: 909
Batch: 910
Batch: 911
Batch: 912
Batch: 913
Batch: 914
Batch: 915
Batch: 916
Batch: 917
Batch: 918
Batch: 919
Batch: 920
Batch: 921
Batch: 922
Batch: 923
Batch: 924
Batch: 925
Batch: 926
Batch: 927
Batch: 928
Batch: 929
Batch: 930
Batch: 931
Batch: 932
Batch: 933
Batch: 934
Batch: 935
Batch: 936
Batch: 937
Batch: 938
Batch: 939
Batch: 940
Batch: 941
Batch: 942
Batch: 943
Batch: 944
Batch: 945
Batch: 946
Batch: 947
Batch: 948
Batch: 949
Batch: 950
Batch: 951
Batch: 952
Batch: 953
Batch: 954
Batch: 955
Batch: 956
Batch: 957
Batch: 958
Batch: 959
Batch: 960
Batch: 961
Batch: 962
Batch: 963
Batch: 964
Batch: 965
Batch: 966
Batch: 967
Batch: 968
Batch: 969
Batch: 970
Batch: 971
Batch: 972
Batch: 973
Batch: 974
Batch: 975
Batch: 976
Batch: 977
Batch: 978
Batch: 979
Batch: 980
Batch: 981
Batch: 982
Batch: 983
Batch: 984
Batch: 985
Batch: 986
Batch: 987
Batch: 988
Batch: 989
Batch: 990
Batch: 991
Batch: 992
Batch: 993
Batch: 994
Batch: 995
Batch: 996
Batch: 997
Batch: 998
Batch: 999
Batch: 1000
Batch: 1001
Batch: 1002
Batch: 1003
Batch: 1004
Batch: 1005
Batch: 1006
Batch: 1007
Batch: 1008
Batch: 1009
Batch: 1010
Batch: 1011
Batch: 1012
Batch: 1013
Batch: 1014
Batch: 1015
Batch: 1016
Batch: 1017
Batch: 1018
Batch: 1019
Batch: 1020
Batch: 1021
Batch: 1022
Batch: 1023
Batch: 1024
Batch: 1025
Batch: 1026
Batch: 1027
Batch: 1028
Batch: 1029
Batch: 1030
Batch: 1031
Batch: 1032
Batch: 1033
Batch: 1034
Save Encoder Model...
	iters: 100, epoch: 1 | loss: 0.0000001
	speed: 0.1703s/iter; left time: 1745.4336s
	iters: 200, epoch: 1 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 1715.8955s
	iters: 300, epoch: 1 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 1701.7067s
	iters: 400, epoch: 1 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1682.8879s
	iters: 500, epoch: 1 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1666.2870s
	iters: 600, epoch: 1 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1649.5525s
	iters: 700, epoch: 1 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1632.7987s
	iters: 800, epoch: 1 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1615.7636s
	iters: 900, epoch: 1 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1598.7426s
	iters: 1000, epoch: 1 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1581.3051s
Epoch: 1 cost time: 175.19094109535217
Epoch: 1, Steps: 1035 | Train Loss: 0.0000001 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0000000
	speed: 1.0309s/iter; left time: 9501.0896s
	iters: 200, epoch: 2 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1542.0874s
	iters: 300, epoch: 2 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1524.6503s
	iters: 400, epoch: 2 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 1506.7621s
	iters: 500, epoch: 2 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1492.0571s
	iters: 600, epoch: 2 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 1473.4117s
	iters: 700, epoch: 2 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 1456.3381s
	iters: 800, epoch: 2 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 1439.2583s
	iters: 900, epoch: 2 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 1422.2140s
	iters: 1000, epoch: 2 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 1405.4363s
Epoch: 2 cost time: 175.1074299812317
Epoch: 2, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0000000
	speed: 1.0309s/iter; left time: 8433.4042s
	iters: 200, epoch: 3 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1367.1395s
	iters: 300, epoch: 3 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1350.2152s
	iters: 400, epoch: 3 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1333.4193s
	iters: 500, epoch: 3 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1316.2178s
	iters: 600, epoch: 3 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1299.4536s
	iters: 700, epoch: 3 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1282.6013s
	iters: 800, epoch: 3 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 1266.5715s
	iters: 900, epoch: 3 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1247.7696s
	iters: 1000, epoch: 3 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 1230.8134s
Epoch: 3 cost time: 175.18171739578247
Epoch: 3, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0000000
	speed: 1.0314s/iter; left time: 7370.7355s
	iters: 200, epoch: 4 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1192.2731s
	iters: 300, epoch: 4 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 1175.1139s
	iters: 400, epoch: 4 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1157.9818s
	iters: 500, epoch: 4 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1140.7267s
	iters: 600, epoch: 4 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1123.6834s
	iters: 700, epoch: 4 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1106.9164s
	iters: 800, epoch: 4 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1089.9799s
	iters: 900, epoch: 4 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 1073.0534s
	iters: 1000, epoch: 4 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 1057.6141s
Epoch: 4 cost time: 175.1902720928192
Epoch: 4, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0000000
	speed: 1.0312s/iter; left time: 6301.4005s
	iters: 200, epoch: 5 | loss: 0.0000000
	speed: 0.1689s/iter; left time: 1015.3710s
	iters: 300, epoch: 5 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 999.3627s
	iters: 400, epoch: 5 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 984.2805s
	iters: 500, epoch: 5 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 965.2853s
	iters: 600, epoch: 5 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 948.5868s
	iters: 700, epoch: 5 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 932.6665s
	iters: 800, epoch: 5 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 915.8018s
	iters: 900, epoch: 5 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 897.7618s
	iters: 1000, epoch: 5 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 880.8752s
Epoch: 5 cost time: 175.13256907463074
Epoch: 5, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0000000
	speed: 1.0320s/iter; left time: 5238.5038s
	iters: 200, epoch: 6 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 842.4194s
	iters: 300, epoch: 6 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 825.3472s
	iters: 400, epoch: 6 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 808.2596s
	iters: 500, epoch: 6 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 791.4442s
	iters: 600, epoch: 6 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 774.4273s
	iters: 700, epoch: 6 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 757.4462s
	iters: 800, epoch: 6 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 740.4371s
	iters: 900, epoch: 6 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 723.4298s
	iters: 1000, epoch: 6 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 706.7251s
Epoch: 6 cost time: 175.30090641975403
Epoch: 6, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0000000
	speed: 1.0320s/iter; left time: 4170.4742s
	iters: 200, epoch: 7 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 667.3232s
	iters: 300, epoch: 7 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 649.3765s
	iters: 400, epoch: 7 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 632.5225s
	iters: 500, epoch: 7 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 615.7069s
	iters: 600, epoch: 7 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 598.8218s
	iters: 700, epoch: 7 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 581.7499s
	iters: 800, epoch: 7 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 564.8827s
	iters: 900, epoch: 7 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 547.9629s
	iters: 1000, epoch: 7 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 531.1117s
Epoch: 7 cost time: 175.1660487651825
Epoch: 7, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0000000
	speed: 1.0325s/iter; left time: 3103.7028s
	iters: 200, epoch: 8 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 491.5253s
	iters: 300, epoch: 8 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 474.6438s
	iters: 400, epoch: 8 | loss: 0.0000000
	speed: 0.1694s/iter; left time: 458.3275s
	iters: 500, epoch: 8 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 440.7213s
	iters: 600, epoch: 8 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 423.6334s
	iters: 700, epoch: 8 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 406.7034s
	iters: 800, epoch: 8 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 389.8330s
	iters: 900, epoch: 8 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 372.9627s
	iters: 1000, epoch: 8 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 356.0117s
Epoch: 8 cost time: 175.15974044799805
Epoch: 8, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0000000
	speed: 1.0318s/iter; left time: 2033.7467s
	iters: 200, epoch: 9 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 316.4615s
	iters: 300, epoch: 9 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 299.4935s
	iters: 400, epoch: 9 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 282.5109s
	iters: 500, epoch: 9 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 265.6346s
	iters: 600, epoch: 9 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 249.0043s
	iters: 700, epoch: 9 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 231.7661s
	iters: 800, epoch: 9 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 214.8163s
	iters: 900, epoch: 9 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 197.9154s
	iters: 1000, epoch: 9 | loss: 0.0000000
	speed: 0.1690s/iter; left time: 181.0264s
Epoch: 9 cost time: 175.12361431121826
Epoch: 9, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0000000
	speed: 1.0318s/iter; left time: 965.7605s
	iters: 200, epoch: 10 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 141.3695s
	iters: 300, epoch: 10 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 124.5242s
	iters: 400, epoch: 10 | loss: 0.0000000
	speed: 0.1692s/iter; left time: 107.6096s
	iters: 500, epoch: 10 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 90.6544s
	iters: 600, epoch: 10 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 73.7339s
	iters: 700, epoch: 10 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 56.8076s
	iters: 800, epoch: 10 | loss: 0.0000000
	speed: 0.1693s/iter; left time: 39.9535s
	iters: 900, epoch: 10 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 22.9930s
	iters: 1000, epoch: 10 | loss: 0.0000000
	speed: 0.1691s/iter; left time: 6.0869s
Epoch: 10 cost time: 175.1654815673828
Epoch: 10, Steps: 1035 | Train Loss: 0.0000000 Vali Loss: 0.0000000 Test Loss: 0.0000000
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : PSM_GPT4TS_PSM_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (87841, 25)
train: (132481, 25)
test 87742
test: (87841, 25)
train: (132481, 25)
train 132382
Threshold : 7.435287732171541e-09
pred:    (8774200,)
gt:      (8774200,)
pred:  (8774200,)
gt:    (8774200,)
Accuracy : 0.9573, Precision : 0.9499, Recall : 0.8934, F-score : 0.9208, AUC : 0.9376
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, channels=25, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=20, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs20_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 4950
test: (449919, 51)
train: (495000, 51)
val 4499
test: (449919, 51)
train: (495000, 51)
test 4499
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Save Encoder Model...
Epoch: 8
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 6.928238391876221
Epoch: 1, Steps: 39 | Train Loss: 0.0000026 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.927257776260376
Epoch: 2, Steps: 39 | Train Loss: 0.0000004 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.912128210067749
Epoch: 3, Steps: 39 | Train Loss: 0.0000003 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.905313014984131
Epoch: 4, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 6.933168411254883
Epoch: 5, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 6.944719314575195
Epoch: 6, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 6.939557075500488
Epoch: 7, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 6.9156107902526855
Epoch: 8, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.912589073181152
Epoch: 9, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.9348413944244385
Epoch: 10, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs20_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 4499
test: (449919, 51)
train: (495000, 51)
train 4950
Threshold : 5.257118800727768e-07
pred:    (449900,)
gt:      (449900,)
pred:  (449900,)
gt:    (449900,)
Accuracy : 0.9577, Precision : 0.8340, Recall : 0.8131, F-score : 0.8235, AUC : 0.8954
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, channels=25, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=20, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs20_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 4950
test: (449919, 51)
train: (495000, 51)
val 4499
test: (449919, 51)
train: (495000, 51)
test 4499
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Save Encoder Model...
Epoch: 8
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 6.9449403285980225
Epoch: 1, Steps: 39 | Train Loss: 0.0000026 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.950695753097534
Epoch: 2, Steps: 39 | Train Loss: 0.0000004 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.955168962478638
Epoch: 3, Steps: 39 | Train Loss: 0.0000003 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.973426342010498
Epoch: 4, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 6.940423488616943
Epoch: 5, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 6.945524215698242
Epoch: 6, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 6.983291149139404
Epoch: 7, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 6.943522691726685
Epoch: 8, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.944928884506226
Epoch: 9, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.936767101287842
Epoch: 10, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs20_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 4499
test: (449919, 51)
train: (495000, 51)
train 4950
Threshold : 5.284080117462509e-07
pred:    (449900,)
gt:      (449900,)
pred:  (449900,)
gt:    (449900,)
Accuracy : 0.9620, Precision : 0.8402, Recall : 0.8483, F-score : 0.8442, AUC : 0.9130
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, channels=25, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=20, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs20_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 4950
test: (449919, 51)
train: (495000, 51)
val 4499
test: (449919, 51)
train: (495000, 51)
test 4499
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Save Encoder Model...
Epoch: 8
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 6.954155683517456
Epoch: 1, Steps: 39 | Train Loss: 0.0000026 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.929992198944092
Epoch: 2, Steps: 39 | Train Loss: 0.0000004 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.934986114501953
Epoch: 3, Steps: 39 | Train Loss: 0.0000003 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.9421796798706055
Epoch: 4, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 6.9366774559021
Epoch: 5, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 6.93278694152832
Epoch: 6, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 6.934760332107544
Epoch: 7, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 6.937939643859863
Epoch: 8, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.930587291717529
Epoch: 9, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.9218056201934814
Epoch: 10, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs20_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 4499
test: (449919, 51)
train: (495000, 51)
train 4950
Threshold : 5.339717631613895e-07
pred:    (449900,)
gt:      (449900,)
pred:  (449900,)
gt:    (449900,)
Accuracy : 0.9601, Precision : 0.8372, Recall : 0.8330, F-score : 0.8351, AUC : 0.9053
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, channels=25, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 4950
test: (449919, 51)
train: (495000, 51)
val 4499
test: (449919, 51)
train: (495000, 51)
test 4499
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Epoch: 8
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 6.963809251785278
Epoch: 1, Steps: 39 | Train Loss: 0.0000022 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.94997763633728
Epoch: 2, Steps: 39 | Train Loss: 0.0000004 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.954512357711792
Epoch: 3, Steps: 39 | Train Loss: 0.0000003 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.951283931732178
Epoch: 4, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 6.951769590377808
Epoch: 5, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 6.943509340286255
Epoch: 6, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 6.97221302986145
Epoch: 7, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 6.956583023071289
Epoch: 8, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.947050333023071
Epoch: 9, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.941909313201904
Epoch: 10, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 4499
test: (449919, 51)
train: (495000, 51)
train 4950
Threshold : 5.032009607930379e-07
pred:    (449900,)
gt:      (449900,)
pred:  (449900,)
gt:    (449900,)
Accuracy : 0.9757, Precision : 0.9787, Recall : 0.8179, F-score : 0.8911, AUC : 0.9077
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, channels=25, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 4950
test: (449919, 51)
train: (495000, 51)
val 4499
test: (449919, 51)
train: (495000, 51)
test 4499
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Epoch: 8
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 6.9408063888549805
Epoch: 1, Steps: 39 | Train Loss: 0.0000019 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.925030946731567
Epoch: 2, Steps: 39 | Train Loss: 0.0000003 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.92227578163147
Epoch: 3, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.92643404006958
Epoch: 4, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 6.92583966255188
Epoch: 5, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 6.92920446395874
Epoch: 6, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 6.945988893508911
Epoch: 7, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 6.9252259731292725
Epoch: 8, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.919274091720581
Epoch: 9, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.922206401824951
Epoch: 10, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 4499
test: (449919, 51)
train: (495000, 51)
train 4950
Threshold : 4.5009279773466917e-07
pred:    (449900,)
gt:      (449900,)
pred:  (449900,)
gt:    (449900,)
Accuracy : 0.9701, Precision : 0.8657, Recall : 0.8920, F-score : 0.8786, AUC : 0.9364
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, channels=25, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 4950
test: (449919, 51)
train: (495000, 51)
val 4499
test: (449919, 51)
train: (495000, 51)
test 4499
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Epoch: 8
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 6.964694023132324
Epoch: 1, Steps: 39 | Train Loss: 0.0000023 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.96390962600708
Epoch: 2, Steps: 39 | Train Loss: 0.0000004 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.957635402679443
Epoch: 3, Steps: 39 | Train Loss: 0.0000003 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.954747438430786
Epoch: 4, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 6.955061912536621
Epoch: 5, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 6.979109287261963
Epoch: 6, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 6.958035230636597
Epoch: 7, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 6.95691180229187
Epoch: 8, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.951195240020752
Epoch: 9, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.964514493942261
Epoch: 10, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 4499
test: (449919, 51)
train: (495000, 51)
train 4950
Threshold : 4.796550268792999e-07
pred:    (449900,)
gt:      (449900,)
pred:  (449900,)
gt:    (449900,)
Accuracy : 0.9725, Precision : 0.9252, Recall : 0.8413, F-score : 0.8812, AUC : 0.9159
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, channels=25, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=5, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs5_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 4950
test: (449919, 51)
train: (495000, 51)
val 4499
test: (449919, 51)
train: (495000, 51)
test 4499
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Save Encoder Model...
Epoch: 8
Save Encoder Model...
Epoch: 9
Epoch: 1 cost time: 6.915719509124756
Epoch: 1, Steps: 39 | Train Loss: 0.0000023 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.909657955169678
Epoch: 2, Steps: 39 | Train Loss: 0.0000004 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.911292314529419
Epoch: 3, Steps: 39 | Train Loss: 0.0000003 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.911951780319214
Epoch: 4, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 6.910751819610596
Epoch: 5, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 6.9372718334198
Epoch: 6, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 6.913257837295532
Epoch: 7, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 6.909325838088989
Epoch: 8, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.911671876907349
Epoch: 9, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.910374879837036
Epoch: 10, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs5_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 4499
test: (449919, 51)
train: (495000, 51)
train 4950
Threshold : 3.7433893396610084e-07
pred:    (449900,)
gt:      (449900,)
pred:  (449900,)
gt:    (449900,)
Accuracy : 0.9668, Precision : 0.8820, Recall : 0.8391, F-score : 0.8600, AUC : 0.9118
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=51, channels=25, checkpoints='./checkpoints/', d_ff=128, d_model=768, data='SWAT', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=51, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='SWAT', moving_avg=25, nb_random_samples=5, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/SWaT', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs5_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (449919, 51)
train: (495000, 51)
train 4950
test: (449919, 51)
train: (495000, 51)
val 4499
test: (449919, 51)
train: (495000, 51)
test 4499
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Save Encoder Model...
Epoch: 8
Save Encoder Model...
Epoch: 9
Epoch: 1 cost time: 6.909081697463989
Epoch: 1, Steps: 39 | Train Loss: 0.0000024 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (inf --> 0.000000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.9215357303619385
Epoch: 2, Steps: 39 | Train Loss: 0.0000004 Vali Loss: 0.0000003 Test Loss: 0.0000003
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.908802270889282
Epoch: 3, Steps: 39 | Train Loss: 0.0000003 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.91306734085083
Epoch: 4, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 6.916752338409424
Epoch: 5, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 6.94504451751709
Epoch: 6, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 6.910835027694702
Epoch: 7, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 6.919758319854736
Epoch: 8, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.919826030731201
Epoch: 9, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.910484790802002
Epoch: 10, Steps: 39 | Train Loss: 0.0000002 Vali Loss: 0.0000002 Test Loss: 0.0000002
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : SWAT_GPT4TS_SWAT_sl100_dm768_df128_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs5_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (449919, 51)
train: (495000, 51)
test 4499
test: (449919, 51)
train: (495000, 51)
train 4950
Threshold : 3.772555118075616e-07
pred:    (449900,)
gt:      (449900,)
pred:  (449900,)
gt:    (449900,)
Accuracy : 0.9665, Precision : 0.8772, Recall : 0.8423, F-score : 0.8594, AUC : 0.9130
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=25, channels=25, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='PSM', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=25, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='PSM', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/PSM', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : PSM_GPT4TS_PSM_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (87841, 25)
train: (132481, 25)
train 1324
test: (87841, 25)
train: (132481, 25)
val 878
test: (87841, 25)
train: (132481, 25)
test 878
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Save Encoder Model...
Epoch: 8
Save Encoder Model...
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 2.100722551345825
Epoch: 1, Steps: 11 | Train Loss: 0.0000069 Vali Loss: 0.0000029 Test Loss: 0.0000035
Validation loss decreased (inf --> 0.000003).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 2.1026999950408936
Epoch: 2, Steps: 11 | Train Loss: 0.0000027 Vali Loss: 0.0000011 Test Loss: 0.0000012
Validation loss decreased (0.000003 --> 0.000001).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 2.1038951873779297
Epoch: 3, Steps: 11 | Train Loss: 0.0000013 Vali Loss: 0.0000007 Test Loss: 0.0000008
Validation loss decreased (0.000001 --> 0.000001).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 2.0972955226898193
Epoch: 4, Steps: 11 | Train Loss: 0.0000010 Vali Loss: 0.0000006 Test Loss: 0.0000007
Validation loss decreased (0.000001 --> 0.000001).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 2.1024365425109863
Epoch: 5, Steps: 11 | Train Loss: 0.0000009 Vali Loss: 0.0000005 Test Loss: 0.0000006
Validation loss decreased (0.000001 --> 0.000001).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 2.101886510848999
Epoch: 6, Steps: 11 | Train Loss: 0.0000008 Vali Loss: 0.0000005 Test Loss: 0.0000006
Validation loss decreased (0.000001 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 2.0982930660247803
Epoch: 7, Steps: 11 | Train Loss: 0.0000008 Vali Loss: 0.0000005 Test Loss: 0.0000006
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 2.1044416427612305
Epoch: 8, Steps: 11 | Train Loss: 0.0000008 Vali Loss: 0.0000005 Test Loss: 0.0000006
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 2.1017088890075684
Epoch: 9, Steps: 11 | Train Loss: 0.0000008 Vali Loss: 0.0000005 Test Loss: 0.0000006
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 2.096778154373169
Epoch: 10, Steps: 11 | Train Loss: 0.0000008 Vali Loss: 0.0000005 Test Loss: 0.0000006
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : PSM_GPT4TS_PSM_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (87841, 25)
train: (132481, 25)
test 878
test: (87841, 25)
train: (132481, 25)
train 1324
Threshold : 6.4620898729117565e-06
pred:    (87800,)
gt:      (87800,)
pred:  (87800,)
gt:    (87800,)
Accuracy : 0.9852, Precision : 0.9623, Recall : 0.9852, F-score : 0.9736, AUC : 0.9852
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=1.0, batch_size=128, c_out=25, channels=25, checkpoints='./checkpoints/', d_ff=768, d_model=768, data='PSM', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=25, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='PSM', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/PSM', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : PSM_GPT4TS_PSM_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (87841, 25)
train: (132481, 25)
train 1324
test: (87841, 25)
train: (132481, 25)
val 878
test: (87841, 25)
train: (132481, 25)
test 878
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Save Encoder Model...
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Save Encoder Model...
Epoch: 8
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 1.9998202323913574
Epoch: 1, Steps: 11 | Train Loss: 0.0000041 Vali Loss: 0.0000017 Test Loss: 0.0000018
Validation loss decreased (inf --> 0.000002).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.9985311031341553
Epoch: 2, Steps: 11 | Train Loss: 0.0000014 Vali Loss: 0.0000006 Test Loss: 0.0000007
Validation loss decreased (0.000002 --> 0.000001).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.9989914894104004
Epoch: 3, Steps: 11 | Train Loss: 0.0000007 Vali Loss: 0.0000005 Test Loss: 0.0000005
Validation loss decreased (0.000001 --> 0.000001).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 2.00020170211792
Epoch: 4, Steps: 11 | Train Loss: 0.0000006 Vali Loss: 0.0000004 Test Loss: 0.0000004
Validation loss decreased (0.000001 --> 0.000000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.997176170349121
Epoch: 5, Steps: 11 | Train Loss: 0.0000005 Vali Loss: 0.0000004 Test Loss: 0.0000004
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.9936420917510986
Epoch: 6, Steps: 11 | Train Loss: 0.0000005 Vali Loss: 0.0000004 Test Loss: 0.0000004
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.9947569370269775
Epoch: 7, Steps: 11 | Train Loss: 0.0000005 Vali Loss: 0.0000004 Test Loss: 0.0000004
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.9961519241333008
Epoch: 8, Steps: 11 | Train Loss: 0.0000005 Vali Loss: 0.0000004 Test Loss: 0.0000004
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 2.012869119644165
Epoch: 9, Steps: 11 | Train Loss: 0.0000005 Vali Loss: 0.0000004 Test Loss: 0.0000004
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 2.0001933574676514
Epoch: 10, Steps: 11 | Train Loss: 0.0000005 Vali Loss: 0.0000004 Test Loss: 0.0000004
Validation loss decreased (0.000000 --> 0.000000).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : PSM_GPT4TS_PSM_sl100_dm768_df768_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (87841, 25)
train: (132481, 25)
test 878
test: (87841, 25)
train: (132481, 25)
train 1324
Threshold : 5.615179297819867e-06
pred:    (87800,)
gt:      (87800,)
pred:  (87800,)
gt:    (87800,)
Accuracy : 0.9581, Precision : 0.9639, Recall : 0.8818, F-score : 0.9210, AUC : 0.9346
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=2.0, batch_size=128, c_out=55, channels=25, checkpoints='./checkpoints/', d_ff=8, d_model=768, data='MSL', data_path='ETTh1.csv', dec_in=7, des='test', devices='0,1,2,3', distil=True, dropout=0.1, embed='timeF', enc_in=55, factor=1, feature_epochs=10, feature_lr=0.001, features='M', freq='h', gpt_layers=6, gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, ln=0, loss='MSE', lradj='type1', mask_rate=0.25, mlp=0, model='GPT4TS', model_id='MSL', moving_avg=25, nb_random_samples=10, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, patience=3, percent=5, pool_size=10, pred_len=0, prompt_len=5, resume=False, root_path='./all_datasets/MSL', seasonal_patterns='Monthly', seq_len=100, stride=1, target='OT', top_k=5, train_epochs=10, use_amp=False, use_feature_embedding=True, use_gpu=True, use_multi_gpu=False, use_prompt_pool=True, use_skip_embedding=True, visualize=False, weight=0)
Use GPU: cuda:0
>>>>>>>start training : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 583
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
val 737
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 737
Start train feature encoder...
Epoch: 0
Save Encoder Model...
Epoch: 1
Save Encoder Model...
Epoch: 2
Save Encoder Model...
Epoch: 3
Epoch: 4
Save Encoder Model...
Epoch: 5
Save Encoder Model...
Epoch: 6
Save Encoder Model...
Epoch: 7
Epoch: 8
Save Encoder Model...
Epoch: 9
Save Encoder Model...
Epoch: 1 cost time: 1.1150596141815186
Epoch: 1, Steps: 5 | Train Loss: 0.0000182 Vali Loss: 0.0000133 Test Loss: 0.0000130
Validation loss decreased (inf --> 0.000013).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.1175596714019775
Epoch: 2, Steps: 5 | Train Loss: 0.0000128 Vali Loss: 0.0000116 Test Loss: 0.0000119
Validation loss decreased (0.000013 --> 0.000012).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1138169765472412
Epoch: 3, Steps: 5 | Train Loss: 0.0000108 Vali Loss: 0.0000108 Test Loss: 0.0000115
Validation loss decreased (0.000012 --> 0.000011).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1143755912780762
Epoch: 4, Steps: 5 | Train Loss: 0.0000101 Vali Loss: 0.0000103 Test Loss: 0.0000110
Validation loss decreased (0.000011 --> 0.000010).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.1198174953460693
Epoch: 5, Steps: 5 | Train Loss: 0.0000093 Vali Loss: 0.0000105 Test Loss: 0.0000107
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.1129164695739746
Epoch: 6, Steps: 5 | Train Loss: 0.0000091 Vali Loss: 0.0000097 Test Loss: 0.0000105
Validation loss decreased (0.000010 --> 0.000010).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.115828514099121
Epoch: 7, Steps: 5 | Train Loss: 0.0000090 Vali Loss: 0.0000097 Test Loss: 0.0000104
Validation loss decreased (0.000010 --> 0.000010).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.1173779964447021
Epoch: 8, Steps: 5 | Train Loss: 0.0000090 Vali Loss: 0.0000096 Test Loss: 0.0000104
Validation loss decreased (0.000010 --> 0.000010).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.1265647411346436
Epoch: 9, Steps: 5 | Train Loss: 0.0000091 Vali Loss: 0.0000095 Test Loss: 0.0000104
Validation loss decreased (0.000010 --> 0.000010).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.118499994277954
Epoch: 10, Steps: 5 | Train Loss: 0.0000089 Vali Loss: 0.0000096 Test Loss: 0.0000103
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : MSL_GPT4TS_MSL_sl100_dm768_df8_0_seTrue_feTrue_ppTrue_top5_pl5_ps10_nrs10_flr0.001_fepo10_ch25_reFalse<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
test 737
test: (73729, 55)
train: (58317, 55)
test_label: (73729,)
train 583
Threshold : 6.18830719031394e-05
pred:    (73700,)
gt:      (73700,)
pred:  (73700,)
gt:    (73700,)
Accuracy : 0.9670, Precision : 0.8159, Recall : 0.8864, F-score : 0.8497, AUC : 0.9314
